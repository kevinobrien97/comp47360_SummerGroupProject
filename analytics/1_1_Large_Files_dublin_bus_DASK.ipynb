{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29f95522",
   "metadata": {},
   "source": [
    "### Large Files Data Processing with DASK\n",
    "- This was the original file we used with the intent of merging and cleaning the datasets.\n",
    "- The file attempted to process teh entire datast using DASK but as the process went on the number of tasks which the dataframe had to compute grew so much that it put the kernel under too much stress so we had to re-evaluate our approach and integrate pandas, as shown in the other large file.\n",
    "- We were able to get to the stage where we extracted the 46a merged file using DASK so it could be sue for tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c8296fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime as dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "911901a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in ./miniconda3/envs/comp47360py39/lib/python3.9/site-packages (3.5.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./miniconda3/envs/comp47360py39/lib/python3.9/site-packages (from matplotlib) (9.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./miniconda3/envs/comp47360py39/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./miniconda3/envs/comp47360py39/lib/python3.9/site-packages (from matplotlib) (1.22.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./miniconda3/envs/comp47360py39/lib/python3.9/site-packages (from matplotlib) (1.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./miniconda3/envs/comp47360py39/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./miniconda3/envs/comp47360py39/lib/python3.9/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in ./miniconda3/envs/comp47360py39/lib/python3.9/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./miniconda3/envs/comp47360py39/lib/python3.9/site-packages (from matplotlib) (4.33.3)\n",
      "Requirement already satisfied: six>=1.5 in ./miniconda3/envs/comp47360py39/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb5b054e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/team9/tmp\n"
     ]
    }
   ],
   "source": [
    "cd tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91e0631a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mdata\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e20928e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/team9/tmp/data\n"
     ]
    }
   ],
   "source": [
    "cd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "336dfc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom_location_53_345035_-6_267261_62b5c8e6c91d98000ba01ceb.csv\r\n",
      "rt_leavetimes_DB_2018.txt\r\n",
      "rt_trips_DB_2018.txt\r\n",
      "rt_vehicles_DB_2018.txt\r\n",
      "\u001b[0m\u001b[01;31mrt_vehicles_DB_2018.zip\u001b[0m\r\n",
      "trips_modelling.csv\r\n",
      "weather_factorised.csv\r\n",
      "weather_trips.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1730ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv('Custom_location_53_345035_-6_267261_62b5c8e6c91d98000ba01ceb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f8e9c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27186, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79e1162d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>dt_iso</th>\n",
       "      <th>timezone</th>\n",
       "      <th>city_name</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>temp</th>\n",
       "      <th>visibility</th>\n",
       "      <th>dew_point</th>\n",
       "      <th>feels_like</th>\n",
       "      <th>...</th>\n",
       "      <th>wind_gust</th>\n",
       "      <th>rain_1h</th>\n",
       "      <th>rain_3h</th>\n",
       "      <th>snow_1h</th>\n",
       "      <th>snow_3h</th>\n",
       "      <th>clouds_all</th>\n",
       "      <th>weather_id</th>\n",
       "      <th>weather_main</th>\n",
       "      <th>weather_description</th>\n",
       "      <th>weather_icon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1483228800</td>\n",
       "      <td>2017-01-01 00:00:00 +0000 UTC</td>\n",
       "      <td>0</td>\n",
       "      <td>Custom location</td>\n",
       "      <td>53.345035</td>\n",
       "      <td>-6.267261</td>\n",
       "      <td>5.39</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>4.35</td>\n",
       "      <td>1.78</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75</td>\n",
       "      <td>501</td>\n",
       "      <td>Rain</td>\n",
       "      <td>moderate rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1483232400</td>\n",
       "      <td>2017-01-01 01:00:00 +0000 UTC</td>\n",
       "      <td>0</td>\n",
       "      <td>Custom location</td>\n",
       "      <td>53.345035</td>\n",
       "      <td>-6.267261</td>\n",
       "      <td>5.39</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.28</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75</td>\n",
       "      <td>501</td>\n",
       "      <td>Rain</td>\n",
       "      <td>moderate rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1483236000</td>\n",
       "      <td>2017-01-01 02:00:00 +0000 UTC</td>\n",
       "      <td>0</td>\n",
       "      <td>Custom location</td>\n",
       "      <td>53.345035</td>\n",
       "      <td>-6.267261</td>\n",
       "      <td>5.39</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.28</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75</td>\n",
       "      <td>500</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1483239600</td>\n",
       "      <td>2017-01-01 03:00:00 +0000 UTC</td>\n",
       "      <td>0</td>\n",
       "      <td>Custom location</td>\n",
       "      <td>53.345035</td>\n",
       "      <td>-6.267261</td>\n",
       "      <td>4.39</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75</td>\n",
       "      <td>500</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1483243200</td>\n",
       "      <td>2017-01-01 04:00:00 +0000 UTC</td>\n",
       "      <td>0</td>\n",
       "      <td>Custom location</td>\n",
       "      <td>53.345035</td>\n",
       "      <td>-6.267261</td>\n",
       "      <td>4.39</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>2.42</td>\n",
       "      <td>0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75</td>\n",
       "      <td>803</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>broken clouds</td>\n",
       "      <td>04n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27181</th>\n",
       "      <td>1577818800</td>\n",
       "      <td>2019-12-31 19:00:00 +0000 UTC</td>\n",
       "      <td>0</td>\n",
       "      <td>Custom location</td>\n",
       "      <td>53.345035</td>\n",
       "      <td>-6.267261</td>\n",
       "      <td>9.33</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>6.42</td>\n",
       "      <td>9.33</td>\n",
       "      <td>...</td>\n",
       "      <td>2.68</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75</td>\n",
       "      <td>803</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>broken clouds</td>\n",
       "      <td>04n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27182</th>\n",
       "      <td>1577822400</td>\n",
       "      <td>2019-12-31 20:00:00 +0000 UTC</td>\n",
       "      <td>0</td>\n",
       "      <td>Custom location</td>\n",
       "      <td>53.345035</td>\n",
       "      <td>-6.267261</td>\n",
       "      <td>8.81</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>6.09</td>\n",
       "      <td>7.02</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90</td>\n",
       "      <td>804</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>overcast clouds</td>\n",
       "      <td>04n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27183</th>\n",
       "      <td>1577826000</td>\n",
       "      <td>2019-12-31 21:00:00 +0000 UTC</td>\n",
       "      <td>0</td>\n",
       "      <td>Custom location</td>\n",
       "      <td>53.345035</td>\n",
       "      <td>-6.267261</td>\n",
       "      <td>7.86</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>5.33</td>\n",
       "      <td>7.86</td>\n",
       "      <td>...</td>\n",
       "      <td>1.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90</td>\n",
       "      <td>804</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>overcast clouds</td>\n",
       "      <td>04n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27184</th>\n",
       "      <td>1577829600</td>\n",
       "      <td>2019-12-31 22:00:00 +0000 UTC</td>\n",
       "      <td>0</td>\n",
       "      <td>Custom location</td>\n",
       "      <td>53.345035</td>\n",
       "      <td>-6.267261</td>\n",
       "      <td>7.49</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>4.09</td>\n",
       "      <td>7.49</td>\n",
       "      <td>...</td>\n",
       "      <td>1.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75</td>\n",
       "      <td>803</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>broken clouds</td>\n",
       "      <td>04n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27185</th>\n",
       "      <td>1577833200</td>\n",
       "      <td>2019-12-31 23:00:00 +0000 UTC</td>\n",
       "      <td>0</td>\n",
       "      <td>Custom location</td>\n",
       "      <td>53.345035</td>\n",
       "      <td>-6.267261</td>\n",
       "      <td>7.21</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>4.52</td>\n",
       "      <td>5.85</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75</td>\n",
       "      <td>803</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>broken clouds</td>\n",
       "      <td>04n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27186 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               dt                         dt_iso  timezone        city_name  \\\n",
       "0      1483228800  2017-01-01 00:00:00 +0000 UTC         0  Custom location   \n",
       "1      1483232400  2017-01-01 01:00:00 +0000 UTC         0  Custom location   \n",
       "2      1483236000  2017-01-01 02:00:00 +0000 UTC         0  Custom location   \n",
       "3      1483239600  2017-01-01 03:00:00 +0000 UTC         0  Custom location   \n",
       "4      1483243200  2017-01-01 04:00:00 +0000 UTC         0  Custom location   \n",
       "...           ...                            ...       ...              ...   \n",
       "27181  1577818800  2019-12-31 19:00:00 +0000 UTC         0  Custom location   \n",
       "27182  1577822400  2019-12-31 20:00:00 +0000 UTC         0  Custom location   \n",
       "27183  1577826000  2019-12-31 21:00:00 +0000 UTC         0  Custom location   \n",
       "27184  1577829600  2019-12-31 22:00:00 +0000 UTC         0  Custom location   \n",
       "27185  1577833200  2019-12-31 23:00:00 +0000 UTC         0  Custom location   \n",
       "\n",
       "             lat       lon  temp  visibility  dew_point  feels_like  ...  \\\n",
       "0      53.345035 -6.267261  5.39      9999.0       4.35        1.78  ...   \n",
       "1      53.345035 -6.267261  5.39      9999.0       4.35        2.28  ...   \n",
       "2      53.345035 -6.267261  5.39      9999.0       4.35        2.28  ...   \n",
       "3      53.345035 -6.267261  4.39      9999.0       3.36        0.04  ...   \n",
       "4      53.345035 -6.267261  4.39      9999.0       2.42        0.04  ...   \n",
       "...          ...       ...   ...         ...        ...         ...  ...   \n",
       "27181  53.345035 -6.267261  9.33      9999.0       6.42        9.33  ...   \n",
       "27182  53.345035 -6.267261  8.81      9999.0       6.09        7.02  ...   \n",
       "27183  53.345035 -6.267261  7.86      9999.0       5.33        7.86  ...   \n",
       "27184  53.345035 -6.267261  7.49      9999.0       4.09        7.49  ...   \n",
       "27185  53.345035 -6.267261  7.21      9999.0       4.52        5.85  ...   \n",
       "\n",
       "       wind_gust  rain_1h  rain_3h  snow_1h  snow_3h  clouds_all  weather_id  \\\n",
       "0            NaN     2.30      NaN      NaN      NaN          75         501   \n",
       "1            NaN     1.51      NaN      NaN      NaN          75         501   \n",
       "2            NaN     0.64      NaN      NaN      NaN          75         500   \n",
       "3            NaN     0.17      NaN      NaN      NaN          75         500   \n",
       "4            NaN      NaN      NaN      NaN      NaN          75         803   \n",
       "...          ...      ...      ...      ...      ...         ...         ...   \n",
       "27181       2.68      NaN      NaN      NaN      NaN          75         803   \n",
       "27182        NaN      NaN      NaN      NaN      NaN          90         804   \n",
       "27183       1.79      NaN      NaN      NaN      NaN          90         804   \n",
       "27184       1.79      NaN      NaN      NaN      NaN          75         803   \n",
       "27185        NaN      NaN      NaN      NaN      NaN          75         803   \n",
       "\n",
       "       weather_main  weather_description  weather_icon  \n",
       "0              Rain        moderate rain           10n  \n",
       "1              Rain        moderate rain           10n  \n",
       "2              Rain           light rain           10n  \n",
       "3              Rain           light rain           10n  \n",
       "4            Clouds        broken clouds           04n  \n",
       "...             ...                  ...           ...  \n",
       "27181        Clouds        broken clouds           04n  \n",
       "27182        Clouds      overcast clouds           04n  \n",
       "27183        Clouds      overcast clouds           04n  \n",
       "27184        Clouds        broken clouds           04n  \n",
       "27185        Clouds        broken clouds           04n  \n",
       "\n",
       "[27186 rows x 28 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8e269af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt                     26280\n",
       "dt_iso                 26280\n",
       "timezone                   2\n",
       "city_name                  1\n",
       "lat                        1\n",
       "lon                        1\n",
       "temp                    2009\n",
       "visibility                45\n",
       "dew_point               1929\n",
       "feels_like              2550\n",
       "temp_min                 672\n",
       "temp_max                 919\n",
       "pressure                  74\n",
       "sea_level                  0\n",
       "grnd_level                 0\n",
       "humidity                  73\n",
       "wind_speed               101\n",
       "wind_deg                 358\n",
       "wind_gust                 85\n",
       "rain_1h                  281\n",
       "rain_3h                    0\n",
       "snow_1h                   49\n",
       "snow_3h                    0\n",
       "clouds_all                17\n",
       "weather_id                27\n",
       "weather_main              10\n",
       "weather_description       29\n",
       "weather_icon              17\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c457ee34",
   "metadata": {},
   "source": [
    "- most look unhelpful\n",
    "- keeping to categorical data\n",
    "- weather description, weather icon, temperature, visibility, wind speed, temperature\n",
    "- will condense the categories down for better predictions\n",
    "- i think i can ignore the timezone issue because the weather icon says whether it is day or night and it is only 1 hour difference and there is a timezone element in the dt iso column\n",
    "- there are not an equal amount of datetimes as total columns so must remove duplicates\n",
    "- weather icon dropoped after model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b04cc9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = weather.drop([\"dt_iso\", \"temp_min\", \"weather_id\", \"clouds_all\", \"snow_3h\", \"snow_1h\", \"rain_3h\", \"rain_1h\", \"wind_gust\", \"wind_deg\", \"humidity\", \"grnd_level\", \"sea_level\", \"pressure\", \"temp_max\", \"feels_like\", \"dew_point\", \"lon\", \"lat\", \"city_name\", \"timezone\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d6cc1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>temp</th>\n",
       "      <th>visibility</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>weather_main</th>\n",
       "      <th>weather_description</th>\n",
       "      <th>weather_icon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1483228800</td>\n",
       "      <td>5.39</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>5.10</td>\n",
       "      <td>Rain</td>\n",
       "      <td>moderate rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1483232400</td>\n",
       "      <td>5.39</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>4.10</td>\n",
       "      <td>Rain</td>\n",
       "      <td>moderate rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1483236000</td>\n",
       "      <td>5.39</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>4.10</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1483239600</td>\n",
       "      <td>4.39</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1483243200</td>\n",
       "      <td>4.39</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>broken clouds</td>\n",
       "      <td>04n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27181</th>\n",
       "      <td>1577818800</td>\n",
       "      <td>9.33</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>broken clouds</td>\n",
       "      <td>04n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27182</th>\n",
       "      <td>1577822400</td>\n",
       "      <td>8.81</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>3.10</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>overcast clouds</td>\n",
       "      <td>04n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27183</th>\n",
       "      <td>1577826000</td>\n",
       "      <td>7.86</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>overcast clouds</td>\n",
       "      <td>04n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27184</th>\n",
       "      <td>1577829600</td>\n",
       "      <td>7.49</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>broken clouds</td>\n",
       "      <td>04n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27185</th>\n",
       "      <td>1577833200</td>\n",
       "      <td>7.21</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>2.10</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>broken clouds</td>\n",
       "      <td>04n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27186 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               dt  temp  visibility  wind_speed weather_main  \\\n",
       "0      1483228800  5.39      9999.0        5.10         Rain   \n",
       "1      1483232400  5.39      9999.0        4.10         Rain   \n",
       "2      1483236000  5.39      9999.0        4.10         Rain   \n",
       "3      1483239600  4.39      9999.0        6.20         Rain   \n",
       "4      1483243200  4.39      9999.0        6.20       Clouds   \n",
       "...           ...   ...         ...         ...          ...   \n",
       "27181  1577818800  9.33      9999.0        0.45       Clouds   \n",
       "27182  1577822400  8.81      9999.0        3.10       Clouds   \n",
       "27183  1577826000  7.86      9999.0        0.45       Clouds   \n",
       "27184  1577829600  7.49      9999.0        0.45       Clouds   \n",
       "27185  1577833200  7.21      9999.0        2.10       Clouds   \n",
       "\n",
       "      weather_description weather_icon  \n",
       "0           moderate rain          10n  \n",
       "1           moderate rain          10n  \n",
       "2              light rain          10n  \n",
       "3              light rain          10n  \n",
       "4           broken clouds          04n  \n",
       "...                   ...          ...  \n",
       "27181       broken clouds          04n  \n",
       "27182     overcast clouds          04n  \n",
       "27183     overcast clouds          04n  \n",
       "27184       broken clouds          04n  \n",
       "27185       broken clouds          04n  \n",
       "\n",
       "[27186 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc57b711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.92\n",
      "-4.61\n"
     ]
    }
   ],
   "source": [
    "print(weather[\"temp\"].max())\n",
    "print(weather[\"temp\"].min())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71c27fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt                       int64\n",
       "temp                   float64\n",
       "visibility             float64\n",
       "wind_speed             float64\n",
       "weather_main            object\n",
       "weather_description     object\n",
       "weather_icon            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc42cf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dupli_epoch = weather[weather.duplicated(['dt'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50096231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>temp</th>\n",
       "      <th>visibility</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>weather_main</th>\n",
       "      <th>weather_description</th>\n",
       "      <th>weather_icon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>1483711200</td>\n",
       "      <td>11.39</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>6.70</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>1483714800</td>\n",
       "      <td>11.39</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>1483718400</td>\n",
       "      <td>11.92</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>6.70</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>1483930800</td>\n",
       "      <td>9.39</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>9.80</td>\n",
       "      <td>Drizzle</td>\n",
       "      <td>light intensity drizzle</td>\n",
       "      <td>09n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>1485568800</td>\n",
       "      <td>6.00</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27002</th>\n",
       "      <td>1577188800</td>\n",
       "      <td>7.48</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>0.89</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27062</th>\n",
       "      <td>1577401200</td>\n",
       "      <td>8.62</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>3.10</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27064</th>\n",
       "      <td>1577404800</td>\n",
       "      <td>8.43</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>2.10</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27066</th>\n",
       "      <td>1577408400</td>\n",
       "      <td>8.70</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>3.10</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27068</th>\n",
       "      <td>1577412000</td>\n",
       "      <td>8.78</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>3.10</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>906 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               dt   temp  visibility  wind_speed weather_main  \\\n",
       "135    1483711200  11.39      9000.0        6.70         Rain   \n",
       "137    1483714800  11.39      9999.0        6.20         Rain   \n",
       "139    1483718400  11.92      9999.0        6.70         Rain   \n",
       "199    1483930800   9.39      9999.0        9.80      Drizzle   \n",
       "655    1485568800   6.00      9999.0        2.60         Rain   \n",
       "...           ...    ...         ...         ...          ...   \n",
       "27002  1577188800   7.48      9000.0        0.89         Rain   \n",
       "27062  1577401200   8.62      8000.0        3.10         Rain   \n",
       "27064  1577404800   8.43      8000.0        2.10         Rain   \n",
       "27066  1577408400   8.70      5000.0        3.10         Rain   \n",
       "27068  1577412000   8.78      8000.0        3.10         Rain   \n",
       "\n",
       "           weather_description weather_icon  \n",
       "135                 light rain          10d  \n",
       "137                 light rain          10d  \n",
       "139                 light rain          10d  \n",
       "199    light intensity drizzle          09n  \n",
       "655                 light rain          10n  \n",
       "...                        ...          ...  \n",
       "27002               light rain          10d  \n",
       "27062               light rain          10n  \n",
       "27064               light rain          10n  \n",
       "27066               light rain          10n  \n",
       "27068               light rain          10n  \n",
       "\n",
       "[906 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dupli_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dcd9793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             dt   temp  visibility  wind_speed weather_main  \\\n",
      "134  1483711200  11.39      9000.0         6.7      Drizzle   \n",
      "135  1483711200  11.39      9000.0         6.7         Rain   \n",
      "\n",
      "              weather_description weather_icon  \n",
      "134  light intensity drizzle rain          09d  \n",
      "135                    light rain          10d  \n",
      "             dt   temp  visibility  wind_speed weather_main  \\\n",
      "138  1483718400  11.92      9999.0         6.7      Drizzle   \n",
      "139  1483718400  11.92      9999.0         6.7         Rain   \n",
      "\n",
      "              weather_description weather_icon  \n",
      "138  light intensity drizzle rain          09d  \n",
      "139                    light rain          10d  \n",
      "             dt   temp  visibility  wind_speed weather_main  \\\n",
      "134  1483711200  11.39      9000.0         6.7      Drizzle   \n",
      "135  1483711200  11.39      9000.0         6.7         Rain   \n",
      "\n",
      "              weather_description weather_icon  \n",
      "134  light intensity drizzle rain          09d  \n",
      "135                    light rain          10d  \n",
      "             dt  temp  visibility  wind_speed weather_main  \\\n",
      "198  1483930800  9.39      9999.0         9.8         Rain   \n",
      "199  1483930800  9.39      9999.0         9.8      Drizzle   \n",
      "\n",
      "         weather_description weather_icon  \n",
      "198               light rain          10n  \n",
      "199  light intensity drizzle          09n  \n",
      "               dt  temp  visibility  wind_speed weather_main  \\\n",
      "27063  1577404800  8.43      8000.0         2.1      Drizzle   \n",
      "27064  1577404800  8.43      8000.0         2.1         Rain   \n",
      "\n",
      "           weather_description weather_icon  \n",
      "27063  light intensity drizzle          09n  \n",
      "27064               light rain          10n  \n",
      "               dt  temp  visibility  wind_speed weather_main  \\\n",
      "27067  1577412000  8.78      8000.0         3.1      Drizzle   \n",
      "27068  1577412000  8.78      8000.0         3.1         Rain   \n",
      "\n",
      "                weather_description weather_icon  \n",
      "27067  light intensity drizzle rain          09n  \n",
      "27068                    light rain          10n  \n"
     ]
    }
   ],
   "source": [
    "dupliexamine_epoch = weather.loc[weather['dt'] == 1483711200]\n",
    "print(dupliexamine_epoch)\n",
    "dupliexamine_epoch = weather.loc[weather['dt'] == 1483718400]\n",
    "print(dupliexamine_epoch)\n",
    "dupliexamine_epoch = weather.loc[weather['dt'] == 1483711200]\n",
    "print(dupliexamine_epoch)\n",
    "dupliexamine_epoch = weather.loc[weather['dt'] == 1483930800]\n",
    "print(dupliexamine_epoch)\n",
    "dupliexamine_epoch = weather.loc[weather['dt'] == 1577404800]\n",
    "print(dupliexamine_epoch)\n",
    "dupliexamine_epoch = weather.loc[weather['dt'] == 1577412000]\n",
    "print(dupliexamine_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6bbd8d",
   "metadata": {},
   "source": [
    "- duplicates are all basically the same with slight variations in the final 2 columns so are not worth keeping in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a295791",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = weather.drop_duplicates(subset=['dt'])\n",
    "weather=weather.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0aee338d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>temp</th>\n",
       "      <th>visibility</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>weather_main</th>\n",
       "      <th>weather_description</th>\n",
       "      <th>weather_icon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>1483696800</td>\n",
       "      <td>10.44</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>1483700400</td>\n",
       "      <td>11.39</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>1483704000</td>\n",
       "      <td>11.39</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>broken clouds</td>\n",
       "      <td>04d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>1483707600</td>\n",
       "      <td>11.39</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>1483711200</td>\n",
       "      <td>11.39</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>Drizzle</td>\n",
       "      <td>light intensity drizzle rain</td>\n",
       "      <td>09d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>1483714800</td>\n",
       "      <td>11.39</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>Drizzle</td>\n",
       "      <td>light intensity drizzle rain</td>\n",
       "      <td>09d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>1483718400</td>\n",
       "      <td>11.92</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>Drizzle</td>\n",
       "      <td>light intensity drizzle rain</td>\n",
       "      <td>09d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>1483722000</td>\n",
       "      <td>11.92</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>Drizzle</td>\n",
       "      <td>light intensity drizzle</td>\n",
       "      <td>09n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>1483725600</td>\n",
       "      <td>11.39</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>broken clouds</td>\n",
       "      <td>04n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>1483729200</td>\n",
       "      <td>11.39</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>Drizzle</td>\n",
       "      <td>light intensity drizzle</td>\n",
       "      <td>09n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>1483732800</td>\n",
       "      <td>11.39</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>broken clouds</td>\n",
       "      <td>04n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             dt   temp  visibility  wind_speed weather_main  \\\n",
       "130  1483696800  10.44      8000.0         6.0         Rain   \n",
       "131  1483700400  11.39      9999.0         4.6         Rain   \n",
       "132  1483704000  11.39      9999.0         6.2       Clouds   \n",
       "133  1483707600  11.39      9999.0         7.2         Rain   \n",
       "134  1483711200  11.39      9000.0         6.7      Drizzle   \n",
       "135  1483714800  11.39      9999.0         6.2      Drizzle   \n",
       "136  1483718400  11.92      9999.0         6.7      Drizzle   \n",
       "137  1483722000  11.92      9999.0         6.2      Drizzle   \n",
       "138  1483725600  11.39      9999.0         6.2       Clouds   \n",
       "139  1483729200  11.39      9999.0         5.7      Drizzle   \n",
       "140  1483732800  11.39      9999.0         3.6       Clouds   \n",
       "\n",
       "              weather_description weather_icon  \n",
       "130                    light rain          10d  \n",
       "131                    light rain          10d  \n",
       "132                 broken clouds          04d  \n",
       "133                    light rain          10d  \n",
       "134  light intensity drizzle rain          09d  \n",
       "135  light intensity drizzle rain          09d  \n",
       "136  light intensity drizzle rain          09d  \n",
       "137       light intensity drizzle          09n  \n",
       "138                 broken clouds          04n  \n",
       "139       light intensity drizzle          09n  \n",
       "140                 broken clouds          04n  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.loc[130:140]\n",
    "#check to see drop happnened\n",
    "# 135 should be gone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec1ead50",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.to_csv('./weather_trips.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbdb3d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_int(x, base):\n",
    "    return int(base * round(float(x)/base))\n",
    "\n",
    "weather['temp'] = weather['temp'].apply(lambda x: round_int(x, base=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bba6bf28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>temp</th>\n",
       "      <th>visibility</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>weather_main</th>\n",
       "      <th>weather_description</th>\n",
       "      <th>weather_icon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1483228800</td>\n",
       "      <td>5</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>Rain</td>\n",
       "      <td>moderate rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1483232400</td>\n",
       "      <td>5</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>Rain</td>\n",
       "      <td>moderate rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1483236000</td>\n",
       "      <td>5</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1483239600</td>\n",
       "      <td>5</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1483243200</td>\n",
       "      <td>5</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>broken clouds</td>\n",
       "      <td>04n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  temp  visibility  wind_speed weather_main weather_description  \\\n",
       "0  1483228800     5      9999.0         5.1         Rain       moderate rain   \n",
       "1  1483232400     5      9999.0         4.1         Rain       moderate rain   \n",
       "2  1483236000     5      9999.0         4.1         Rain          light rain   \n",
       "3  1483239600     5      9999.0         6.2         Rain          light rain   \n",
       "4  1483243200     5      9999.0         6.2       Clouds       broken clouds   \n",
       "\n",
       "  weather_icon  \n",
       "0          10n  \n",
       "1          10n  \n",
       "2          10n  \n",
       "3          10n  \n",
       "4          04n  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6aceeb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['temp'] = pd.factorize(weather['temp'], sort=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fea98ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>temp</th>\n",
       "      <th>visibility</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>weather_main</th>\n",
       "      <th>weather_description</th>\n",
       "      <th>weather_icon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1483228800</td>\n",
       "      <td>2</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>Rain</td>\n",
       "      <td>moderate rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1483232400</td>\n",
       "      <td>2</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>Rain</td>\n",
       "      <td>moderate rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1483236000</td>\n",
       "      <td>2</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1483239600</td>\n",
       "      <td>2</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1483243200</td>\n",
       "      <td>2</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>broken clouds</td>\n",
       "      <td>04n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1483246800</td>\n",
       "      <td>2</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>scattered clouds</td>\n",
       "      <td>03n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1483250400</td>\n",
       "      <td>1</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>few clouds</td>\n",
       "      <td>02n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1483254000</td>\n",
       "      <td>1</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>few clouds</td>\n",
       "      <td>02n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1483257600</td>\n",
       "      <td>1</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>few clouds</td>\n",
       "      <td>02n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1483261200</td>\n",
       "      <td>2</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>few clouds</td>\n",
       "      <td>02d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  temp  visibility  wind_speed weather_main weather_description  \\\n",
       "0  1483228800     2      9999.0         5.1         Rain       moderate rain   \n",
       "1  1483232400     2      9999.0         4.1         Rain       moderate rain   \n",
       "2  1483236000     2      9999.0         4.1         Rain          light rain   \n",
       "3  1483239600     2      9999.0         6.2         Rain          light rain   \n",
       "4  1483243200     2      9999.0         6.2       Clouds       broken clouds   \n",
       "5  1483246800     2      9999.0         6.2       Clouds    scattered clouds   \n",
       "6  1483250400     1      9999.0         5.7       Clouds          few clouds   \n",
       "7  1483254000     1      9999.0         6.7       Clouds          few clouds   \n",
       "8  1483257600     1      9999.0         6.2       Clouds          few clouds   \n",
       "9  1483261200     2      9999.0         5.7       Clouds          few clouds   \n",
       "\n",
       "  weather_icon  \n",
       "0          10n  \n",
       "1          10n  \n",
       "2          10n  \n",
       "3          10n  \n",
       "4          04n  \n",
       "5          03n  \n",
       "6          02n  \n",
       "7          02n  \n",
       "8          02n  \n",
       "9          02d  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "509a1f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt                      0\n",
       "temp                    0\n",
       "visibility             45\n",
       "wind_speed              0\n",
       "weather_main            0\n",
       "weather_description     0\n",
       "weather_icon            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f84d039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vis = weather['visibility'].mean()\n",
    "weather['visibility'] = weather['visibility'].fillna(mean_vis)\n",
    "\n",
    "weather['visibility'].round(0).astype(int)\n",
    "\n",
    "weather['visibility'] = weather['visibility'].apply(lambda x: round_int(x, base=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ff4157a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>temp</th>\n",
       "      <th>visibility</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>weather_main</th>\n",
       "      <th>weather_description</th>\n",
       "      <th>weather_icon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>1486828800</td>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>8.8</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>1486832400</td>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>9.8</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>1486836000</td>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>11.3</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>1486839600</td>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>9.3</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>1486843200</td>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>10.8</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>1486846800</td>\n",
       "      <td>2</td>\n",
       "      <td>8000</td>\n",
       "      <td>9.3</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>1486850400</td>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>9.8</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>1486854000</td>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>10.3</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>1486857600</td>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>9.3</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>1486861200</td>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>9.8</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>1486864800</td>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>9.8</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>1486868400</td>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>8.2</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>1486872000</td>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>9.3</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>1486875600</td>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>10.8</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>1486879200</td>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>10.3</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>1486882800</td>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>9.3</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>1486886400</td>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>9.8</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>1486890000</td>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>9.3</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>broken clouds</td>\n",
       "      <td>04d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>1486893600</td>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>11.3</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>broken clouds</td>\n",
       "      <td>04d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>1486897200</td>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>11.3</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>broken clouds</td>\n",
       "      <td>04d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>1486900800</td>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>9.8</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>broken clouds</td>\n",
       "      <td>04d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>1486904400</td>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>8.8</td>\n",
       "      <td>Drizzle</td>\n",
       "      <td>light intensity drizzle</td>\n",
       "      <td>09d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>1486908000</td>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>9.8</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light intensity shower rain</td>\n",
       "      <td>09d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>1486911600</td>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>10.8</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light intensity shower rain</td>\n",
       "      <td>09d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>1486915200</td>\n",
       "      <td>2</td>\n",
       "      <td>8000</td>\n",
       "      <td>9.3</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>1486918800</td>\n",
       "      <td>2</td>\n",
       "      <td>8000</td>\n",
       "      <td>9.8</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>1486922400</td>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>11.3</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>1486926000</td>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>10.8</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>1486929600</td>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>10.3</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>1486933200</td>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>10.3</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>broken clouds</td>\n",
       "      <td>04n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>1486936800</td>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>9.8</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>broken clouds</td>\n",
       "      <td>04n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              dt  temp  visibility  wind_speed weather_main  \\\n",
       "1000  1486828800     2       10000         8.8         Rain   \n",
       "1001  1486832400     2       10000         9.8         Rain   \n",
       "1002  1486836000     2       10000        11.3         Rain   \n",
       "1003  1486839600     2       10000         9.3         Rain   \n",
       "1004  1486843200     2       10000        10.8         Rain   \n",
       "1005  1486846800     2        8000         9.3         Rain   \n",
       "1006  1486850400     2       10000         9.8         Rain   \n",
       "1007  1486854000     2       10000        10.3         Rain   \n",
       "1008  1486857600     2       10000         9.3         Rain   \n",
       "1009  1486861200     2       10000         9.8         Rain   \n",
       "1010  1486864800     2       10000         9.8         Rain   \n",
       "1011  1486868400     2       10000         8.2         Rain   \n",
       "1012  1486872000     2       10000         9.3         Rain   \n",
       "1013  1486875600     2       10000        10.8         Rain   \n",
       "1014  1486879200     2       10000        10.3         Rain   \n",
       "1015  1486882800     2       10000         9.3         Rain   \n",
       "1016  1486886400     2       10000         9.8         Rain   \n",
       "1017  1486890000     2       10000         9.3       Clouds   \n",
       "1018  1486893600     2       10000        11.3       Clouds   \n",
       "1019  1486897200     2       10000        11.3       Clouds   \n",
       "1020  1486900800     2       10000         9.8       Clouds   \n",
       "1021  1486904400     2       10000         8.8      Drizzle   \n",
       "1022  1486908000     2       10000         9.8         Rain   \n",
       "1023  1486911600     2       10000        10.8         Rain   \n",
       "1024  1486915200     2        8000         9.3         Rain   \n",
       "1025  1486918800     2        8000         9.8         Rain   \n",
       "1026  1486922400     2       10000        11.3         Rain   \n",
       "1027  1486926000     2       10000        10.8         Rain   \n",
       "1028  1486929600     2       10000        10.3         Rain   \n",
       "1029  1486933200     2       10000        10.3       Clouds   \n",
       "1030  1486936800     2       10000         9.8       Clouds   \n",
       "\n",
       "              weather_description weather_icon  \n",
       "1000                   light rain          10d  \n",
       "1001                   light rain          10d  \n",
       "1002                   light rain          10n  \n",
       "1003                   light rain          10n  \n",
       "1004                   light rain          10n  \n",
       "1005                   light rain          10n  \n",
       "1006                   light rain          10n  \n",
       "1007                   light rain          10n  \n",
       "1008                   light rain          10n  \n",
       "1009                   light rain          10n  \n",
       "1010                   light rain          10n  \n",
       "1011                   light rain          10n  \n",
       "1012                   light rain          10n  \n",
       "1013                   light rain          10n  \n",
       "1014                   light rain          10n  \n",
       "1015                   light rain          10n  \n",
       "1016                   light rain          10d  \n",
       "1017                broken clouds          04d  \n",
       "1018                broken clouds          04d  \n",
       "1019                broken clouds          04d  \n",
       "1020                broken clouds          04d  \n",
       "1021      light intensity drizzle          09d  \n",
       "1022  light intensity shower rain          09d  \n",
       "1023  light intensity shower rain          09d  \n",
       "1024                   light rain          10d  \n",
       "1025                   light rain          10d  \n",
       "1026                   light rain          10n  \n",
       "1027                   light rain          10n  \n",
       "1028                   light rain          10n  \n",
       "1029                broken clouds          04n  \n",
       "1030                broken clouds          04n  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.loc[1000:1030]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b310437",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['visibility'] = pd.factorize(weather['visibility'], sort=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62ed0576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>temp</th>\n",
       "      <th>visibility</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>weather_main</th>\n",
       "      <th>weather_description</th>\n",
       "      <th>weather_icon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1483228800</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5.1</td>\n",
       "      <td>Rain</td>\n",
       "      <td>moderate rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1483232400</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4.1</td>\n",
       "      <td>Rain</td>\n",
       "      <td>moderate rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1483236000</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4.1</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  temp  visibility  wind_speed weather_main weather_description  \\\n",
       "0  1483228800     2          10         5.1         Rain       moderate rain   \n",
       "1  1483232400     2          10         4.1         Rain       moderate rain   \n",
       "2  1483236000     2          10         4.1         Rain          light rain   \n",
       "\n",
       "  weather_icon  \n",
       "0          10n  \n",
       "1          10n  \n",
       "2          10n  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f80c3d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt                     26280\n",
       "temp                       7\n",
       "visibility                11\n",
       "wind_speed               101\n",
       "weather_main              10\n",
       "weather_description       29\n",
       "weather_icon              17\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0da40023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.1\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "max_wind = weather[\"wind_speed\"].max()\n",
    "min_wind = weather[\"wind_speed\"].min()\n",
    "\n",
    "print(max_wind)\n",
    "print(min_wind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ecfdff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather[\"wind_speed\"] = weather[\"wind_speed\"].apply(lambda x: round_int(x, base=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36150594",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather[\"wind_speed\"] = pd.factorize(weather[\"wind_speed\"], sort=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60e26eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>temp</th>\n",
       "      <th>visibility</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>weather_main</th>\n",
       "      <th>weather_description</th>\n",
       "      <th>weather_icon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1483228800</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>Rain</td>\n",
       "      <td>moderate rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1483232400</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>Rain</td>\n",
       "      <td>moderate rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1483236000</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1483239600</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>10n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1483243200</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>broken clouds</td>\n",
       "      <td>04n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1483246800</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>scattered clouds</td>\n",
       "      <td>03n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1483250400</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>few clouds</td>\n",
       "      <td>02n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1483254000</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>few clouds</td>\n",
       "      <td>02n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1483257600</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>few clouds</td>\n",
       "      <td>02n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1483261200</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>few clouds</td>\n",
       "      <td>02d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  temp  visibility  wind_speed weather_main weather_description  \\\n",
       "0  1483228800     2          10           3         Rain       moderate rain   \n",
       "1  1483232400     2          10           2         Rain       moderate rain   \n",
       "2  1483236000     2          10           2         Rain          light rain   \n",
       "3  1483239600     2          10           3         Rain          light rain   \n",
       "4  1483243200     2          10           3       Clouds       broken clouds   \n",
       "5  1483246800     2          10           3       Clouds    scattered clouds   \n",
       "6  1483250400     1          10           3       Clouds          few clouds   \n",
       "7  1483254000     1          10           3       Clouds          few clouds   \n",
       "8  1483257600     1          10           3       Clouds          few clouds   \n",
       "9  1483261200     2          10           3       Clouds          few clouds   \n",
       "\n",
       "  weather_icon  \n",
       "0          10n  \n",
       "1          10n  \n",
       "2          10n  \n",
       "3          10n  \n",
       "4          04n  \n",
       "5          03n  \n",
       "6          02n  \n",
       "7          02n  \n",
       "8          02n  \n",
       "9          02d  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d56edbb",
   "metadata": {},
   "source": [
    "### saving weather as this for initial model.\n",
    "- rest of it is a way to condense it so it will be manageable to add it to the leavetimes file which is muc larger and requires efficient use of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0d05bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather.to_csv('./weather_trips.csv') - forgot needed numbers for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f6a5d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['weather_main'] = weather['weather_main'].apply(lambda x: 1 if (x == \"Rain\" or x == \"Mist\" or x == \"Thunderstorm\") else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800aabc5",
   "metadata": {},
   "source": [
    "- dropping the weather icon and weather description\n",
    "    - after model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5239c26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = weather.drop([\"weather_description\", \"weather_icon\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aeeaef91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>temp</th>\n",
       "      <th>visibility</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>weather_main</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1483228800</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1483232400</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1483236000</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1483239600</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1483243200</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26275</th>\n",
       "      <td>1577818800</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26276</th>\n",
       "      <td>1577822400</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26277</th>\n",
       "      <td>1577826000</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26278</th>\n",
       "      <td>1577829600</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26279</th>\n",
       "      <td>1577833200</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26280 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               dt  temp  visibility  wind_speed  weather_main\n",
       "0      1483228800     2          10           3             1\n",
       "1      1483232400     2          10           2             1\n",
       "2      1483236000     2          10           2             1\n",
       "3      1483239600     2          10           3             1\n",
       "4      1483243200     2          10           3             0\n",
       "...           ...   ...         ...         ...           ...\n",
       "26275  1577818800     3          10           0             0\n",
       "26276  1577822400     3          10           2             0\n",
       "26277  1577826000     3          10           0             0\n",
       "26278  1577829600     2          10           0             0\n",
       "26279  1577833200     2          10           1             0\n",
       "\n",
       "[26280 rows x 5 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c3d307bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weather[\"weather_description\"] = pd.factorize(weather[\"weather_description\"], sort=True)[0]\n",
    "weather[\"weather_main\"] = pd.factorize(weather[\"weather_main\"], sort=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4822372",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weather[\"weather_icon\"] = pd.factorize(weather[\"weather_icon\"], sort=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "29c4ed64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>temp</th>\n",
       "      <th>visibility</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>weather_main</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1483228800</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1483232400</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1483236000</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1483239600</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1483243200</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  temp  visibility  wind_speed  weather_main\n",
       "0  1483228800     2          10           3             1\n",
       "1  1483232400     2          10           2             1\n",
       "2  1483236000     2          10           2             1\n",
       "3  1483239600     2          10           3             1\n",
       "4  1483243200     2          10           3             0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "760b6b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt              int64\n",
       "temp            int64\n",
       "visibility      int64\n",
       "wind_speed      int64\n",
       "weather_main    int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d7584ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>temp</th>\n",
       "      <th>visibility</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>weather_main</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1483228800</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1483232400</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-01-01 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1483236000</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-01-01 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1483239600</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-01-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1483243200</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26275</th>\n",
       "      <td>1577818800</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-31 19:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26276</th>\n",
       "      <td>1577822400</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-31 20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26277</th>\n",
       "      <td>1577826000</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-31 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26278</th>\n",
       "      <td>1577829600</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-31 22:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26279</th>\n",
       "      <td>1577833200</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-31 23:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26280 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               dt  temp  visibility  wind_speed  weather_main  \\\n",
       "0      1483228800     2          10           3             1   \n",
       "1      1483232400     2          10           2             1   \n",
       "2      1483236000     2          10           2             1   \n",
       "3      1483239600     2          10           3             1   \n",
       "4      1483243200     2          10           3             0   \n",
       "...           ...   ...         ...         ...           ...   \n",
       "26275  1577818800     3          10           0             0   \n",
       "26276  1577822400     3          10           2             0   \n",
       "26277  1577826000     3          10           0             0   \n",
       "26278  1577829600     2          10           0             0   \n",
       "26279  1577833200     2          10           1             0   \n",
       "\n",
       "                     date  \n",
       "0     2017-01-01 00:00:00  \n",
       "1     2017-01-01 01:00:00  \n",
       "2     2017-01-01 02:00:00  \n",
       "3     2017-01-01 03:00:00  \n",
       "4     2017-01-01 04:00:00  \n",
       "...                   ...  \n",
       "26275 2019-12-31 19:00:00  \n",
       "26276 2019-12-31 20:00:00  \n",
       "26277 2019-12-31 21:00:00  \n",
       "26278 2019-12-31 22:00:00  \n",
       "26279 2019-12-31 23:00:00  \n",
       "\n",
       "[26280 rows x 6 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather['date'] = pd.to_datetime(weather['dt'], unit='s')\n",
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4de43954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only interested in weather from 2018\n",
    "weather = weather.loc[weather['date']<'2019']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "87f7ab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = weather.loc[weather['date']>'2018']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f8492a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>temp</th>\n",
       "      <th>visibility</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>weather_main</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1514768400</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-01 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1514772000</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1514775600</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1514779200</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1514782800</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8754</th>\n",
       "      <td>1546282800</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-12-31 19:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8755</th>\n",
       "      <td>1546286400</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-12-31 20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8756</th>\n",
       "      <td>1546290000</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-12-31 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8757</th>\n",
       "      <td>1546293600</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-12-31 22:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8758</th>\n",
       "      <td>1546297200</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-12-31 23:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8759 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              dt  temp  visibility  wind_speed  weather_main  \\\n",
       "0     1514768400     2          10           6             1   \n",
       "1     1514772000     2          10           6             0   \n",
       "2     1514775600     2          10           6             0   \n",
       "3     1514779200     2          10           6             0   \n",
       "4     1514782800     2          10           5             0   \n",
       "...          ...   ...         ...         ...           ...   \n",
       "8754  1546282800     3          10           1             0   \n",
       "8755  1546286400     3          10           0             0   \n",
       "8756  1546290000     3          10           0             0   \n",
       "8757  1546293600     3          10           1             0   \n",
       "8758  1546297200     3          10           1             0   \n",
       "\n",
       "                    date  \n",
       "0    2018-01-01 01:00:00  \n",
       "1    2018-01-01 02:00:00  \n",
       "2    2018-01-01 03:00:00  \n",
       "3    2018-01-01 04:00:00  \n",
       "4    2018-01-01 05:00:00  \n",
       "...                  ...  \n",
       "8754 2018-12-31 19:00:00  \n",
       "8755 2018-12-31 20:00:00  \n",
       "8756 2018-12-31 21:00:00  \n",
       "8757 2018-12-31 22:00:00  \n",
       "8758 2018-12-31 23:00:00  \n",
       "\n",
       "[8759 rows x 6 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather=weather.reset_index(drop=True)\n",
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c62e2c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weather.to_csv('./weather_factorised.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae65ba0",
   "metadata": {},
   "source": [
    "# Bus Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4a8e4fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.75 ms, sys: 2.68 ms, total: 4.43 ms\n",
      "Wall time: 29.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trips = pd.read_csv('rt_trips_DB_2018.txt', sep=\";\", chunksize=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0783fffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.io.parsers.readers.TextFileReader'>\n"
     ]
    }
   ],
   "source": [
    "print(type(trips))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "de607ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for chunk in trips:\n",
    "#    print(chunk.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "987d5379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trips.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e876901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trips.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c865d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e9e1845e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.2 ms, sys: 728 µs, total: 21.9 ms\n",
      "Wall time: 59.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trips = dd.read_csv('rt_trips_DB_2018.txt', sep=\";\", dtype={'DATASOURCE': 'category', 'DAYOFSERVICE': 'category', 'TRIPID': 'int32', 'LINEID': 'category', 'ROUTEID': 'category', 'DIRECTION': 'category', 'PLANNEDTIME_ARR': 'float64', 'PLANNEDTIME_DEP': 'float64', 'ACTUALTIME_ARR': 'float64', 'ACTUALTIME_DEP': 'float64', 'BASIN': 'category', 'TENDERLOT': 'category', 'SUPPRESSED': 'category', 'JUSTIFICATIONID': 'string', 'LASTUPDATE': 'string', 'NOTE': 'string'})\n",
    "#trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7d185b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(trips))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6f5876e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    <class 'pandas.core.frame.DataFrame'>\n",
       "1    <class 'pandas.core.frame.DataFrame'>\n",
       "2    <class 'pandas.core.frame.DataFrame'>\n",
       "dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips.map_partitions(type).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a706845f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATASOURCE</th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>LINEID</th>\n",
       "      <th>ROUTEID</th>\n",
       "      <th>DIRECTION</th>\n",
       "      <th>PLANNEDTIME_ARR</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_ARR</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "      <th>BASIN</th>\n",
       "      <th>TENDERLOT</th>\n",
       "      <th>SUPPRESSED</th>\n",
       "      <th>JUSTIFICATIONID</th>\n",
       "      <th>LASTUPDATE</th>\n",
       "      <th>NOTE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>category[unknown]</td>\n",
       "      <td>category[unknown]</td>\n",
       "      <td>int32</td>\n",
       "      <td>category[unknown]</td>\n",
       "      <td>category[unknown]</td>\n",
       "      <td>category[unknown]</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>category[unknown]</td>\n",
       "      <td>category[unknown]</td>\n",
       "      <td>category[unknown]</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: read-csv, 3 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                      DATASOURCE       DAYOFSERVICE TRIPID             LINEID            ROUTEID          DIRECTION PLANNEDTIME_ARR PLANNEDTIME_DEP ACTUALTIME_ARR ACTUALTIME_DEP              BASIN          TENDERLOT         SUPPRESSED JUSTIFICATIONID LASTUPDATE    NOTE\n",
       "npartitions=3                                                                                                                                                                                                                                                                \n",
       "               category[unknown]  category[unknown]  int32  category[unknown]  category[unknown]  category[unknown]         float64         float64        float64        float64  category[unknown]  category[unknown]  category[unknown]          string     string  string\n",
       "                             ...                ...    ...                ...                ...                ...             ...             ...            ...            ...                ...                ...                ...             ...        ...     ...\n",
       "                             ...                ...    ...                ...                ...                ...             ...             ...            ...            ...                ...                ...                ...             ...        ...     ...\n",
       "                             ...                ...    ...                ...                ...                ...             ...             ...            ...            ...                ...                ...                ...             ...        ...     ...\n",
       "Dask Name: read-csv, 3 tasks"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "96d78101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATASOURCE</th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>LINEID</th>\n",
       "      <th>ROUTEID</th>\n",
       "      <th>DIRECTION</th>\n",
       "      <th>PLANNEDTIME_ARR</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_ARR</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "      <th>BASIN</th>\n",
       "      <th>TENDERLOT</th>\n",
       "      <th>SUPPRESSED</th>\n",
       "      <th>JUSTIFICATIONID</th>\n",
       "      <th>LASTUPDATE</th>\n",
       "      <th>NOTE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DB</td>\n",
       "      <td>07-FEB-18 00:00:00</td>\n",
       "      <td>6253783</td>\n",
       "      <td>68</td>\n",
       "      <td>68_80</td>\n",
       "      <td>1</td>\n",
       "      <td>87245.0</td>\n",
       "      <td>84600.0</td>\n",
       "      <td>87524.0</td>\n",
       "      <td>84600.0</td>\n",
       "      <td>BasDef</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>28-FEB-18 12:05:11</td>\n",
       "      <td>,2967409,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DB</td>\n",
       "      <td>07-FEB-18 00:00:00</td>\n",
       "      <td>6262138</td>\n",
       "      <td>25B</td>\n",
       "      <td>25B_271</td>\n",
       "      <td>2</td>\n",
       "      <td>30517.0</td>\n",
       "      <td>26460.0</td>\n",
       "      <td>32752.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BasDef</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>28-FEB-18 12:05:11</td>\n",
       "      <td>,2580260,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DB</td>\n",
       "      <td>07-FEB-18 00:00:00</td>\n",
       "      <td>6254942</td>\n",
       "      <td>45A</td>\n",
       "      <td>45A_70</td>\n",
       "      <td>2</td>\n",
       "      <td>35512.0</td>\n",
       "      <td>32100.0</td>\n",
       "      <td>36329.0</td>\n",
       "      <td>32082.0</td>\n",
       "      <td>BasDef</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>28-FEB-18 12:05:11</td>\n",
       "      <td>,2448968,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  DATASOURCE        DAYOFSERVICE   TRIPID LINEID  ROUTEID DIRECTION  \\\n",
       "0         DB  07-FEB-18 00:00:00  6253783     68    68_80        1    \n",
       "1         DB  07-FEB-18 00:00:00  6262138    25B  25B_271        2    \n",
       "2         DB  07-FEB-18 00:00:00  6254942    45A   45A_70        2    \n",
       "\n",
       "   PLANNEDTIME_ARR  PLANNEDTIME_DEP  ACTUALTIME_ARR  ACTUALTIME_DEP   BASIN  \\\n",
       "0          87245.0          84600.0         87524.0         84600.0  BasDef   \n",
       "1          30517.0          26460.0         32752.0             NaN  BasDef   \n",
       "2          35512.0          32100.0         36329.0         32082.0  BasDef   \n",
       "\n",
       "  TENDERLOT SUPPRESSED JUSTIFICATIONID          LASTUPDATE       NOTE  \n",
       "0       NaN        NaN            <NA>  28-FEB-18 12:05:11  ,2967409,  \n",
       "1       NaN        NaN            <NA>  28-FEB-18 12:05:11  ,2580260,  \n",
       "2       NaN        NaN            <NA>  28-FEB-18 12:05:11  ,2448968,  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6f37af05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATASOURCE</th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>LINEID</th>\n",
       "      <th>ROUTEID</th>\n",
       "      <th>DIRECTION</th>\n",
       "      <th>PLANNEDTIME_ARR</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_ARR</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "      <th>BASIN</th>\n",
       "      <th>TENDERLOT</th>\n",
       "      <th>SUPPRESSED</th>\n",
       "      <th>JUSTIFICATIONID</th>\n",
       "      <th>LASTUPDATE</th>\n",
       "      <th>NOTE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>727496</th>\n",
       "      <td>DB</td>\n",
       "      <td>14-MAY-18 00:00:00</td>\n",
       "      <td>6765486</td>\n",
       "      <td>33D</td>\n",
       "      <td>33D_62</td>\n",
       "      <td>2</td>\n",
       "      <td>29460.0</td>\n",
       "      <td>26400.0</td>\n",
       "      <td>29904.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BasDef</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>26-JUN-18 09:13:13</td>\n",
       "      <td>,3077688,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727497</th>\n",
       "      <td>DB</td>\n",
       "      <td>14-MAY-18 00:00:00</td>\n",
       "      <td>6764987</td>\n",
       "      <td>70</td>\n",
       "      <td>70_60</td>\n",
       "      <td>1</td>\n",
       "      <td>65277.0</td>\n",
       "      <td>60600.0</td>\n",
       "      <td>66341.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BasDef</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>26-JUN-18 09:13:13</td>\n",
       "      <td>,3208841,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727498</th>\n",
       "      <td>DB</td>\n",
       "      <td>14-MAY-18 00:00:00</td>\n",
       "      <td>6765012</td>\n",
       "      <td>27</td>\n",
       "      <td>27_19</td>\n",
       "      <td>1</td>\n",
       "      <td>47722.0</td>\n",
       "      <td>41700.0</td>\n",
       "      <td>47508.0</td>\n",
       "      <td>41642.0</td>\n",
       "      <td>BasDef</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>26-JUN-18 09:13:13</td>\n",
       "      <td>,2960092,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       DATASOURCE        DAYOFSERVICE   TRIPID LINEID ROUTEID DIRECTION  \\\n",
       "727496         DB  14-MAY-18 00:00:00  6765486    33D  33D_62        2    \n",
       "727497         DB  14-MAY-18 00:00:00  6764987     70   70_60        1    \n",
       "727498         DB  14-MAY-18 00:00:00  6765012     27   27_19        1    \n",
       "\n",
       "        PLANNEDTIME_ARR  PLANNEDTIME_DEP  ACTUALTIME_ARR  ACTUALTIME_DEP  \\\n",
       "727496          29460.0          26400.0         29904.0             NaN   \n",
       "727497          65277.0          60600.0         66341.0             NaN   \n",
       "727498          47722.0          41700.0         47508.0         41642.0   \n",
       "\n",
       "         BASIN TENDERLOT SUPPRESSED JUSTIFICATIONID          LASTUPDATE  \\\n",
       "727496  BasDef       NaN        NaN            <NA>  26-JUN-18 09:13:13   \n",
       "727497  BasDef       NaN        NaN            <NA>  26-JUN-18 09:13:13   \n",
       "727498  BasDef       NaN        NaN            <NA>  26-JUN-18 09:13:13   \n",
       "\n",
       "             NOTE  \n",
       "727496  ,3077688,  \n",
       "727497  ,3208841,  \n",
       "727498  ,2960092,  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6345650e",
   "metadata": {},
   "source": [
    "- multiple empty columns\n",
    "- multiple probably useless ones\n",
    "    - \"note\", \"last update\", \"basin\", \"LASTUPDATE\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d43ce9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = trips.drop({'TENDERLOT', \"SUPPRESSED\", \"JUSTIFICATIONID\", \"DATASOURCE\", \"LASTUPDATE\", \"NOTE\",\"BASIN\", \"ACTUALTIME_ARR\", \"PLANNEDTIME_ARR\"}, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c157d100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>LINEID</th>\n",
       "      <th>ROUTEID</th>\n",
       "      <th>DIRECTION</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>category[unknown]</td>\n",
       "      <td>int32</td>\n",
       "      <td>category[unknown]</td>\n",
       "      <td>category[unknown]</td>\n",
       "      <td>category[unknown]</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: drop_by_shallow_copy, 6 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                    DAYOFSERVICE TRIPID             LINEID            ROUTEID          DIRECTION PLANNEDTIME_DEP ACTUALTIME_DEP\n",
       "npartitions=3                                                                                                                  \n",
       "               category[unknown]  int32  category[unknown]  category[unknown]  category[unknown]         float64        float64\n",
       "                             ...    ...                ...                ...                ...             ...            ...\n",
       "                             ...    ...                ...                ...                ...             ...            ...\n",
       "                             ...    ...                ...                ...                ...             ...            ...\n",
       "Dask Name: drop_by_shallow_copy, 6 tasks"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bdc3c9c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>LINEID</th>\n",
       "      <th>ROUTEID</th>\n",
       "      <th>DIRECTION</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>07-FEB-18 00:00:00</td>\n",
       "      <td>6253783</td>\n",
       "      <td>68</td>\n",
       "      <td>68_80</td>\n",
       "      <td>1</td>\n",
       "      <td>84600.0</td>\n",
       "      <td>84600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>07-FEB-18 00:00:00</td>\n",
       "      <td>6262138</td>\n",
       "      <td>25B</td>\n",
       "      <td>25B_271</td>\n",
       "      <td>2</td>\n",
       "      <td>26460.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>07-FEB-18 00:00:00</td>\n",
       "      <td>6254942</td>\n",
       "      <td>45A</td>\n",
       "      <td>45A_70</td>\n",
       "      <td>2</td>\n",
       "      <td>32100.0</td>\n",
       "      <td>32082.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>07-FEB-18 00:00:00</td>\n",
       "      <td>6259460</td>\n",
       "      <td>25A</td>\n",
       "      <td>25A_273</td>\n",
       "      <td>1</td>\n",
       "      <td>54420.0</td>\n",
       "      <td>54443.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07-FEB-18 00:00:00</td>\n",
       "      <td>6253175</td>\n",
       "      <td>14</td>\n",
       "      <td>14_15</td>\n",
       "      <td>1</td>\n",
       "      <td>81600.0</td>\n",
       "      <td>81608.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727494</th>\n",
       "      <td>14-MAY-18 00:00:00</td>\n",
       "      <td>6765849</td>\n",
       "      <td>123</td>\n",
       "      <td>123_36</td>\n",
       "      <td>2</td>\n",
       "      <td>57840.0</td>\n",
       "      <td>57859.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727495</th>\n",
       "      <td>14-MAY-18 00:00:00</td>\n",
       "      <td>6765469</td>\n",
       "      <td>75</td>\n",
       "      <td>75_17</td>\n",
       "      <td>1</td>\n",
       "      <td>48600.0</td>\n",
       "      <td>48823.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727496</th>\n",
       "      <td>14-MAY-18 00:00:00</td>\n",
       "      <td>6765486</td>\n",
       "      <td>33D</td>\n",
       "      <td>33D_62</td>\n",
       "      <td>2</td>\n",
       "      <td>26400.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727497</th>\n",
       "      <td>14-MAY-18 00:00:00</td>\n",
       "      <td>6764987</td>\n",
       "      <td>70</td>\n",
       "      <td>70_60</td>\n",
       "      <td>1</td>\n",
       "      <td>60600.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727498</th>\n",
       "      <td>14-MAY-18 00:00:00</td>\n",
       "      <td>6765012</td>\n",
       "      <td>27</td>\n",
       "      <td>27_19</td>\n",
       "      <td>1</td>\n",
       "      <td>41700.0</td>\n",
       "      <td>41642.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2182637 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              DAYOFSERVICE   TRIPID LINEID  ROUTEID DIRECTION  \\\n",
       "0       07-FEB-18 00:00:00  6253783     68    68_80        1    \n",
       "1       07-FEB-18 00:00:00  6262138    25B  25B_271        2    \n",
       "2       07-FEB-18 00:00:00  6254942    45A   45A_70        2    \n",
       "3       07-FEB-18 00:00:00  6259460    25A  25A_273        1    \n",
       "4       07-FEB-18 00:00:00  6253175     14    14_15        1    \n",
       "...                    ...      ...    ...      ...       ...   \n",
       "727494  14-MAY-18 00:00:00  6765849    123   123_36        2    \n",
       "727495  14-MAY-18 00:00:00  6765469     75    75_17        1    \n",
       "727496  14-MAY-18 00:00:00  6765486    33D   33D_62        2    \n",
       "727497  14-MAY-18 00:00:00  6764987     70    70_60        1    \n",
       "727498  14-MAY-18 00:00:00  6765012     27    27_19        1    \n",
       "\n",
       "        PLANNEDTIME_DEP  ACTUALTIME_DEP  \n",
       "0               84600.0         84600.0  \n",
       "1               26460.0             NaN  \n",
       "2               32100.0         32082.0  \n",
       "3               54420.0         54443.0  \n",
       "4               81600.0         81608.0  \n",
       "...                 ...             ...  \n",
       "727494          57840.0         57859.0  \n",
       "727495          48600.0         48823.0  \n",
       "727496          26400.0             NaN  \n",
       "727497          60600.0             NaN  \n",
       "727498          41700.0         41642.0  \n",
       "\n",
       "[2182637 rows x 7 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "885019d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>LINEID</th>\n",
       "      <th>ROUTEID</th>\n",
       "      <th>DIRECTION</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>category[unknown]</td>\n",
       "      <td>int32</td>\n",
       "      <td>category[unknown]</td>\n",
       "      <td>category[unknown]</td>\n",
       "      <td>category[unknown]</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: drop_by_shallow_copy, 6 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                    DAYOFSERVICE TRIPID             LINEID            ROUTEID          DIRECTION PLANNEDTIME_DEP ACTUALTIME_DEP\n",
       "npartitions=3                                                                                                                  \n",
       "               category[unknown]  int32  category[unknown]  category[unknown]  category[unknown]         float64        float64\n",
       "                             ...    ...                ...                ...                ...             ...            ...\n",
       "                             ...    ...                ...                ...                ...             ...            ...\n",
       "                             ...    ...                ...                ...                ...             ...            ...\n",
       "Dask Name: drop_by_shallow_copy, 6 tasks"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "07dbc898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>LINEID</th>\n",
       "      <th>ROUTEID</th>\n",
       "      <th>DIRECTION</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>07-FEB-18 00:00:00</td>\n",
       "      <td>6253783</td>\n",
       "      <td>68</td>\n",
       "      <td>68_80</td>\n",
       "      <td>1</td>\n",
       "      <td>84600.0</td>\n",
       "      <td>84600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>07-FEB-18 00:00:00</td>\n",
       "      <td>6262138</td>\n",
       "      <td>25B</td>\n",
       "      <td>25B_271</td>\n",
       "      <td>2</td>\n",
       "      <td>26460.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>07-FEB-18 00:00:00</td>\n",
       "      <td>6254942</td>\n",
       "      <td>45A</td>\n",
       "      <td>45A_70</td>\n",
       "      <td>2</td>\n",
       "      <td>32100.0</td>\n",
       "      <td>32082.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         DAYOFSERVICE   TRIPID LINEID  ROUTEID DIRECTION  PLANNEDTIME_DEP  \\\n",
       "0  07-FEB-18 00:00:00  6253783     68    68_80        1           84600.0   \n",
       "1  07-FEB-18 00:00:00  6262138    25B  25B_271        2           26460.0   \n",
       "2  07-FEB-18 00:00:00  6254942    45A   45A_70        2           32100.0   \n",
       "\n",
       "   ACTUALTIME_DEP  \n",
       "0         84600.0  \n",
       "1             NaN  \n",
       "2         32082.0  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dd3e6bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>LINEID</th>\n",
       "      <th>ROUTEID</th>\n",
       "      <th>DIRECTION</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>category[unknown]</td>\n",
       "      <td>int32</td>\n",
       "      <td>category[unknown]</td>\n",
       "      <td>category[unknown]</td>\n",
       "      <td>category[unknown]</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: drop_by_shallow_copy, 6 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                    DAYOFSERVICE TRIPID             LINEID            ROUTEID          DIRECTION PLANNEDTIME_DEP ACTUALTIME_DEP\n",
       "npartitions=3                                                                                                                  \n",
       "               category[unknown]  int32  category[unknown]  category[unknown]  category[unknown]         float64        float64\n",
       "                             ...    ...                ...                ...                ...             ...            ...\n",
       "                             ...    ...                ...                ...                ...             ...            ...\n",
       "                             ...    ...                ...                ...                ...             ...            ...\n",
       "Dask Name: drop_by_shallow_copy, 6 tasks"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6aadf508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 7 entries, DAYOFSERVICE to ACTUALTIME_DEP\n",
      "dtypes: category(4), float64(2), int32(1)"
     ]
    }
   ],
   "source": [
    "trips.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c56d55a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: describe-numeric, 71 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                TRIPID PLANNEDTIME_DEP ACTUALTIME_DEP\n",
       "npartitions=1                                        \n",
       "               float64         float64        float64\n",
       "                   ...             ...            ...\n",
       "Dask Name: describe-numeric, 71 tasks"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "55fcaa49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>LINEID</th>\n",
       "      <th>ROUTEID</th>\n",
       "      <th>DIRECTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>07-FEB-18 00:00:00</td>\n",
       "      <td>6253783</td>\n",
       "      <td>68</td>\n",
       "      <td>68_80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>07-FEB-18 00:00:00</td>\n",
       "      <td>6262138</td>\n",
       "      <td>25B</td>\n",
       "      <td>25B_271</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>07-FEB-18 00:00:00</td>\n",
       "      <td>6254942</td>\n",
       "      <td>45A</td>\n",
       "      <td>45A_70</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>07-FEB-18 00:00:00</td>\n",
       "      <td>6259460</td>\n",
       "      <td>25A</td>\n",
       "      <td>25A_273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07-FEB-18 00:00:00</td>\n",
       "      <td>6253175</td>\n",
       "      <td>14</td>\n",
       "      <td>14_15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727494</th>\n",
       "      <td>14-MAY-18 00:00:00</td>\n",
       "      <td>6765849</td>\n",
       "      <td>123</td>\n",
       "      <td>123_36</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727495</th>\n",
       "      <td>14-MAY-18 00:00:00</td>\n",
       "      <td>6765469</td>\n",
       "      <td>75</td>\n",
       "      <td>75_17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727496</th>\n",
       "      <td>14-MAY-18 00:00:00</td>\n",
       "      <td>6765486</td>\n",
       "      <td>33D</td>\n",
       "      <td>33D_62</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727497</th>\n",
       "      <td>14-MAY-18 00:00:00</td>\n",
       "      <td>6764987</td>\n",
       "      <td>70</td>\n",
       "      <td>70_60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727498</th>\n",
       "      <td>14-MAY-18 00:00:00</td>\n",
       "      <td>6765012</td>\n",
       "      <td>27</td>\n",
       "      <td>27_19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2182637 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              DAYOFSERVICE   TRIPID LINEID  ROUTEID DIRECTION\n",
       "0       07-FEB-18 00:00:00  6253783     68    68_80        1 \n",
       "1       07-FEB-18 00:00:00  6262138    25B  25B_271        2 \n",
       "2       07-FEB-18 00:00:00  6254942    45A   45A_70        2 \n",
       "3       07-FEB-18 00:00:00  6259460    25A  25A_273        1 \n",
       "4       07-FEB-18 00:00:00  6253175     14    14_15        1 \n",
       "...                    ...      ...    ...      ...       ...\n",
       "727494  14-MAY-18 00:00:00  6765849    123   123_36        2 \n",
       "727495  14-MAY-18 00:00:00  6765469     75    75_17        1 \n",
       "727496  14-MAY-18 00:00:00  6765486    33D   33D_62        2 \n",
       "727497  14-MAY-18 00:00:00  6764987     70    70_60        1 \n",
       "727498  14-MAY-18 00:00:00  6765012     27    27_19        1 \n",
       "\n",
       "[2182637 rows x 5 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips.drop({\"PLANNEDTIME_DEP\", \"ACTUALTIME_DEP\"}, axis=1).compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d04d3793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(trips))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d6c21709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>LINEID</th>\n",
       "      <th>ROUTEID</th>\n",
       "      <th>DIRECTION</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>category[unknown]</td>\n",
       "      <td>int32</td>\n",
       "      <td>category[unknown]</td>\n",
       "      <td>category[unknown]</td>\n",
       "      <td>category[unknown]</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: drop_by_shallow_copy, 6 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                    DAYOFSERVICE TRIPID             LINEID            ROUTEID          DIRECTION PLANNEDTIME_DEP ACTUALTIME_DEP\n",
       "npartitions=3                                                                                                                  \n",
       "               category[unknown]  int32  category[unknown]  category[unknown]  category[unknown]         float64        float64\n",
       "                             ...    ...                ...                ...                ...             ...            ...\n",
       "                             ...    ...                ...                ...                ...             ...            ...\n",
       "                             ...    ...                ...                ...                ...             ...            ...\n",
       "Dask Name: drop_by_shallow_copy, 6 tasks"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "531f43d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>LINEID</th>\n",
       "      <th>ROUTEID</th>\n",
       "      <th>DIRECTION</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>07-FEB-18 00:00:00</td>\n",
       "      <td>6253783</td>\n",
       "      <td>68</td>\n",
       "      <td>68_80</td>\n",
       "      <td>1</td>\n",
       "      <td>84600.0</td>\n",
       "      <td>84600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>07-FEB-18 00:00:00</td>\n",
       "      <td>6262138</td>\n",
       "      <td>25B</td>\n",
       "      <td>25B_271</td>\n",
       "      <td>2</td>\n",
       "      <td>26460.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         DAYOFSERVICE   TRIPID LINEID  ROUTEID DIRECTION  PLANNEDTIME_DEP  \\\n",
       "0  07-FEB-18 00:00:00  6253783     68    68_80        1           84600.0   \n",
       "1  07-FEB-18 00:00:00  6262138    25B  25B_271        2           26460.0   \n",
       "\n",
       "   ACTUALTIME_DEP  \n",
       "0         84600.0  \n",
       "1             NaN  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845036dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b4c099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ee10dbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#leaveTimes = pd.read_csv('rt_leavetimes_DB_2018.txt', sep=\";\", chunksize=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "01a697cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#for chunk in leaveTimes:\n",
    "#    print(chunk.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "874f0499",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#leaveTimes = pd.read_csv('rt_leavetimes_DB_2018.txt', sep=\";\", nrows=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "25d0fb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#leaveTimes.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "22f6a9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.6 ms, sys: 4.94 ms, total: 28.6 ms\n",
      "Wall time: 53.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "leaveTimes = dd.read_csv('rt_leavetimes_DB_2018.txt', sep=\";\", dtype={'DATASOURCE': 'category', 'DAYOFSERVICE': 'category', 'PROGRNUMBER': 'int8', 'STOPPOINTID': 'int32','TRIPID': 'int32', 'LINEID': 'category', 'ROUTEID': 'category', 'VEHICLEID': 'category', 'PLANNEDTIME_ARR': 'float64', 'PLANNEDTIME_DEP': 'float64', 'ACTUALTIME_ARR': 'float64', 'ACTUALTIME_DEP': 'float64','PASSENGERSIN': 'category', 'PASSENGERSOUT': 'category', 'DISTANCE': 'category', 'SUPPRESSED': 'category', 'JUSTIFICATIONID': 'string', 'LASTUPDATE': 'string', 'NOTE': 'string'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "86d9c103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATASOURCE</th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>PROGRNUMBER</th>\n",
       "      <th>STOPPOINTID</th>\n",
       "      <th>PLANNEDTIME_ARR</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_ARR</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "      <th>VEHICLEID</th>\n",
       "      <th>PASSENGERS</th>\n",
       "      <th>PASSENGERSIN</th>\n",
       "      <th>PASSENGERSOUT</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>SUPPRESSED</th>\n",
       "      <th>JUSTIFICATIONID</th>\n",
       "      <th>LASTUPDATE</th>\n",
       "      <th>NOTE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DB</td>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>48030.0</td>\n",
       "      <td>48030.0</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>08-JAN-18 17:21:10</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DB</td>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5966674</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>54001.0</td>\n",
       "      <td>54001.0</td>\n",
       "      <td>54023.0</td>\n",
       "      <td>54023.0</td>\n",
       "      <td>2693267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>08-JAN-18 17:21:10</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DB</td>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5959105</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>60001.0</td>\n",
       "      <td>60001.0</td>\n",
       "      <td>59955.0</td>\n",
       "      <td>59955.0</td>\n",
       "      <td>2693263</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>08-JAN-18 17:21:10</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  DATASOURCE        DAYOFSERVICE   TRIPID  PROGRNUMBER  STOPPOINTID  \\\n",
       "0         DB  01-JAN-18 00:00:00  5972116           12          119   \n",
       "1         DB  01-JAN-18 00:00:00  5966674           12          119   \n",
       "2         DB  01-JAN-18 00:00:00  5959105           12          119   \n",
       "\n",
       "   PLANNEDTIME_ARR  PLANNEDTIME_DEP  ACTUALTIME_ARR  ACTUALTIME_DEP VEHICLEID  \\\n",
       "0          48030.0          48030.0         48012.0         48012.0   2693211   \n",
       "1          54001.0          54001.0         54023.0         54023.0   2693267   \n",
       "2          60001.0          60001.0         59955.0         59955.0   2693263   \n",
       "\n",
       "   PASSENGERS PASSENGERSIN PASSENGERSOUT DISTANCE SUPPRESSED JUSTIFICATIONID  \\\n",
       "0         NaN          NaN           NaN      NaN        NaN            <NA>   \n",
       "1         NaN          NaN           NaN      NaN        NaN            <NA>   \n",
       "2         NaN          NaN           NaN      NaN        NaN            <NA>   \n",
       "\n",
       "           LASTUPDATE  NOTE  \n",
       "0  08-JAN-18 17:21:10  <NA>  \n",
       "1  08-JAN-18 17:21:10  <NA>  \n",
       "2  08-JAN-18 17:21:10  <NA>  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaveTimes.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3c413893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dd.Scalar<series-..., dtype=int32>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaveTimes[\"STOPPOINTID\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "76e2b16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaveTimes = leaveTimes.drop({\"SUPPRESSED\", \"JUSTIFICATIONID\", \"DATASOURCE\", \"NOTE\", \"PASSENGERSOUT\", \"PASSENGERSIN\", \"PASSENGERS\", \"LASTUPDATE\" }, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fcd6aecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>PROGRNUMBER</th>\n",
       "      <th>STOPPOINTID</th>\n",
       "      <th>PLANNEDTIME_ARR</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_ARR</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "      <th>VEHICLEID</th>\n",
       "      <th>DISTANCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>48030.0</td>\n",
       "      <td>48030.0</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5966674</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>54001.0</td>\n",
       "      <td>54001.0</td>\n",
       "      <td>54023.0</td>\n",
       "      <td>54023.0</td>\n",
       "      <td>2693267</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5959105</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>60001.0</td>\n",
       "      <td>60001.0</td>\n",
       "      <td>59955.0</td>\n",
       "      <td>59955.0</td>\n",
       "      <td>2693263</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         DAYOFSERVICE   TRIPID  PROGRNUMBER  STOPPOINTID  PLANNEDTIME_ARR  \\\n",
       "0  01-JAN-18 00:00:00  5972116           12          119          48030.0   \n",
       "1  01-JAN-18 00:00:00  5966674           12          119          54001.0   \n",
       "2  01-JAN-18 00:00:00  5959105           12          119          60001.0   \n",
       "\n",
       "   PLANNEDTIME_DEP  ACTUALTIME_ARR  ACTUALTIME_DEP VEHICLEID DISTANCE  \n",
       "0          48030.0         48012.0         48012.0   2693211      NaN  \n",
       "1          54001.0         54023.0         54023.0   2693267      NaN  \n",
       "2          60001.0         59955.0         59955.0   2693263      NaN  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaveTimes.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5fbf73df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>PROGRNUMBER</th>\n",
       "      <th>STOPPOINTID</th>\n",
       "      <th>PLANNEDTIME_ARR</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_ARR</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "      <th>VEHICLEID</th>\n",
       "      <th>DISTANCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>664448</th>\n",
       "      <td>31-DEC-18 00:00:00</td>\n",
       "      <td>8586183</td>\n",
       "      <td>78</td>\n",
       "      <td>4383</td>\n",
       "      <td>51481.0</td>\n",
       "      <td>51481.0</td>\n",
       "      <td>52237.0</td>\n",
       "      <td>52283.0</td>\n",
       "      <td>2693229</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664449</th>\n",
       "      <td>31-DEC-18 00:00:00</td>\n",
       "      <td>8589374</td>\n",
       "      <td>23</td>\n",
       "      <td>7053</td>\n",
       "      <td>53659.0</td>\n",
       "      <td>53659.0</td>\n",
       "      <td>53525.0</td>\n",
       "      <td>53525.0</td>\n",
       "      <td>3265669</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664450</th>\n",
       "      <td>31-DEC-18 00:00:00</td>\n",
       "      <td>8589372</td>\n",
       "      <td>24</td>\n",
       "      <td>2088</td>\n",
       "      <td>46383.0</td>\n",
       "      <td>46383.0</td>\n",
       "      <td>46315.0</td>\n",
       "      <td>46325.0</td>\n",
       "      <td>3265669</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              DAYOFSERVICE   TRIPID  PROGRNUMBER  STOPPOINTID  \\\n",
       "664448  31-DEC-18 00:00:00  8586183           78         4383   \n",
       "664449  31-DEC-18 00:00:00  8589374           23         7053   \n",
       "664450  31-DEC-18 00:00:00  8589372           24         2088   \n",
       "\n",
       "        PLANNEDTIME_ARR  PLANNEDTIME_DEP  ACTUALTIME_ARR  ACTUALTIME_DEP  \\\n",
       "664448          51481.0          51481.0         52237.0         52283.0   \n",
       "664449          53659.0          53659.0         53525.0         53525.0   \n",
       "664450          46383.0          46383.0         46315.0         46325.0   \n",
       "\n",
       "       VEHICLEID DISTANCE  \n",
       "664448   2693229      NaN  \n",
       "664449   3265669      NaN  \n",
       "664450   3265669      NaN  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaveTimes.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "13f678d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>PROGRNUMBER</th>\n",
       "      <th>STOPPOINTID</th>\n",
       "      <th>PLANNEDTIME_ARR</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_ARR</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "      <th>VEHICLEID</th>\n",
       "      <th>DISTANCE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=176</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>category[unknown]</td>\n",
       "      <td>int32</td>\n",
       "      <td>int8</td>\n",
       "      <td>int32</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>category[unknown]</td>\n",
       "      <td>category[unknown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: drop_by_shallow_copy, 352 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                      DAYOFSERVICE TRIPID PROGRNUMBER STOPPOINTID PLANNEDTIME_ARR PLANNEDTIME_DEP ACTUALTIME_ARR ACTUALTIME_DEP          VEHICLEID           DISTANCE\n",
       "npartitions=176                                                                                                                                                      \n",
       "                 category[unknown]  int32        int8       int32         float64         float64        float64        float64  category[unknown]  category[unknown]\n",
       "                               ...    ...         ...         ...             ...             ...            ...            ...                ...                ...\n",
       "...                            ...    ...         ...         ...             ...             ...            ...            ...                ...                ...\n",
       "                               ...    ...         ...         ...             ...             ...            ...            ...                ...                ...\n",
       "                               ...    ...         ...         ...             ...             ...            ...            ...                ...                ...\n",
       "Dask Name: drop_by_shallow_copy, 352 tasks"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaveTimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "634f5253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drops distance and switches to pandas\n",
    "leaveTimes = leaveTimes.drop({\"DISTANCE\"}, axis=1).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "eb3d105d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>PROGRNUMBER</th>\n",
       "      <th>STOPPOINTID</th>\n",
       "      <th>PLANNEDTIME_ARR</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_ARR</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "      <th>VEHICLEID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>48030.0</td>\n",
       "      <td>48030.0</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>2693211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5966674</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>54001.0</td>\n",
       "      <td>54001.0</td>\n",
       "      <td>54023.0</td>\n",
       "      <td>54023.0</td>\n",
       "      <td>2693267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5959105</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>60001.0</td>\n",
       "      <td>60001.0</td>\n",
       "      <td>59955.0</td>\n",
       "      <td>59955.0</td>\n",
       "      <td>2693263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5966888</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>58801.0</td>\n",
       "      <td>58801.0</td>\n",
       "      <td>58771.0</td>\n",
       "      <td>58771.0</td>\n",
       "      <td>2693284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5965960</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>56401.0</td>\n",
       "      <td>56401.0</td>\n",
       "      <td>56309.0</td>\n",
       "      <td>56323.0</td>\n",
       "      <td>2693209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664446</th>\n",
       "      <td>31-DEC-18 00:00:00</td>\n",
       "      <td>8588153</td>\n",
       "      <td>78</td>\n",
       "      <td>4383</td>\n",
       "      <td>28605.0</td>\n",
       "      <td>28605.0</td>\n",
       "      <td>28998.0</td>\n",
       "      <td>29013.0</td>\n",
       "      <td>3265721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664447</th>\n",
       "      <td>31-DEC-18 00:00:00</td>\n",
       "      <td>8587459</td>\n",
       "      <td>78</td>\n",
       "      <td>4383</td>\n",
       "      <td>22695.0</td>\n",
       "      <td>22695.0</td>\n",
       "      <td>23247.0</td>\n",
       "      <td>23247.0</td>\n",
       "      <td>3265687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664448</th>\n",
       "      <td>31-DEC-18 00:00:00</td>\n",
       "      <td>8586183</td>\n",
       "      <td>78</td>\n",
       "      <td>4383</td>\n",
       "      <td>51481.0</td>\n",
       "      <td>51481.0</td>\n",
       "      <td>52237.0</td>\n",
       "      <td>52283.0</td>\n",
       "      <td>2693229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664449</th>\n",
       "      <td>31-DEC-18 00:00:00</td>\n",
       "      <td>8589374</td>\n",
       "      <td>23</td>\n",
       "      <td>7053</td>\n",
       "      <td>53659.0</td>\n",
       "      <td>53659.0</td>\n",
       "      <td>53525.0</td>\n",
       "      <td>53525.0</td>\n",
       "      <td>3265669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664450</th>\n",
       "      <td>31-DEC-18 00:00:00</td>\n",
       "      <td>8589372</td>\n",
       "      <td>24</td>\n",
       "      <td>2088</td>\n",
       "      <td>46383.0</td>\n",
       "      <td>46383.0</td>\n",
       "      <td>46315.0</td>\n",
       "      <td>46325.0</td>\n",
       "      <td>3265669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116949113 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              DAYOFSERVICE   TRIPID  PROGRNUMBER  STOPPOINTID  \\\n",
       "0       01-JAN-18 00:00:00  5972116           12          119   \n",
       "1       01-JAN-18 00:00:00  5966674           12          119   \n",
       "2       01-JAN-18 00:00:00  5959105           12          119   \n",
       "3       01-JAN-18 00:00:00  5966888           12          119   \n",
       "4       01-JAN-18 00:00:00  5965960           12          119   \n",
       "...                    ...      ...          ...          ...   \n",
       "664446  31-DEC-18 00:00:00  8588153           78         4383   \n",
       "664447  31-DEC-18 00:00:00  8587459           78         4383   \n",
       "664448  31-DEC-18 00:00:00  8586183           78         4383   \n",
       "664449  31-DEC-18 00:00:00  8589374           23         7053   \n",
       "664450  31-DEC-18 00:00:00  8589372           24         2088   \n",
       "\n",
       "        PLANNEDTIME_ARR  PLANNEDTIME_DEP  ACTUALTIME_ARR  ACTUALTIME_DEP  \\\n",
       "0               48030.0          48030.0         48012.0         48012.0   \n",
       "1               54001.0          54001.0         54023.0         54023.0   \n",
       "2               60001.0          60001.0         59955.0         59955.0   \n",
       "3               58801.0          58801.0         58771.0         58771.0   \n",
       "4               56401.0          56401.0         56309.0         56323.0   \n",
       "...                 ...              ...             ...             ...   \n",
       "664446          28605.0          28605.0         28998.0         29013.0   \n",
       "664447          22695.0          22695.0         23247.0         23247.0   \n",
       "664448          51481.0          51481.0         52237.0         52283.0   \n",
       "664449          53659.0          53659.0         53525.0         53525.0   \n",
       "664450          46383.0          46383.0         46315.0         46325.0   \n",
       "\n",
       "       VEHICLEID  \n",
       "0        2693211  \n",
       "1        2693267  \n",
       "2        2693263  \n",
       "3        2693284  \n",
       "4        2693209  \n",
       "...          ...  \n",
       "664446   3265721  \n",
       "664447   3265687  \n",
       "664448   2693229  \n",
       "664449   3265669  \n",
       "664450   3265669  \n",
       "\n",
       "[116949113 rows x 9 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaveTimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "410be323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116949113"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(leaveTimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "34c73252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>PROGRNUMBER</th>\n",
       "      <th>STOPPOINTID</th>\n",
       "      <th>PLANNEDTIME_ARR</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_ARR</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "      <th>VEHICLEID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>48030.0</td>\n",
       "      <td>48030.0</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>2693211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         DAYOFSERVICE   TRIPID  PROGRNUMBER  STOPPOINTID  PLANNEDTIME_ARR  \\\n",
       "0  01-JAN-18 00:00:00  5972116           12          119          48030.0   \n",
       "\n",
       "   PLANNEDTIME_DEP  ACTUALTIME_ARR  ACTUALTIME_DEP VEHICLEID  \n",
       "0          48030.0         48012.0         48012.0   2693211  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaveTimes.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "58875f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#leaveTimes= leaveTimes.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "542b8bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>PROGRNUMBER</th>\n",
       "      <th>STOPPOINTID</th>\n",
       "      <th>PLANNEDTIME_ARR</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_ARR</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "      <th>VEHICLEID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>664446</th>\n",
       "      <td>31-DEC-18 00:00:00</td>\n",
       "      <td>8588153</td>\n",
       "      <td>78</td>\n",
       "      <td>4383</td>\n",
       "      <td>28605.0</td>\n",
       "      <td>28605.0</td>\n",
       "      <td>28998.0</td>\n",
       "      <td>29013.0</td>\n",
       "      <td>3265721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664447</th>\n",
       "      <td>31-DEC-18 00:00:00</td>\n",
       "      <td>8587459</td>\n",
       "      <td>78</td>\n",
       "      <td>4383</td>\n",
       "      <td>22695.0</td>\n",
       "      <td>22695.0</td>\n",
       "      <td>23247.0</td>\n",
       "      <td>23247.0</td>\n",
       "      <td>3265687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664448</th>\n",
       "      <td>31-DEC-18 00:00:00</td>\n",
       "      <td>8586183</td>\n",
       "      <td>78</td>\n",
       "      <td>4383</td>\n",
       "      <td>51481.0</td>\n",
       "      <td>51481.0</td>\n",
       "      <td>52237.0</td>\n",
       "      <td>52283.0</td>\n",
       "      <td>2693229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664449</th>\n",
       "      <td>31-DEC-18 00:00:00</td>\n",
       "      <td>8589374</td>\n",
       "      <td>23</td>\n",
       "      <td>7053</td>\n",
       "      <td>53659.0</td>\n",
       "      <td>53659.0</td>\n",
       "      <td>53525.0</td>\n",
       "      <td>53525.0</td>\n",
       "      <td>3265669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664450</th>\n",
       "      <td>31-DEC-18 00:00:00</td>\n",
       "      <td>8589372</td>\n",
       "      <td>24</td>\n",
       "      <td>2088</td>\n",
       "      <td>46383.0</td>\n",
       "      <td>46383.0</td>\n",
       "      <td>46315.0</td>\n",
       "      <td>46325.0</td>\n",
       "      <td>3265669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              DAYOFSERVICE   TRIPID  PROGRNUMBER  STOPPOINTID  \\\n",
       "664446  31-DEC-18 00:00:00  8588153           78         4383   \n",
       "664447  31-DEC-18 00:00:00  8587459           78         4383   \n",
       "664448  31-DEC-18 00:00:00  8586183           78         4383   \n",
       "664449  31-DEC-18 00:00:00  8589374           23         7053   \n",
       "664450  31-DEC-18 00:00:00  8589372           24         2088   \n",
       "\n",
       "        PLANNEDTIME_ARR  PLANNEDTIME_DEP  ACTUALTIME_ARR  ACTUALTIME_DEP  \\\n",
       "664446          28605.0          28605.0         28998.0         29013.0   \n",
       "664447          22695.0          22695.0         23247.0         23247.0   \n",
       "664448          51481.0          51481.0         52237.0         52283.0   \n",
       "664449          53659.0          53659.0         53525.0         53525.0   \n",
       "664450          46383.0          46383.0         46315.0         46325.0   \n",
       "\n",
       "       VEHICLEID  \n",
       "664446   3265721  \n",
       "664447   3265687  \n",
       "664448   2693229  \n",
       "664449   3265669  \n",
       "664450   3265669  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaveTimes.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4f322b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 DAYOFSERVICE   TRIPID  PROGRNUMBER  STOPPOINTID  \\\n",
      "116949108  31-DEC-18 00:00:00  8588153           78         4383   \n",
      "116949109  31-DEC-18 00:00:00  8587459           78         4383   \n",
      "116949110  31-DEC-18 00:00:00  8586183           78         4383   \n",
      "116949111  31-DEC-18 00:00:00  8589374           23         7053   \n",
      "116949112  31-DEC-18 00:00:00  8589372           24         2088   \n",
      "\n",
      "           PLANNEDTIME_ARR  PLANNEDTIME_DEP  ACTUALTIME_ARR  ACTUALTIME_DEP  \\\n",
      "116949108          28605.0          28605.0         28998.0         29013.0   \n",
      "116949109          22695.0          22695.0         23247.0         23247.0   \n",
      "116949110          51481.0          51481.0         52237.0         52283.0   \n",
      "116949111          53659.0          53659.0         53525.0         53525.0   \n",
      "116949112          46383.0          46383.0         46315.0         46325.0   \n",
      "\n",
      "          VEHICLEID  \n",
      "116949108   3265721  \n",
      "116949109   3265687  \n",
      "116949110   2693229  \n",
      "116949111   3265669  \n",
      "116949112   3265669  \n"
     ]
    }
   ],
   "source": [
    "print(leaveTimes.reset_index(drop=True).tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f405138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaveTimes = leaveTimes.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "37c82357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>PROGRNUMBER</th>\n",
       "      <th>STOPPOINTID</th>\n",
       "      <th>PLANNEDTIME_ARR</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_ARR</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "      <th>VEHICLEID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>48030.0</td>\n",
       "      <td>48030.0</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>2693211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5966674</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>54001.0</td>\n",
       "      <td>54001.0</td>\n",
       "      <td>54023.0</td>\n",
       "      <td>54023.0</td>\n",
       "      <td>2693267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5959105</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>60001.0</td>\n",
       "      <td>60001.0</td>\n",
       "      <td>59955.0</td>\n",
       "      <td>59955.0</td>\n",
       "      <td>2693263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5966888</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>58801.0</td>\n",
       "      <td>58801.0</td>\n",
       "      <td>58771.0</td>\n",
       "      <td>58771.0</td>\n",
       "      <td>2693284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5965960</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>56401.0</td>\n",
       "      <td>56401.0</td>\n",
       "      <td>56309.0</td>\n",
       "      <td>56323.0</td>\n",
       "      <td>2693209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116949108</th>\n",
       "      <td>31-DEC-18 00:00:00</td>\n",
       "      <td>8588153</td>\n",
       "      <td>78</td>\n",
       "      <td>4383</td>\n",
       "      <td>28605.0</td>\n",
       "      <td>28605.0</td>\n",
       "      <td>28998.0</td>\n",
       "      <td>29013.0</td>\n",
       "      <td>3265721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116949109</th>\n",
       "      <td>31-DEC-18 00:00:00</td>\n",
       "      <td>8587459</td>\n",
       "      <td>78</td>\n",
       "      <td>4383</td>\n",
       "      <td>22695.0</td>\n",
       "      <td>22695.0</td>\n",
       "      <td>23247.0</td>\n",
       "      <td>23247.0</td>\n",
       "      <td>3265687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116949110</th>\n",
       "      <td>31-DEC-18 00:00:00</td>\n",
       "      <td>8586183</td>\n",
       "      <td>78</td>\n",
       "      <td>4383</td>\n",
       "      <td>51481.0</td>\n",
       "      <td>51481.0</td>\n",
       "      <td>52237.0</td>\n",
       "      <td>52283.0</td>\n",
       "      <td>2693229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116949111</th>\n",
       "      <td>31-DEC-18 00:00:00</td>\n",
       "      <td>8589374</td>\n",
       "      <td>23</td>\n",
       "      <td>7053</td>\n",
       "      <td>53659.0</td>\n",
       "      <td>53659.0</td>\n",
       "      <td>53525.0</td>\n",
       "      <td>53525.0</td>\n",
       "      <td>3265669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116949112</th>\n",
       "      <td>31-DEC-18 00:00:00</td>\n",
       "      <td>8589372</td>\n",
       "      <td>24</td>\n",
       "      <td>2088</td>\n",
       "      <td>46383.0</td>\n",
       "      <td>46383.0</td>\n",
       "      <td>46315.0</td>\n",
       "      <td>46325.0</td>\n",
       "      <td>3265669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116949113 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 DAYOFSERVICE   TRIPID  PROGRNUMBER  STOPPOINTID  \\\n",
       "0          01-JAN-18 00:00:00  5972116           12          119   \n",
       "1          01-JAN-18 00:00:00  5966674           12          119   \n",
       "2          01-JAN-18 00:00:00  5959105           12          119   \n",
       "3          01-JAN-18 00:00:00  5966888           12          119   \n",
       "4          01-JAN-18 00:00:00  5965960           12          119   \n",
       "...                       ...      ...          ...          ...   \n",
       "116949108  31-DEC-18 00:00:00  8588153           78         4383   \n",
       "116949109  31-DEC-18 00:00:00  8587459           78         4383   \n",
       "116949110  31-DEC-18 00:00:00  8586183           78         4383   \n",
       "116949111  31-DEC-18 00:00:00  8589374           23         7053   \n",
       "116949112  31-DEC-18 00:00:00  8589372           24         2088   \n",
       "\n",
       "           PLANNEDTIME_ARR  PLANNEDTIME_DEP  ACTUALTIME_ARR  ACTUALTIME_DEP  \\\n",
       "0                  48030.0          48030.0         48012.0         48012.0   \n",
       "1                  54001.0          54001.0         54023.0         54023.0   \n",
       "2                  60001.0          60001.0         59955.0         59955.0   \n",
       "3                  58801.0          58801.0         58771.0         58771.0   \n",
       "4                  56401.0          56401.0         56309.0         56323.0   \n",
       "...                    ...              ...             ...             ...   \n",
       "116949108          28605.0          28605.0         28998.0         29013.0   \n",
       "116949109          22695.0          22695.0         23247.0         23247.0   \n",
       "116949110          51481.0          51481.0         52237.0         52283.0   \n",
       "116949111          53659.0          53659.0         53525.0         53525.0   \n",
       "116949112          46383.0          46383.0         46315.0         46325.0   \n",
       "\n",
       "          VEHICLEID  \n",
       "0           2693211  \n",
       "1           2693267  \n",
       "2           2693263  \n",
       "3           2693284  \n",
       "4           2693209  \n",
       "...             ...  \n",
       "116949108   3265721  \n",
       "116949109   3265687  \n",
       "116949110   2693229  \n",
       "116949111   3265669  \n",
       "116949112   3265669  \n",
       "\n",
       "[116949113 rows x 9 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaveTimes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17068bb8",
   "metadata": {},
   "source": [
    "- data is intact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "56f5cd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 116949113 entries, 0 to 116949112\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Dtype   \n",
      "---  ------           -----   \n",
      " 0   DAYOFSERVICE     category\n",
      " 1   TRIPID           int32   \n",
      " 2   PROGRNUMBER      int8    \n",
      " 3   STOPPOINTID      int32   \n",
      " 4   PLANNEDTIME_ARR  float64 \n",
      " 5   PLANNEDTIME_DEP  float64 \n",
      " 6   ACTUALTIME_ARR   float64 \n",
      " 7   ACTUALTIME_DEP   float64 \n",
      " 8   VEHICLEID        category\n",
      "dtypes: category(2), float64(4), int32(2), int8(1)\n",
      "memory usage: 4.9 GB\n"
     ]
    }
   ],
   "source": [
    "leaveTimes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "51f4b3a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'persist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [85]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m leaveTimes \u001b[38;5;241m=\u001b[39m \u001b[43mleaveTimes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpersist\u001b[49m()\n",
      "File \u001b[0;32m~/miniconda3/envs/comp47360py39/lib/python3.9/site-packages/pandas/core/generic.py:5575\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5569\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   5570\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   5571\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   5572\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5573\u001b[0m ):\n\u001b[1;32m   5574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 5575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'persist'"
     ]
    }
   ],
   "source": [
    "#leaveTimes = leaveTimes.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cf4e1e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(leaveTimes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cb3ee381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1e+03 ns, total: 6 µs\n",
      "Wall time: 11 µs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>PROGRNUMBER</th>\n",
       "      <th>STOPPOINTID</th>\n",
       "      <th>PLANNEDTIME_ARR</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_ARR</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "      <th>VEHICLEID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>48030.0</td>\n",
       "      <td>48030.0</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>2693211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5966674</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>54001.0</td>\n",
       "      <td>54001.0</td>\n",
       "      <td>54023.0</td>\n",
       "      <td>54023.0</td>\n",
       "      <td>2693267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5959105</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>60001.0</td>\n",
       "      <td>60001.0</td>\n",
       "      <td>59955.0</td>\n",
       "      <td>59955.0</td>\n",
       "      <td>2693263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5966888</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>58801.0</td>\n",
       "      <td>58801.0</td>\n",
       "      <td>58771.0</td>\n",
       "      <td>58771.0</td>\n",
       "      <td>2693284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5965960</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>56401.0</td>\n",
       "      <td>56401.0</td>\n",
       "      <td>56309.0</td>\n",
       "      <td>56323.0</td>\n",
       "      <td>2693209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116949108</th>\n",
       "      <td>31-DEC-18 00:00:00</td>\n",
       "      <td>8588153</td>\n",
       "      <td>78</td>\n",
       "      <td>4383</td>\n",
       "      <td>28605.0</td>\n",
       "      <td>28605.0</td>\n",
       "      <td>28998.0</td>\n",
       "      <td>29013.0</td>\n",
       "      <td>3265721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116949109</th>\n",
       "      <td>31-DEC-18 00:00:00</td>\n",
       "      <td>8587459</td>\n",
       "      <td>78</td>\n",
       "      <td>4383</td>\n",
       "      <td>22695.0</td>\n",
       "      <td>22695.0</td>\n",
       "      <td>23247.0</td>\n",
       "      <td>23247.0</td>\n",
       "      <td>3265687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116949110</th>\n",
       "      <td>31-DEC-18 00:00:00</td>\n",
       "      <td>8586183</td>\n",
       "      <td>78</td>\n",
       "      <td>4383</td>\n",
       "      <td>51481.0</td>\n",
       "      <td>51481.0</td>\n",
       "      <td>52237.0</td>\n",
       "      <td>52283.0</td>\n",
       "      <td>2693229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116949111</th>\n",
       "      <td>31-DEC-18 00:00:00</td>\n",
       "      <td>8589374</td>\n",
       "      <td>23</td>\n",
       "      <td>7053</td>\n",
       "      <td>53659.0</td>\n",
       "      <td>53659.0</td>\n",
       "      <td>53525.0</td>\n",
       "      <td>53525.0</td>\n",
       "      <td>3265669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116949112</th>\n",
       "      <td>31-DEC-18 00:00:00</td>\n",
       "      <td>8589372</td>\n",
       "      <td>24</td>\n",
       "      <td>2088</td>\n",
       "      <td>46383.0</td>\n",
       "      <td>46383.0</td>\n",
       "      <td>46315.0</td>\n",
       "      <td>46325.0</td>\n",
       "      <td>3265669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116949113 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 DAYOFSERVICE   TRIPID  PROGRNUMBER  STOPPOINTID  \\\n",
       "0          01-JAN-18 00:00:00  5972116           12          119   \n",
       "1          01-JAN-18 00:00:00  5966674           12          119   \n",
       "2          01-JAN-18 00:00:00  5959105           12          119   \n",
       "3          01-JAN-18 00:00:00  5966888           12          119   \n",
       "4          01-JAN-18 00:00:00  5965960           12          119   \n",
       "...                       ...      ...          ...          ...   \n",
       "116949108  31-DEC-18 00:00:00  8588153           78         4383   \n",
       "116949109  31-DEC-18 00:00:00  8587459           78         4383   \n",
       "116949110  31-DEC-18 00:00:00  8586183           78         4383   \n",
       "116949111  31-DEC-18 00:00:00  8589374           23         7053   \n",
       "116949112  31-DEC-18 00:00:00  8589372           24         2088   \n",
       "\n",
       "           PLANNEDTIME_ARR  PLANNEDTIME_DEP  ACTUALTIME_ARR  ACTUALTIME_DEP  \\\n",
       "0                  48030.0          48030.0         48012.0         48012.0   \n",
       "1                  54001.0          54001.0         54023.0         54023.0   \n",
       "2                  60001.0          60001.0         59955.0         59955.0   \n",
       "3                  58801.0          58801.0         58771.0         58771.0   \n",
       "4                  56401.0          56401.0         56309.0         56323.0   \n",
       "...                    ...              ...             ...             ...   \n",
       "116949108          28605.0          28605.0         28998.0         29013.0   \n",
       "116949109          22695.0          22695.0         23247.0         23247.0   \n",
       "116949110          51481.0          51481.0         52237.0         52283.0   \n",
       "116949111          53659.0          53659.0         53525.0         53525.0   \n",
       "116949112          46383.0          46383.0         46315.0         46325.0   \n",
       "\n",
       "          VEHICLEID  \n",
       "0           2693211  \n",
       "1           2693267  \n",
       "2           2693263  \n",
       "3           2693284  \n",
       "4           2693209  \n",
       "...             ...  \n",
       "116949108   3265721  \n",
       "116949109   3265687  \n",
       "116949110   2693229  \n",
       "116949111   3265669  \n",
       "116949112   3265669  \n",
       "\n",
       "[116949113 rows x 9 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "leaveTimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "554c519b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.58 ms, sys: 3.09 ms, total: 6.67 ms\n",
      "Wall time: 40.6 ms\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "#vehicles = pd.read_csv('rt_vehicles_DB_2018.txt', sep=\";\", chunksize=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "f69a2d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASOURCE          1\n",
      "DAYOFSERVICE      118\n",
      "VEHICLEID        1090\n",
      "DISTANCE        45302\n",
      "MINUTES         29651\n",
      "LASTUPDATE        118\n",
      "NOTE                0\n",
      "dtype: int64\n",
      "DATASOURCE          1\n",
      "DAYOFSERVICE       82\n",
      "VEHICLEID        1072\n",
      "DISTANCE        45363\n",
      "MINUTES         30012\n",
      "LASTUPDATE         82\n",
      "NOTE                0\n",
      "dtype: int64\n",
      "DATASOURCE          1\n",
      "DAYOFSERVICE       88\n",
      "VEHICLEID        1045\n",
      "DISTANCE        45379\n",
      "MINUTES         30237\n",
      "LASTUPDATE         88\n",
      "NOTE                0\n",
      "dtype: int64\n",
      "DATASOURCE          1\n",
      "DAYOFSERVICE       84\n",
      "VEHICLEID        1097\n",
      "DISTANCE        45353\n",
      "MINUTES         30474\n",
      "LASTUPDATE         84\n",
      "NOTE                0\n",
      "dtype: int64\n",
      "DATASOURCE          1\n",
      "DAYOFSERVICE       82\n",
      "VEHICLEID        1042\n",
      "DISTANCE        45349\n",
      "MINUTES         30126\n",
      "LASTUPDATE         82\n",
      "NOTE                0\n",
      "dtype: int64\n",
      "DATASOURCE          1\n",
      "DAYOFSERVICE       42\n",
      "VEHICLEID        1064\n",
      "DISTANCE        21607\n",
      "MINUTES         17842\n",
      "LASTUPDATE         42\n",
      "NOTE                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#for chunk in vehicles:\n",
    " #   print(chunk.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "7869c096",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vehicles_p = pd.read_csv('rt_vehicles_DB_2018.txt', sep=\";\", nrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "f95244a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATASOURCE</th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>VEHICLEID</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>MINUTES</th>\n",
       "      <th>LASTUPDATE</th>\n",
       "      <th>NOTE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DB</td>\n",
       "      <td>23-NOV-18 00:00:00</td>\n",
       "      <td>3303848</td>\n",
       "      <td>286166</td>\n",
       "      <td>58849</td>\n",
       "      <td>04-DEC-18 08:03:09</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DB</td>\n",
       "      <td>23-NOV-18 00:00:00</td>\n",
       "      <td>3303847</td>\n",
       "      <td>259545</td>\n",
       "      <td>56828</td>\n",
       "      <td>04-DEC-18 08:03:09</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DB</td>\n",
       "      <td>28-FEB-18 00:00:00</td>\n",
       "      <td>2868329</td>\n",
       "      <td>103096</td>\n",
       "      <td>40967</td>\n",
       "      <td>08-MAR-18 10:35:59</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DB</td>\n",
       "      <td>28-FEB-18 00:00:00</td>\n",
       "      <td>2868330</td>\n",
       "      <td>147277</td>\n",
       "      <td>43599</td>\n",
       "      <td>08-MAR-18 10:35:59</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DB</td>\n",
       "      <td>28-FEB-18 00:00:00</td>\n",
       "      <td>2868331</td>\n",
       "      <td>224682</td>\n",
       "      <td>40447</td>\n",
       "      <td>08-MAR-18 10:35:59</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DB</td>\n",
       "      <td>28-FEB-18 00:00:00</td>\n",
       "      <td>2868332</td>\n",
       "      <td>19499</td>\n",
       "      <td>6289</td>\n",
       "      <td>08-MAR-18 10:35:59</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DB</td>\n",
       "      <td>28-FEB-18 00:00:00</td>\n",
       "      <td>2868333</td>\n",
       "      <td>133014</td>\n",
       "      <td>43647</td>\n",
       "      <td>08-MAR-18 10:35:59</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DB</td>\n",
       "      <td>28-FEB-18 00:00:00</td>\n",
       "      <td>2868334</td>\n",
       "      <td>168964</td>\n",
       "      <td>47167</td>\n",
       "      <td>08-MAR-18 10:35:59</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DB</td>\n",
       "      <td>28-FEB-18 00:00:00</td>\n",
       "      <td>2868335</td>\n",
       "      <td>360842</td>\n",
       "      <td>55477</td>\n",
       "      <td>08-MAR-18 10:35:59</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DB</td>\n",
       "      <td>28-FEB-18 00:00:00</td>\n",
       "      <td>2868336</td>\n",
       "      <td>139959</td>\n",
       "      <td>39599</td>\n",
       "      <td>08-MAR-18 10:35:59</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DB</td>\n",
       "      <td>28-FEB-18 00:00:00</td>\n",
       "      <td>2868337</td>\n",
       "      <td>131071</td>\n",
       "      <td>38785</td>\n",
       "      <td>08-MAR-18 10:35:59</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>DB</td>\n",
       "      <td>28-FEB-18 00:00:00</td>\n",
       "      <td>2868338</td>\n",
       "      <td>249542</td>\n",
       "      <td>45036</td>\n",
       "      <td>08-MAR-18 10:35:59</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DB</td>\n",
       "      <td>28-FEB-18 00:00:00</td>\n",
       "      <td>2868339</td>\n",
       "      <td>202134</td>\n",
       "      <td>47154</td>\n",
       "      <td>08-MAR-18 10:35:59</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>DB</td>\n",
       "      <td>28-FEB-18 00:00:00</td>\n",
       "      <td>2868340</td>\n",
       "      <td>127213</td>\n",
       "      <td>40033</td>\n",
       "      <td>08-MAR-18 10:35:59</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DATASOURCE        DAYOFSERVICE  VEHICLEID  DISTANCE  MINUTES  \\\n",
       "0          DB  23-NOV-18 00:00:00    3303848    286166    58849   \n",
       "1          DB  23-NOV-18 00:00:00    3303847    259545    56828   \n",
       "2          DB  28-FEB-18 00:00:00    2868329    103096    40967   \n",
       "3          DB  28-FEB-18 00:00:00    2868330    147277    43599   \n",
       "4          DB  28-FEB-18 00:00:00    2868331    224682    40447   \n",
       "5          DB  28-FEB-18 00:00:00    2868332     19499     6289   \n",
       "6          DB  28-FEB-18 00:00:00    2868333    133014    43647   \n",
       "7          DB  28-FEB-18 00:00:00    2868334    168964    47167   \n",
       "8          DB  28-FEB-18 00:00:00    2868335    360842    55477   \n",
       "9          DB  28-FEB-18 00:00:00    2868336    139959    39599   \n",
       "10         DB  28-FEB-18 00:00:00    2868337    131071    38785   \n",
       "11         DB  28-FEB-18 00:00:00    2868338    249542    45036   \n",
       "12         DB  28-FEB-18 00:00:00    2868339    202134    47154   \n",
       "13         DB  28-FEB-18 00:00:00    2868340    127213    40033   \n",
       "\n",
       "            LASTUPDATE  NOTE  \n",
       "0   04-DEC-18 08:03:09   NaN  \n",
       "1   04-DEC-18 08:03:09   NaN  \n",
       "2   08-MAR-18 10:35:59   NaN  \n",
       "3   08-MAR-18 10:35:59   NaN  \n",
       "4   08-MAR-18 10:35:59   NaN  \n",
       "5   08-MAR-18 10:35:59   NaN  \n",
       "6   08-MAR-18 10:35:59   NaN  \n",
       "7   08-MAR-18 10:35:59   NaN  \n",
       "8   08-MAR-18 10:35:59   NaN  \n",
       "9   08-MAR-18 10:35:59   NaN  \n",
       "10  08-MAR-18 10:35:59   NaN  \n",
       "11  08-MAR-18 10:35:59   NaN  \n",
       "12  08-MAR-18 10:35:59   NaN  \n",
       "13  08-MAR-18 10:35:59   NaN  "
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vehicles_p.head(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "24ca3fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATASOURCE</th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>VEHICLEID</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>MINUTES</th>\n",
       "      <th>LASTUPDATE</th>\n",
       "      <th>NOTE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>DB</td>\n",
       "      <td>22-JUN-18 00:00:00</td>\n",
       "      <td>2868343</td>\n",
       "      <td>251644</td>\n",
       "      <td>45142</td>\n",
       "      <td>02-JUL-18 13:32:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>DB</td>\n",
       "      <td>22-JUN-18 00:00:00</td>\n",
       "      <td>2868344</td>\n",
       "      <td>198033</td>\n",
       "      <td>51354</td>\n",
       "      <td>02-JUL-18 13:32:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>DB</td>\n",
       "      <td>22-JUN-18 00:00:00</td>\n",
       "      <td>2868345</td>\n",
       "      <td>220367</td>\n",
       "      <td>59893</td>\n",
       "      <td>02-JUL-18 13:32:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DATASOURCE        DAYOFSERVICE  VEHICLEID  DISTANCE  MINUTES  \\\n",
       "97         DB  22-JUN-18 00:00:00    2868343    251644    45142   \n",
       "98         DB  22-JUN-18 00:00:00    2868344    198033    51354   \n",
       "99         DB  22-JUN-18 00:00:00    2868345    220367    59893   \n",
       "\n",
       "            LASTUPDATE  NOTE  \n",
       "97  02-JUL-18 13:32:00   NaN  \n",
       "98  02-JUL-18 13:32:00   NaN  \n",
       "99  02-JUL-18 13:32:00   NaN  "
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vehicles_p.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "168f5e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.4 ms, sys: 6.63 ms, total: 17 ms\n",
      "Wall time: 289 ms\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "#vehicles = dd.read_csv('rt_vehicles_DB_2018.txt', sep=\";\", dtype={'DATASOURCE': 'category', 'DAYOFSERVICE': 'string', 'VEHICLEID': 'int32', 'DISTANCE': 'int32','MINUTES': 'int32', \"LASTUPDATE\": \"string\", \"NOTE\": \"string\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "ee40f362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATASOURCE</th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>VEHICLEID</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>MINUTES</th>\n",
       "      <th>LASTUPDATE</th>\n",
       "      <th>NOTE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>category[unknown]</td>\n",
       "      <td>string</td>\n",
       "      <td>int32</td>\n",
       "      <td>int32</td>\n",
       "      <td>int32</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: read-csv, 1 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                      DATASOURCE DAYOFSERVICE VEHICLEID DISTANCE MINUTES LASTUPDATE    NOTE\n",
       "npartitions=1                                                                              \n",
       "               category[unknown]       string     int32    int32   int32     string  string\n",
       "                             ...          ...       ...      ...     ...        ...     ...\n",
       "Dask Name: read-csv, 1 tasks"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "a236ba56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATASOURCE</th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>VEHICLEID</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>MINUTES</th>\n",
       "      <th>LASTUPDATE</th>\n",
       "      <th>NOTE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DB</td>\n",
       "      <td>23-NOV-18 00:00:00</td>\n",
       "      <td>3303848</td>\n",
       "      <td>286166</td>\n",
       "      <td>58849</td>\n",
       "      <td>04-DEC-18 08:03:09</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DB</td>\n",
       "      <td>23-NOV-18 00:00:00</td>\n",
       "      <td>3303847</td>\n",
       "      <td>259545</td>\n",
       "      <td>56828</td>\n",
       "      <td>04-DEC-18 08:03:09</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DB</td>\n",
       "      <td>28-FEB-18 00:00:00</td>\n",
       "      <td>2868329</td>\n",
       "      <td>103096</td>\n",
       "      <td>40967</td>\n",
       "      <td>08-MAR-18 10:35:59</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  DATASOURCE        DAYOFSERVICE  VEHICLEID  DISTANCE  MINUTES  \\\n",
       "0         DB  23-NOV-18 00:00:00    3303848    286166    58849   \n",
       "1         DB  23-NOV-18 00:00:00    3303847    259545    56828   \n",
       "2         DB  28-FEB-18 00:00:00    2868329    103096    40967   \n",
       "\n",
       "           LASTUPDATE  NOTE  \n",
       "0  04-DEC-18 08:03:09  <NA>  \n",
       "1  04-DEC-18 08:03:09  <NA>  \n",
       "2  08-MAR-18 10:35:59  <NA>  "
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vehicles.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "cb371bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DataFrame in module dask.dataframe.core object:\n",
      "\n",
      "class DataFrame(_Frame)\n",
      " |  DataFrame(dsk, name, meta, divisions)\n",
      " |  \n",
      " |  Parallel Pandas DataFrame\n",
      " |  \n",
      " |  Do not use this class directly.  Instead use functions like\n",
      " |  ``dd.read_csv``, ``dd.read_parquet``, or ``dd.from_pandas``.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  dsk: dict\n",
      " |      The dask graph to compute this DataFrame\n",
      " |  name: str\n",
      " |      The key prefix that specifies which keys in the dask comprise this\n",
      " |      particular DataFrame\n",
      " |  meta: pandas.DataFrame\n",
      " |      An empty ``pandas.DataFrame`` with names, dtypes, and index matching\n",
      " |      the expected output.\n",
      " |  divisions: tuple of index values\n",
      " |      Values along which we partition our blocks on the index\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DataFrame\n",
      " |      _Frame\n",
      " |      dask.base.DaskMethodsMixin\n",
      " |      dask.utils.OperatorMethodMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __array_wrap__(self, array, context=None)\n",
      " |  \n",
      " |  __contains__(self, key)\n",
      " |  \n",
      " |  __delitem__(self, key)\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, key)\n",
      " |  \n",
      " |  __getitem__(self, key)\n",
      " |  \n",
      " |  __init__(self, dsk, name, meta, divisions)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  __setattr__(self, key, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setitem__(self, key, value)\n",
      " |  \n",
      " |  add(self, other, axis='columns', level=None, fill_value=None)\n",
      " |      Get Addition of dataframe and other, element-wise (binary operator `add`).\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.add.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Equivalent to ``dataframe + other``, but with support to substitute a fill_value\n",
      " |      for missing data in one of the inputs. With reverse version, `radd`.\n",
      " |      \n",
      " |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `mod`, `pow`) to\n",
      " |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}\n",
      " |          Whether to compare by the index (0 or 'index') or columns\n",
      " |          (1 or 'columns'). For Series input, axis to match Series index on.\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      fill_value : float or None, default None\n",
      " |          Fill existing missing (NaN) values, and any new element needed for\n",
      " |          successful DataFrame alignment, with this value before computation.\n",
      " |          If data in both corresponding DataFrame locations is missing\n",
      " |          the result will be missing.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Result of the arithmetic operation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.add : Add DataFrames.\n",
      " |      DataFrame.sub : Subtract DataFrames.\n",
      " |      DataFrame.mul : Multiply DataFrames.\n",
      " |      DataFrame.div : Divide DataFrames (float division).\n",
      " |      DataFrame.truediv : Divide DataFrames (float division).\n",
      " |      DataFrame.floordiv : Divide DataFrames (integer division).\n",
      " |      DataFrame.mod : Calculate modulo (remainder after division).\n",
      " |      DataFrame.pow : Calculate exponential power.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'angles': [0, 3, 4],  # doctest: +SKIP\n",
      " |      ...                    'degrees': [360, 180, 360]},\n",
      " |      ...                   index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      360\n",
      " |      triangle        3      180\n",
      " |      rectangle       4      360\n",
      " |      \n",
      " |      Add a scalar with operator version which return the same\n",
      " |      results.\n",
      " |      \n",
      " |      >>> df + 1  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      >>> df.add(1)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      Divide by constant with reverse version.\n",
      " |      \n",
      " |      >>> df.div(10)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle        0.0     36.0\n",
      " |      triangle      0.3     18.0\n",
      " |      rectangle     0.4     36.0\n",
      " |      \n",
      " |      >>> df.rdiv(10)  # doctest: +SKIP\n",
      " |                   angles   degrees\n",
      " |      circle          inf  0.027778\n",
      " |      triangle   3.333333  0.055556\n",
      " |      rectangle  2.500000  0.027778\n",
      " |      \n",
      " |      Subtract a list and Series by axis with operator version.\n",
      " |      \n",
      " |      >>> df - [1, 2]  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub([1, 2], axis='columns')  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),  # doctest: +SKIP\n",
      " |      ...        axis='index')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      359\n",
      " |      triangle        2      179\n",
      " |      rectangle       3      359\n",
      " |      \n",
      " |      Multiply a DataFrame of different shape with operator version.\n",
      " |      \n",
      " |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},  # doctest: +SKIP\n",
      " |      ...                      index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> other  # doctest: +SKIP\n",
      " |                 angles\n",
      " |      circle          0\n",
      " |      triangle        3\n",
      " |      rectangle       4\n",
      " |      \n",
      " |      >>> df * other  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      NaN\n",
      " |      triangle        9      NaN\n",
      " |      rectangle      16      NaN\n",
      " |      \n",
      " |      >>> df.mul(other, fill_value=0)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      0.0\n",
      " |      triangle        9      0.0\n",
      " |      rectangle      16      0.0\n",
      " |      \n",
      " |      Divide by a MultiIndex by level.\n",
      " |      \n",
      " |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],  # doctest: +SKIP\n",
      " |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n",
      " |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n",
      " |      ...                                    ['circle', 'triangle', 'rectangle',\n",
      " |      ...                                     'square', 'pentagon', 'hexagon']])\n",
      " |      >>> df_multindex  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle          0      360\n",
      " |        triangle        3      180\n",
      " |        rectangle       4      360\n",
      " |      B square          4      360\n",
      " |        pentagon        5      540\n",
      " |        hexagon         6      720\n",
      " |      \n",
      " |      >>> df.div(df_multindex, level=1, fill_value=0)  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle        NaN      1.0\n",
      " |        triangle      1.0      1.0\n",
      " |        rectangle     1.0      1.0\n",
      " |      B square        0.0      0.0\n",
      " |        pentagon      0.0      0.0\n",
      " |        hexagon       0.0      0.0\n",
      " |  \n",
      " |  append(self, other, interleave_partitions=False)\n",
      " |      Append rows of `other` to the end of caller, returning a new object.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.append.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      .. deprecated:: 1.4.0\n",
      " |          Use :func:`concat` instead. For further details see\n",
      " |          :ref:`whatsnew_140.deprecations.frame_series_append`\n",
      " |      \n",
      " |      Columns in `other` that are not in the caller are added as new columns.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : DataFrame or Series/dict-like object, or list of these\n",
      " |          The data to append.\n",
      " |      ignore_index : bool, default False  (Not supported in Dask)\n",
      " |          If True, the resulting axis will be labeled 0, 1, …, n - 1.\n",
      " |      verify_integrity : bool, default False  (Not supported in Dask)\n",
      " |          If True, raise ValueError on creating index with duplicates.\n",
      " |      sort : bool, default False  (Not supported in Dask)\n",
      " |          Sort columns if the columns of `self` and `other` are not aligned.\n",
      " |      \n",
      " |          .. versionchanged:: 1.0.0\n",
      " |      \n",
      " |              Changed to not sort by default.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          A new DataFrame consisting of the rows of caller and the rows of `other`.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      concat : General function to concatenate DataFrame or Series objects.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If a list of dict/series is passed and the keys are all contained in\n",
      " |      the DataFrame's index, the order of the columns in the resulting\n",
      " |      DataFrame will be unchanged.\n",
      " |      \n",
      " |      Iteratively appending rows to a DataFrame can be more computationally\n",
      " |      intensive than a single concatenate. A better solution is to append\n",
      " |      those rows to a list and then concatenate the list with the original\n",
      " |      DataFrame all at once.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'), index=['x', 'y'])  # doctest: +SKIP\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |         A  B\n",
      " |      x  1  2\n",
      " |      y  3  4\n",
      " |      >>> df2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'), index=['x', 'y'])  # doctest: +SKIP\n",
      " |      >>> df.append(df2)  # doctest: +SKIP\n",
      " |         A  B\n",
      " |      x  1  2\n",
      " |      y  3  4\n",
      " |      x  5  6\n",
      " |      y  7  8\n",
      " |      \n",
      " |      With `ignore_index` set to True:\n",
      " |      \n",
      " |      >>> df.append(df2, ignore_index=True)  # doctest: +SKIP\n",
      " |         A  B\n",
      " |      0  1  2\n",
      " |      1  3  4\n",
      " |      2  5  6\n",
      " |      3  7  8\n",
      " |      \n",
      " |      The following, while not recommended methods for generating DataFrames,\n",
      " |      show two ways to generate a DataFrame from multiple data sources.\n",
      " |      \n",
      " |      Less efficient:\n",
      " |      \n",
      " |      >>> df = pd.DataFrame(columns=['A'])  # doctest: +SKIP\n",
      " |      >>> for i in range(5):  # doctest: +SKIP\n",
      " |      ...     df = df.append({'A': i}, ignore_index=True)\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |         A\n",
      " |      0  0\n",
      " |      1  1\n",
      " |      2  2\n",
      " |      3  3\n",
      " |      4  4\n",
      " |      \n",
      " |      More efficient:\n",
      " |      \n",
      " |      >>> pd.concat([pd.DataFrame([i], columns=['A']) for i in range(5)],  # doctest: +SKIP\n",
      " |      ...           ignore_index=True)\n",
      " |         A\n",
      " |      0  0\n",
      " |      1  1\n",
      " |      2  2\n",
      " |      3  3\n",
      " |      4  4\n",
      " |  \n",
      " |  apply(self, func, axis=0, broadcast=None, raw=False, reduce=None, args=(), meta='__no_default__', result_type=None, **kwds)\n",
      " |      Parallel version of pandas.DataFrame.apply\n",
      " |      \n",
      " |      This mimics the pandas version except for the following:\n",
      " |      \n",
      " |      1.  Only ``axis=1`` is supported (and must be specified explicitly).\n",
      " |      2.  The user should provide output metadata via the `meta` keyword.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : function\n",
      " |          Function to apply to each column/row\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          - 0 or 'index': apply function to each column (NOT SUPPORTED)\n",
      " |          - 1 or 'columns': apply function to each row\n",
      " |      meta : pd.DataFrame, pd.Series, dict, iterable, tuple, optional\n",
      " |          An empty ``pd.DataFrame`` or ``pd.Series`` that matches the dtypes\n",
      " |          and column names of the output. This metadata is necessary for\n",
      " |          many algorithms in dask dataframe to work.  For ease of use, some\n",
      " |          alternative inputs are also available. Instead of a ``DataFrame``,\n",
      " |          a ``dict`` of ``{name: dtype}`` or iterable of ``(name, dtype)``\n",
      " |          can be provided (note that the order of the names should match the\n",
      " |          order of the columns). Instead of a series, a tuple of ``(name,\n",
      " |          dtype)`` can be used. If not provided, dask will try to infer the\n",
      " |          metadata. This may lead to unexpected results, so providing\n",
      " |          ``meta`` is recommended. For more information, see\n",
      " |          ``dask.dataframe.utils.make_meta``.\n",
      " |      args : tuple\n",
      " |          Positional arguments to pass to function in addition to the array/series\n",
      " |      \n",
      " |      Additional keyword arguments will be passed as keywords to the function\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      applied : Series or DataFrame\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import pandas as pd\n",
      " |      >>> import dask.dataframe as dd\n",
      " |      >>> df = pd.DataFrame({'x': [1, 2, 3, 4, 5],\n",
      " |      ...                    'y': [1., 2., 3., 4., 5.]})\n",
      " |      >>> ddf = dd.from_pandas(df, npartitions=2)\n",
      " |      \n",
      " |      Apply a function to row-wise passing in extra arguments in ``args`` and\n",
      " |      ``kwargs``:\n",
      " |      \n",
      " |      >>> def myadd(row, a, b=1):\n",
      " |      ...     return row.sum() + a + b\n",
      " |      >>> res = ddf.apply(myadd, axis=1, args=(2,), b=1.5)  # doctest: +SKIP\n",
      " |      \n",
      " |      By default, dask tries to infer the output metadata by running your\n",
      " |      provided function on some fake data. This works well in many cases, but\n",
      " |      can sometimes be expensive, or even fail. To avoid this, you can\n",
      " |      manually specify the output metadata with the ``meta`` keyword. This\n",
      " |      can be specified in many forms, for more information see\n",
      " |      ``dask.dataframe.utils.make_meta``.\n",
      " |      \n",
      " |      Here we specify the output is a Series with name ``'x'``, and dtype\n",
      " |      ``float64``:\n",
      " |      \n",
      " |      >>> res = ddf.apply(myadd, axis=1, args=(2,), b=1.5, meta=('x', 'f8'))\n",
      " |      \n",
      " |      In the case where the metadata doesn't change, you can also pass in\n",
      " |      the object itself directly:\n",
      " |      \n",
      " |      >>> res = ddf.apply(lambda row: row + 1, axis=1, meta=ddf)\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      dask.DataFrame.map_partitions\n",
      " |  \n",
      " |  applymap(self, func, meta='__no_default__')\n",
      " |      Apply a function to a Dataframe elementwise.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.applymap.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      This method applies a function that accepts and returns a scalar\n",
      " |      to every element of a DataFrame.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : callable\n",
      " |          Python function, returns a single value from a single value.\n",
      " |      na_action : {None, 'ignore'}, default None  (Not supported in Dask)\n",
      " |          If ‘ignore’, propagate NaN values, without passing them to func.\n",
      " |      \n",
      " |          .. versionadded:: 1.2\n",
      " |      \n",
      " |      **kwargs\n",
      " |          Additional keyword arguments to pass as keywords arguments to\n",
      " |          `func`.\n",
      " |      \n",
      " |          .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Transformed DataFrame.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.apply : Apply a function along input axis of DataFrame.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame([[1, 2.12], [3.356, 4.567]])  # doctest: +SKIP\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |             0      1\n",
      " |      0  1.000  2.120\n",
      " |      1  3.356  4.567\n",
      " |      \n",
      " |      >>> df.applymap(lambda x: len(str(x)))  # doctest: +SKIP\n",
      " |         0  1\n",
      " |      0  3  4\n",
      " |      1  5  5\n",
      " |      \n",
      " |      Like Series.map, NA values can be ignored:\n",
      " |      \n",
      " |      >>> df_copy = df.copy()  # doctest: +SKIP\n",
      " |      >>> df_copy.iloc[0, 0] = pd.NA  # doctest: +SKIP\n",
      " |      >>> df_copy.applymap(lambda x: len(str(x)), na_action='ignore')  # doctest: +SKIP\n",
      " |            0  1\n",
      " |      0  <NA>  4\n",
      " |      1     5  5\n",
      " |      \n",
      " |      Note that a vectorized version of `func` often exists, which will\n",
      " |      be much faster. You could square each number elementwise.\n",
      " |      \n",
      " |      >>> df.applymap(lambda x: x**2)  # doctest: +SKIP\n",
      " |                 0          1\n",
      " |      0   1.000000   4.494400\n",
      " |      1  11.262736  20.857489\n",
      " |      \n",
      " |      But it's better to avoid applymap in that case.\n",
      " |      \n",
      " |      >>> df ** 2  # doctest: +SKIP\n",
      " |                 0          1\n",
      " |      0   1.000000   4.494400\n",
      " |      1  11.262736  20.857489\n",
      " |  \n",
      " |  assign(self, **kwargs)\n",
      " |      Assign new columns to a DataFrame.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.assign.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Returns a new object with all original columns in addition to new ones.\n",
      " |      Existing columns that are re-assigned will be overwritten.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **kwargs : dict of {str: callable or Series}\n",
      " |          The column names are keywords. If the values are\n",
      " |          callable, they are computed on the DataFrame and\n",
      " |          assigned to the new columns. The callable must not\n",
      " |          change input DataFrame (though pandas doesn't check it).\n",
      " |          If the values are not callable, (e.g. a Series, scalar, or array),\n",
      " |          they are simply assigned.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          A new DataFrame with the new columns in addition to\n",
      " |          all the existing columns.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Assigning multiple columns within the same ``assign`` is possible.\n",
      " |      Later items in '\\*\\*kwargs' may refer to newly created or modified\n",
      " |      columns in 'df'; items are computed and assigned into 'df' in order.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'temp_c': [17.0, 25.0]},  # doctest: +SKIP\n",
      " |      ...                   index=['Portland', 'Berkeley'])\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |                temp_c\n",
      " |      Portland    17.0\n",
      " |      Berkeley    25.0\n",
      " |      \n",
      " |      Where the value is a callable, evaluated on `df`:\n",
      " |      \n",
      " |      >>> df.assign(temp_f=lambda x: x.temp_c * 9 / 5 + 32)  # doctest: +SKIP\n",
      " |                temp_c  temp_f\n",
      " |      Portland    17.0    62.6\n",
      " |      Berkeley    25.0    77.0\n",
      " |      \n",
      " |      Alternatively, the same behavior can be achieved by directly\n",
      " |      referencing an existing Series or sequence:\n",
      " |      \n",
      " |      >>> df.assign(temp_f=df['temp_c'] * 9 / 5 + 32)  # doctest: +SKIP\n",
      " |                temp_c  temp_f\n",
      " |      Portland    17.0    62.6\n",
      " |      Berkeley    25.0    77.0\n",
      " |      \n",
      " |      You can create multiple columns within the same assign where one\n",
      " |      of the columns depends on another one defined within the same assign:\n",
      " |      \n",
      " |      >>> df.assign(temp_f=lambda x: x['temp_c'] * 9 / 5 + 32,  # doctest: +SKIP\n",
      " |      ...           temp_k=lambda x: (x['temp_f'] +  459.67) * 5 / 9)\n",
      " |                temp_c  temp_f  temp_k\n",
      " |      Portland    17.0    62.6  290.15\n",
      " |      Berkeley    25.0    77.0  298.15\n",
      " |  \n",
      " |  categorize(df, columns=None, index=None, split_every=None, **kwargs)\n",
      " |      Convert columns of the DataFrame to category dtype.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      columns : list, optional\n",
      " |          A list of column names to convert to categoricals. By default any\n",
      " |          column with an object dtype is converted to a categorical, and any\n",
      " |          unknown categoricals are made known.\n",
      " |      index : bool, optional\n",
      " |          Whether to categorize the index. By default, object indices are\n",
      " |          converted to categorical, and unknown categorical indices are made\n",
      " |          known. Set True to always categorize the index, False to never.\n",
      " |      split_every : int, optional\n",
      " |          Group partitions into groups of this size while performing a\n",
      " |          tree-reduction. If set to False, no tree-reduction will be used.\n",
      " |          Default is 16.\n",
      " |      kwargs\n",
      " |          Keyword arguments are passed on to compute.\n",
      " |  \n",
      " |  clip(self, lower=None, upper=None, out=None)\n",
      " |      Trim values at input threshold(s).\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.clip.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Assigns values outside boundary to boundary values. Thresholds\n",
      " |      can be singular values or array like, and in the latter case\n",
      " |      the clipping is performed element-wise in the specified axis.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      lower : float or array-like, default None\n",
      " |          Minimum threshold value. All values below this\n",
      " |          threshold will be set to it. A missing\n",
      " |          threshold (e.g `NA`) will not clip the value.\n",
      " |      upper : float or array-like, default None\n",
      " |          Maximum threshold value. All values above this\n",
      " |          threshold will be set to it. A missing\n",
      " |          threshold (e.g `NA`) will not clip the value.\n",
      " |      axis : int or str axis name, optional  (Not supported in Dask)\n",
      " |          Align object with lower and upper along the given axis.\n",
      " |      inplace : bool, default False  (Not supported in Dask)\n",
      " |          Whether to perform the operation in place on the data.\n",
      " |      *args, **kwargs\n",
      " |          Additional keywords have no effect but might be accepted\n",
      " |          for compatibility with numpy.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame or None\n",
      " |          Same type as calling object with the values outside the\n",
      " |          clip boundaries replaced or None if ``inplace=True``.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.clip : Trim values at input threshold in series.\n",
      " |      DataFrame.clip : Trim values at input threshold in dataframe.\n",
      " |      numpy.clip : Clip (limit) the values in an array.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> data = {'col_0': [9, -3, 0, -1, 5], 'col_1': [-2, -7, 6, 8, -5]}  # doctest: +SKIP\n",
      " |      >>> df = pd.DataFrame(data)  # doctest: +SKIP\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |         col_0  col_1\n",
      " |      0      9     -2\n",
      " |      1     -3     -7\n",
      " |      2      0      6\n",
      " |      3     -1      8\n",
      " |      4      5     -5\n",
      " |      \n",
      " |      Clips per column using lower and upper thresholds:\n",
      " |      \n",
      " |      >>> df.clip(-4, 6)  # doctest: +SKIP\n",
      " |         col_0  col_1\n",
      " |      0      6     -2\n",
      " |      1     -3     -4\n",
      " |      2      0      6\n",
      " |      3     -1      6\n",
      " |      4      5     -4\n",
      " |      \n",
      " |      Clips using specific lower and upper thresholds per column element:\n",
      " |      \n",
      " |      >>> t = pd.Series([2, -4, -1, 6, 3])  # doctest: +SKIP\n",
      " |      >>> t  # doctest: +SKIP\n",
      " |      0    2\n",
      " |      1   -4\n",
      " |      2   -1\n",
      " |      3    6\n",
      " |      4    3\n",
      " |      dtype: int64\n",
      " |      \n",
      " |      >>> df.clip(t, t + 4, axis=0)  # doctest: +SKIP\n",
      " |         col_0  col_1\n",
      " |      0      6      2\n",
      " |      1     -3     -4\n",
      " |      2      0      3\n",
      " |      3      6      8\n",
      " |      4      5      3\n",
      " |      \n",
      " |      Clips using specific lower threshold per column element, with missing values:\n",
      " |      \n",
      " |      >>> t = pd.Series([2, -4, np.NaN, 6, 3])  # doctest: +SKIP\n",
      " |      >>> t  # doctest: +SKIP\n",
      " |      0    2.0\n",
      " |      1   -4.0\n",
      " |      2    NaN\n",
      " |      3    6.0\n",
      " |      4    3.0\n",
      " |      dtype: float64\n",
      " |      \n",
      " |      >>> df.clip(t, axis=0)  # doctest: +SKIP\n",
      " |      col_0  col_1\n",
      " |      0      9      2\n",
      " |      1     -3     -4\n",
      " |      2      0      6\n",
      " |      3      6      8\n",
      " |      4      5      3\n",
      " |  \n",
      " |  clip_lower(self, threshold)\n",
      " |  \n",
      " |  clip_upper(self, threshold)\n",
      " |  \n",
      " |  corr(self, method='pearson', min_periods=None, split_every=False)\n",
      " |      Compute pairwise correlation of columns, excluding NA/null values.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.corr.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      method : {'pearson', 'kendall', 'spearman'} or callable\n",
      " |          Method of correlation:\n",
      " |      \n",
      " |          * pearson : standard correlation coefficient\n",
      " |          * kendall : Kendall Tau correlation coefficient\n",
      " |          * spearman : Spearman rank correlation\n",
      " |          * callable: callable with input two 1d ndarrays\n",
      " |              and returning a float. Note that the returned matrix from corr\n",
      " |              will have 1 along the diagonals and will be symmetric\n",
      " |              regardless of the callable's behavior.\n",
      " |      min_periods : int, optional\n",
      " |          Minimum number of observations required per pair of columns\n",
      " |          to have a valid result. Currently only available for Pearson\n",
      " |          and Spearman correlation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Correlation matrix.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.corrwith : Compute pairwise correlation with another\n",
      " |          DataFrame or Series.\n",
      " |      Series.corr : Compute the correlation between two Series.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> def histogram_intersection(a, b):  # doctest: +SKIP\n",
      " |      ...     v = np.minimum(a, b).sum().round(decimals=1)\n",
      " |      ...     return v\n",
      " |      >>> df = pd.DataFrame([(.2, .3), (.0, .6), (.6, .0), (.2, .1)],  # doctest: +SKIP\n",
      " |      ...                   columns=['dogs', 'cats'])\n",
      " |      >>> df.corr(method=histogram_intersection)  # doctest: +SKIP\n",
      " |            dogs  cats\n",
      " |      dogs   1.0   0.3\n",
      " |      cats   0.3   1.0\n",
      " |  \n",
      " |  cov(self, min_periods=None, split_every=False)\n",
      " |      Compute pairwise covariance of columns, excluding NA/null values.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.cov.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Compute the pairwise covariance among the series of a DataFrame.\n",
      " |      The returned data frame is the `covariance matrix\n",
      " |      <https://en.wikipedia.org/wiki/Covariance_matrix>`__ of the columns\n",
      " |      of the DataFrame.\n",
      " |      \n",
      " |      Both NA and null values are automatically excluded from the\n",
      " |      calculation. (See the note below about bias from missing values.)\n",
      " |      A threshold can be set for the minimum number of\n",
      " |      observations for each value created. Comparisons with observations\n",
      " |      below this threshold will be returned as ``NaN``.\n",
      " |      \n",
      " |      This method is generally used for the analysis of time series data to\n",
      " |      understand the relationship between different measures\n",
      " |      across time.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      min_periods : int, optional\n",
      " |          Minimum number of observations required per pair of columns\n",
      " |          to have a valid result.\n",
      " |      \n",
      " |      ddof : int, default 1  (Not supported in Dask)\n",
      " |          Delta degrees of freedom.  The divisor used in calculations\n",
      " |          is ``N - ddof``, where ``N`` represents the number of elements.\n",
      " |      \n",
      " |          .. versionadded:: 1.1.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          The covariance matrix of the series of the DataFrame.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.cov : Compute covariance with another Series.\n",
      " |      core.window.ExponentialMovingWindow.cov: Exponential weighted sample covariance.\n",
      " |      core.window.Expanding.cov : Expanding sample covariance.\n",
      " |      core.window.Rolling.cov : Rolling sample covariance.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Returns the covariance matrix of the DataFrame's time series.\n",
      " |      The covariance is normalized by N-ddof.\n",
      " |      \n",
      " |      For DataFrames that have Series that are missing data (assuming that\n",
      " |      data is `missing at random\n",
      " |      <https://en.wikipedia.org/wiki/Missing_data#Missing_at_random>`__)\n",
      " |      the returned covariance matrix will be an unbiased estimate\n",
      " |      of the variance and covariance between the member Series.\n",
      " |      \n",
      " |      However, for many applications this estimate may not be acceptable\n",
      " |      because the estimate covariance matrix is not guaranteed to be positive\n",
      " |      semi-definite. This could lead to estimate correlations having\n",
      " |      absolute values which are greater than one, and/or a non-invertible\n",
      " |      covariance matrix. See `Estimation of covariance matrices\n",
      " |      <https://en.wikipedia.org/w/index.php?title=Estimation_of_covariance_\n",
      " |      matrices>`__ for more details.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame([(1, 2), (0, 3), (2, 0), (1, 1)],  # doctest: +SKIP\n",
      " |      ...                   columns=['dogs', 'cats'])\n",
      " |      >>> df.cov()  # doctest: +SKIP\n",
      " |                dogs      cats\n",
      " |      dogs  0.666667 -1.000000\n",
      " |      cats -1.000000  1.666667\n",
      " |      \n",
      " |      >>> np.random.seed(42)  # doctest: +SKIP\n",
      " |      >>> df = pd.DataFrame(np.random.randn(1000, 5),  # doctest: +SKIP\n",
      " |      ...                   columns=['a', 'b', 'c', 'd', 'e'])\n",
      " |      >>> df.cov()  # doctest: +SKIP\n",
      " |                a         b         c         d         e\n",
      " |      a  0.998438 -0.020161  0.059277 -0.008943  0.014144\n",
      " |      b -0.020161  1.059352 -0.008543 -0.024738  0.009826\n",
      " |      c  0.059277 -0.008543  1.010670 -0.001486 -0.000271\n",
      " |      d -0.008943 -0.024738 -0.001486  0.921297 -0.013692\n",
      " |      e  0.014144  0.009826 -0.000271 -0.013692  0.977795\n",
      " |      \n",
      " |      **Minimum number of periods**\n",
      " |      \n",
      " |      This method also supports an optional ``min_periods`` keyword\n",
      " |      that specifies the required minimum number of non-NA observations for\n",
      " |      each column pair in order to have a valid result:\n",
      " |      \n",
      " |      >>> np.random.seed(42)  # doctest: +SKIP\n",
      " |      >>> df = pd.DataFrame(np.random.randn(20, 3),  # doctest: +SKIP\n",
      " |      ...                   columns=['a', 'b', 'c'])\n",
      " |      >>> df.loc[df.index[:5], 'a'] = np.nan  # doctest: +SKIP\n",
      " |      >>> df.loc[df.index[5:10], 'b'] = np.nan  # doctest: +SKIP\n",
      " |      >>> df.cov(min_periods=12)  # doctest: +SKIP\n",
      " |                a         b         c\n",
      " |      a  0.316741       NaN -0.150812\n",
      " |      b       NaN  1.248003  0.191417\n",
      " |      c -0.150812  0.191417  0.895202\n",
      " |  \n",
      " |  div(self, other, axis='columns', level=None, fill_value=None)\n",
      " |      Get Floating division of dataframe and other, element-wise (binary operator `truediv`).\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.div.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Equivalent to ``dataframe / other``, but with support to substitute a fill_value\n",
      " |      for missing data in one of the inputs. With reverse version, `rtruediv`.\n",
      " |      \n",
      " |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `mod`, `pow`) to\n",
      " |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}\n",
      " |          Whether to compare by the index (0 or 'index') or columns\n",
      " |          (1 or 'columns'). For Series input, axis to match Series index on.\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      fill_value : float or None, default None\n",
      " |          Fill existing missing (NaN) values, and any new element needed for\n",
      " |          successful DataFrame alignment, with this value before computation.\n",
      " |          If data in both corresponding DataFrame locations is missing\n",
      " |          the result will be missing.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Result of the arithmetic operation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.add : Add DataFrames.\n",
      " |      DataFrame.sub : Subtract DataFrames.\n",
      " |      DataFrame.mul : Multiply DataFrames.\n",
      " |      DataFrame.div : Divide DataFrames (float division).\n",
      " |      DataFrame.truediv : Divide DataFrames (float division).\n",
      " |      DataFrame.floordiv : Divide DataFrames (integer division).\n",
      " |      DataFrame.mod : Calculate modulo (remainder after division).\n",
      " |      DataFrame.pow : Calculate exponential power.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'angles': [0, 3, 4],  # doctest: +SKIP\n",
      " |      ...                    'degrees': [360, 180, 360]},\n",
      " |      ...                   index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      360\n",
      " |      triangle        3      180\n",
      " |      rectangle       4      360\n",
      " |      \n",
      " |      Add a scalar with operator version which return the same\n",
      " |      results.\n",
      " |      \n",
      " |      >>> df + 1  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      >>> df.add(1)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      Divide by constant with reverse version.\n",
      " |      \n",
      " |      >>> df.div(10)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle        0.0     36.0\n",
      " |      triangle      0.3     18.0\n",
      " |      rectangle     0.4     36.0\n",
      " |      \n",
      " |      >>> df.rdiv(10)  # doctest: +SKIP\n",
      " |                   angles   degrees\n",
      " |      circle          inf  0.027778\n",
      " |      triangle   3.333333  0.055556\n",
      " |      rectangle  2.500000  0.027778\n",
      " |      \n",
      " |      Subtract a list and Series by axis with operator version.\n",
      " |      \n",
      " |      >>> df - [1, 2]  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub([1, 2], axis='columns')  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),  # doctest: +SKIP\n",
      " |      ...        axis='index')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      359\n",
      " |      triangle        2      179\n",
      " |      rectangle       3      359\n",
      " |      \n",
      " |      Multiply a DataFrame of different shape with operator version.\n",
      " |      \n",
      " |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},  # doctest: +SKIP\n",
      " |      ...                      index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> other  # doctest: +SKIP\n",
      " |                 angles\n",
      " |      circle          0\n",
      " |      triangle        3\n",
      " |      rectangle       4\n",
      " |      \n",
      " |      >>> df * other  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      NaN\n",
      " |      triangle        9      NaN\n",
      " |      rectangle      16      NaN\n",
      " |      \n",
      " |      >>> df.mul(other, fill_value=0)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      0.0\n",
      " |      triangle        9      0.0\n",
      " |      rectangle      16      0.0\n",
      " |      \n",
      " |      Divide by a MultiIndex by level.\n",
      " |      \n",
      " |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],  # doctest: +SKIP\n",
      " |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n",
      " |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n",
      " |      ...                                    ['circle', 'triangle', 'rectangle',\n",
      " |      ...                                     'square', 'pentagon', 'hexagon']])\n",
      " |      >>> df_multindex  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle          0      360\n",
      " |        triangle        3      180\n",
      " |        rectangle       4      360\n",
      " |      B square          4      360\n",
      " |        pentagon        5      540\n",
      " |        hexagon         6      720\n",
      " |      \n",
      " |      >>> df.div(df_multindex, level=1, fill_value=0)  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle        NaN      1.0\n",
      " |        triangle      1.0      1.0\n",
      " |        rectangle     1.0      1.0\n",
      " |      B square        0.0      0.0\n",
      " |        pentagon      0.0      0.0\n",
      " |        hexagon       0.0      0.0\n",
      " |  \n",
      " |  divide(self, other, axis='columns', level=None, fill_value=None)\n",
      " |      Get Floating division of dataframe and other, element-wise (binary operator `truediv`).\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.divide.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Equivalent to ``dataframe / other``, but with support to substitute a fill_value\n",
      " |      for missing data in one of the inputs. With reverse version, `rtruediv`.\n",
      " |      \n",
      " |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `mod`, `pow`) to\n",
      " |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}\n",
      " |          Whether to compare by the index (0 or 'index') or columns\n",
      " |          (1 or 'columns'). For Series input, axis to match Series index on.\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      fill_value : float or None, default None\n",
      " |          Fill existing missing (NaN) values, and any new element needed for\n",
      " |          successful DataFrame alignment, with this value before computation.\n",
      " |          If data in both corresponding DataFrame locations is missing\n",
      " |          the result will be missing.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Result of the arithmetic operation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.add : Add DataFrames.\n",
      " |      DataFrame.sub : Subtract DataFrames.\n",
      " |      DataFrame.mul : Multiply DataFrames.\n",
      " |      DataFrame.div : Divide DataFrames (float division).\n",
      " |      DataFrame.truediv : Divide DataFrames (float division).\n",
      " |      DataFrame.floordiv : Divide DataFrames (integer division).\n",
      " |      DataFrame.mod : Calculate modulo (remainder after division).\n",
      " |      DataFrame.pow : Calculate exponential power.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'angles': [0, 3, 4],  # doctest: +SKIP\n",
      " |      ...                    'degrees': [360, 180, 360]},\n",
      " |      ...                   index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      360\n",
      " |      triangle        3      180\n",
      " |      rectangle       4      360\n",
      " |      \n",
      " |      Add a scalar with operator version which return the same\n",
      " |      results.\n",
      " |      \n",
      " |      >>> df + 1  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      >>> df.add(1)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      Divide by constant with reverse version.\n",
      " |      \n",
      " |      >>> df.div(10)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle        0.0     36.0\n",
      " |      triangle      0.3     18.0\n",
      " |      rectangle     0.4     36.0\n",
      " |      \n",
      " |      >>> df.rdiv(10)  # doctest: +SKIP\n",
      " |                   angles   degrees\n",
      " |      circle          inf  0.027778\n",
      " |      triangle   3.333333  0.055556\n",
      " |      rectangle  2.500000  0.027778\n",
      " |      \n",
      " |      Subtract a list and Series by axis with operator version.\n",
      " |      \n",
      " |      >>> df - [1, 2]  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub([1, 2], axis='columns')  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),  # doctest: +SKIP\n",
      " |      ...        axis='index')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      359\n",
      " |      triangle        2      179\n",
      " |      rectangle       3      359\n",
      " |      \n",
      " |      Multiply a DataFrame of different shape with operator version.\n",
      " |      \n",
      " |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},  # doctest: +SKIP\n",
      " |      ...                      index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> other  # doctest: +SKIP\n",
      " |                 angles\n",
      " |      circle          0\n",
      " |      triangle        3\n",
      " |      rectangle       4\n",
      " |      \n",
      " |      >>> df * other  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      NaN\n",
      " |      triangle        9      NaN\n",
      " |      rectangle      16      NaN\n",
      " |      \n",
      " |      >>> df.mul(other, fill_value=0)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      0.0\n",
      " |      triangle        9      0.0\n",
      " |      rectangle      16      0.0\n",
      " |      \n",
      " |      Divide by a MultiIndex by level.\n",
      " |      \n",
      " |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],  # doctest: +SKIP\n",
      " |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n",
      " |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n",
      " |      ...                                    ['circle', 'triangle', 'rectangle',\n",
      " |      ...                                     'square', 'pentagon', 'hexagon']])\n",
      " |      >>> df_multindex  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle          0      360\n",
      " |        triangle        3      180\n",
      " |        rectangle       4      360\n",
      " |      B square          4      360\n",
      " |        pentagon        5      540\n",
      " |        hexagon         6      720\n",
      " |      \n",
      " |      >>> df.div(df_multindex, level=1, fill_value=0)  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle        NaN      1.0\n",
      " |        triangle      1.0      1.0\n",
      " |        rectangle     1.0      1.0\n",
      " |      B square        0.0      0.0\n",
      " |        pentagon      0.0      0.0\n",
      " |        hexagon       0.0      0.0\n",
      " |  \n",
      " |  drop(self, labels=None, axis=0, columns=None, errors='raise')\n",
      " |      Drop specified labels from rows or columns.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.drop.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Remove rows or columns by specifying label names and corresponding\n",
      " |      axis, or by specifying directly index or column names. When using a\n",
      " |      multi-index, labels on different levels can be removed by specifying\n",
      " |      the level. See the `user guide <advanced.shown_levels>`\n",
      " |      for more information about the now unused levels.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      labels : single label or list-like\n",
      " |          Index or column labels to drop. A tuple will be used as a single\n",
      " |          label and not treated as a list-like.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          Whether to drop labels from the index (0 or 'index') or\n",
      " |          columns (1 or 'columns').\n",
      " |      index : single label or list-like  (Not supported in Dask)\n",
      " |          Alternative to specifying axis (``labels, axis=0``\n",
      " |          is equivalent to ``index=labels``).\n",
      " |      columns : single label or list-like\n",
      " |          Alternative to specifying axis (``labels, axis=1``\n",
      " |          is equivalent to ``columns=labels``).\n",
      " |      level : int or level name, optional  (Not supported in Dask)\n",
      " |          For MultiIndex, level from which the labels will be removed.\n",
      " |      inplace : bool, default False  (Not supported in Dask)\n",
      " |          If False, return a copy. Otherwise, do operation\n",
      " |          inplace and return None.\n",
      " |      errors : {'ignore', 'raise'}, default 'raise'\n",
      " |          If 'ignore', suppress error and only existing labels are\n",
      " |          dropped.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame or None\n",
      " |          DataFrame without the removed index or column labels or\n",
      " |          None if ``inplace=True``.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If any of the labels is not found in the selected axis.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.loc : Label-location based indexer for selection by label.\n",
      " |      DataFrame.dropna : Return DataFrame with labels on given axis omitted\n",
      " |          where (all or any) data are missing.\n",
      " |      DataFrame.drop_duplicates : Return DataFrame with duplicate rows\n",
      " |          removed, optionally only considering certain columns.\n",
      " |      Series.drop : Return Series with specified index labels removed.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame(np.arange(12).reshape(3, 4),  # doctest: +SKIP\n",
      " |      ...                   columns=['A', 'B', 'C', 'D'])\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |         A  B   C   D\n",
      " |      0  0  1   2   3\n",
      " |      1  4  5   6   7\n",
      " |      2  8  9  10  11\n",
      " |      \n",
      " |      Drop columns\n",
      " |      \n",
      " |      >>> df.drop(['B', 'C'], axis=1)  # doctest: +SKIP\n",
      " |         A   D\n",
      " |      0  0   3\n",
      " |      1  4   7\n",
      " |      2  8  11\n",
      " |      \n",
      " |      >>> df.drop(columns=['B', 'C'])  # doctest: +SKIP\n",
      " |         A   D\n",
      " |      0  0   3\n",
      " |      1  4   7\n",
      " |      2  8  11\n",
      " |      \n",
      " |      Drop a row by index\n",
      " |      \n",
      " |      >>> df.drop([0, 1])  # doctest: +SKIP\n",
      " |         A  B   C   D\n",
      " |      2  8  9  10  11\n",
      " |      \n",
      " |      Drop columns and/or rows of MultiIndex DataFrame\n",
      " |      \n",
      " |      >>> midx = pd.MultiIndex(levels=[['lama', 'cow', 'falcon'],  # doctest: +SKIP\n",
      " |      ...                              ['speed', 'weight', 'length']],\n",
      " |      ...                      codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2],\n",
      " |      ...                             [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n",
      " |      >>> df = pd.DataFrame(index=midx, columns=['big', 'small'],  # doctest: +SKIP\n",
      " |      ...                   data=[[45, 30], [200, 100], [1.5, 1], [30, 20],\n",
      " |      ...                         [250, 150], [1.5, 0.8], [320, 250],\n",
      " |      ...                         [1, 0.8], [0.3, 0.2]])\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |                      big     small\n",
      " |      lama    speed   45.0    30.0\n",
      " |              weight  200.0   100.0\n",
      " |              length  1.5     1.0\n",
      " |      cow     speed   30.0    20.0\n",
      " |              weight  250.0   150.0\n",
      " |              length  1.5     0.8\n",
      " |      falcon  speed   320.0   250.0\n",
      " |              weight  1.0     0.8\n",
      " |              length  0.3     0.2\n",
      " |      \n",
      " |      Drop a specific index combination from the MultiIndex\n",
      " |      DataFrame, i.e., drop the combination ``'falcon'`` and\n",
      " |      ``'weight'``, which deletes only the corresponding row\n",
      " |      \n",
      " |      >>> df.drop(index=('falcon', 'weight'))  # doctest: +SKIP\n",
      " |                      big     small\n",
      " |      lama    speed   45.0    30.0\n",
      " |              weight  200.0   100.0\n",
      " |              length  1.5     1.0\n",
      " |      cow     speed   30.0    20.0\n",
      " |              weight  250.0   150.0\n",
      " |              length  1.5     0.8\n",
      " |      falcon  speed   320.0   250.0\n",
      " |              length  0.3     0.2\n",
      " |      \n",
      " |      >>> df.drop(index='cow', columns='small')  # doctest: +SKIP\n",
      " |                      big\n",
      " |      lama    speed   45.0\n",
      " |              weight  200.0\n",
      " |              length  1.5\n",
      " |      falcon  speed   320.0\n",
      " |              weight  1.0\n",
      " |              length  0.3\n",
      " |      \n",
      " |      >>> df.drop(index='length', level=1)  # doctest: +SKIP\n",
      " |                      big     small\n",
      " |      lama    speed   45.0    30.0\n",
      " |              weight  200.0   100.0\n",
      " |      cow     speed   30.0    20.0\n",
      " |              weight  250.0   150.0\n",
      " |      falcon  speed   320.0   250.0\n",
      " |              weight  1.0     0.8\n",
      " |  \n",
      " |  dropna(self, how='any', subset=None, thresh=None)\n",
      " |      Remove missing values.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.dropna.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      See the :ref:`User Guide <missing_data>` for more on which values are\n",
      " |      considered missing, and how to work with missing data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0  (Not supported in Dask)\n",
      " |          Determine if rows or columns which contain missing values are\n",
      " |          removed.\n",
      " |      \n",
      " |          * 0, or 'index' : Drop rows which contain missing values.\n",
      " |          * 1, or 'columns' : Drop columns which contain missing value.\n",
      " |      \n",
      " |          .. versionchanged:: 1.0.0\n",
      " |      \n",
      " |             Pass tuple or list to drop on multiple axes.\n",
      " |             Only a single axis is allowed.\n",
      " |      \n",
      " |      how : {'any', 'all'}, default 'any'\n",
      " |          Determine if row or column is removed from DataFrame, when we have\n",
      " |          at least one NA or all NA.\n",
      " |      \n",
      " |          * 'any' : If any NA values are present, drop that row or column.\n",
      " |          * 'all' : If all values are NA, drop that row or column.\n",
      " |      \n",
      " |      thresh : int, optional\n",
      " |          Require that many non-NA values.\n",
      " |      subset : column label or sequence of labels, optional\n",
      " |          Labels along other axis to consider, e.g. if you are dropping rows\n",
      " |          these would be a list of columns to include.\n",
      " |      inplace : bool, default False  (Not supported in Dask)\n",
      " |          If True, do operation inplace and return None.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame or None\n",
      " |          DataFrame with NA entries dropped from it or None if ``inplace=True``.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.isna: Indicate missing values.\n",
      " |      DataFrame.notna : Indicate existing (non-missing) values.\n",
      " |      DataFrame.fillna : Replace missing values.\n",
      " |      Series.dropna : Drop missing values.\n",
      " |      Index.dropna : Drop missing indices.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({\"name\": ['Alfred', 'Batman', 'Catwoman'],  # doctest: +SKIP\n",
      " |      ...                    \"toy\": [np.nan, 'Batmobile', 'Bullwhip'],\n",
      " |      ...                    \"born\": [pd.NaT, pd.Timestamp(\"1940-04-25\"),\n",
      " |      ...                             pd.NaT]})\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |             name        toy       born\n",
      " |      0    Alfred        NaN        NaT\n",
      " |      1    Batman  Batmobile 1940-04-25\n",
      " |      2  Catwoman   Bullwhip        NaT\n",
      " |      \n",
      " |      Drop the rows where at least one element is missing.\n",
      " |      \n",
      " |      >>> df.dropna()  # doctest: +SKIP\n",
      " |           name        toy       born\n",
      " |      1  Batman  Batmobile 1940-04-25\n",
      " |      \n",
      " |      Drop the columns where at least one element is missing.\n",
      " |      \n",
      " |      >>> df.dropna(axis='columns')  # doctest: +SKIP\n",
      " |             name\n",
      " |      0    Alfred\n",
      " |      1    Batman\n",
      " |      2  Catwoman\n",
      " |      \n",
      " |      Drop the rows where all elements are missing.\n",
      " |      \n",
      " |      >>> df.dropna(how='all')  # doctest: +SKIP\n",
      " |             name        toy       born\n",
      " |      0    Alfred        NaN        NaT\n",
      " |      1    Batman  Batmobile 1940-04-25\n",
      " |      2  Catwoman   Bullwhip        NaT\n",
      " |      \n",
      " |      Keep only the rows with at least 2 non-NA values.\n",
      " |      \n",
      " |      >>> df.dropna(thresh=2)  # doctest: +SKIP\n",
      " |             name        toy       born\n",
      " |      1    Batman  Batmobile 1940-04-25\n",
      " |      2  Catwoman   Bullwhip        NaT\n",
      " |      \n",
      " |      Define in which columns to look for missing values.\n",
      " |      \n",
      " |      >>> df.dropna(subset=['name', 'toy'])  # doctest: +SKIP\n",
      " |             name        toy       born\n",
      " |      1    Batman  Batmobile 1940-04-25\n",
      " |      2  Catwoman   Bullwhip        NaT\n",
      " |      \n",
      " |      Keep the DataFrame with valid entries in the same variable.\n",
      " |      \n",
      " |      >>> df.dropna(inplace=True)  # doctest: +SKIP\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |           name        toy       born\n",
      " |      1  Batman  Batmobile 1940-04-25\n",
      " |  \n",
      " |  eq(self, other, axis='columns', level=None)\n",
      " |      Get Equal to of dataframe and other, element-wise (binary operator `eq`).\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.eq.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Among flexible wrappers (`eq`, `ne`, `le`, `lt`, `ge`, `gt`) to comparison\n",
      " |      operators.\n",
      " |      \n",
      " |      Equivalent to `==`, `!=`, `<=`, `<`, `>=`, `>` with support to choose axis\n",
      " |      (rows or columns) and level for comparison.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 'columns'\n",
      " |          Whether to compare by the index (0 or 'index') or columns\n",
      " |          (1 or 'columns').\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the passed\n",
      " |          MultiIndex level.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame of bool\n",
      " |          Result of the comparison.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.eq : Compare DataFrames for equality elementwise.\n",
      " |      DataFrame.ne : Compare DataFrames for inequality elementwise.\n",
      " |      DataFrame.le : Compare DataFrames for less than inequality\n",
      " |          or equality elementwise.\n",
      " |      DataFrame.lt : Compare DataFrames for strictly less than\n",
      " |          inequality elementwise.\n",
      " |      DataFrame.ge : Compare DataFrames for greater than inequality\n",
      " |          or equality elementwise.\n",
      " |      DataFrame.gt : Compare DataFrames for strictly greater than\n",
      " |          inequality elementwise.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |      `NaN` values are considered different (i.e. `NaN` != `NaN`).\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'cost': [250, 150, 100],  # doctest: +SKIP\n",
      " |      ...                    'revenue': [100, 250, 300]},\n",
      " |      ...                   index=['A', 'B', 'C'])\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |         cost  revenue\n",
      " |      A   250      100\n",
      " |      B   150      250\n",
      " |      C   100      300\n",
      " |      \n",
      " |      Comparison with a scalar, using either the operator or method:\n",
      " |      \n",
      " |      >>> df == 100  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A  False     True\n",
      " |      B  False    False\n",
      " |      C   True    False\n",
      " |      \n",
      " |      >>> df.eq(100)  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A  False     True\n",
      " |      B  False    False\n",
      " |      C   True    False\n",
      " |      \n",
      " |      When `other` is a :class:`Series`, the columns of a DataFrame are aligned\n",
      " |      with the index of `other` and broadcast:\n",
      " |      \n",
      " |      >>> df != pd.Series([100, 250], index=[\"cost\", \"revenue\"])  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A   True     True\n",
      " |      B   True    False\n",
      " |      C  False     True\n",
      " |      \n",
      " |      Use the method to control the broadcast axis:\n",
      " |      \n",
      " |      >>> df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index')  # doctest: +SKIP\n",
      " |         cost  revenue\n",
      " |      A  True    False\n",
      " |      B  True     True\n",
      " |      C  True     True\n",
      " |      D  True     True\n",
      " |      \n",
      " |      When comparing to an arbitrary sequence, the number of columns must\n",
      " |      match the number elements in `other`:\n",
      " |      \n",
      " |      >>> df == [250, 100]  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A   True     True\n",
      " |      B  False    False\n",
      " |      C  False    False\n",
      " |      \n",
      " |      Use the method to control the axis:\n",
      " |      \n",
      " |      >>> df.eq([250, 250, 100], axis='index')  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A   True    False\n",
      " |      B  False     True\n",
      " |      C   True    False\n",
      " |      \n",
      " |      Compare to a DataFrame of different shape.\n",
      " |      \n",
      " |      >>> other = pd.DataFrame({'revenue': [300, 250, 100, 150]},  # doctest: +SKIP\n",
      " |      ...                      index=['A', 'B', 'C', 'D'])\n",
      " |      >>> other  # doctest: +SKIP\n",
      " |         revenue\n",
      " |      A      300\n",
      " |      B      250\n",
      " |      C      100\n",
      " |      D      150\n",
      " |      \n",
      " |      >>> df.gt(other)  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A  False    False\n",
      " |      B  False    False\n",
      " |      C  False     True\n",
      " |      D  False    False\n",
      " |      \n",
      " |      Compare to a MultiIndex by level.\n",
      " |      \n",
      " |      >>> df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220],  # doctest: +SKIP\n",
      " |      ...                              'revenue': [100, 250, 300, 200, 175, 225]},\n",
      " |      ...                             index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'],\n",
      " |      ...                                    ['A', 'B', 'C', 'A', 'B', 'C']])\n",
      " |      >>> df_multindex  # doctest: +SKIP\n",
      " |            cost  revenue\n",
      " |      Q1 A   250      100\n",
      " |         B   150      250\n",
      " |         C   100      300\n",
      " |      Q2 A   150      200\n",
      " |         B   300      175\n",
      " |         C   220      225\n",
      " |      \n",
      " |      >>> df.le(df_multindex, level=1)  # doctest: +SKIP\n",
      " |             cost  revenue\n",
      " |      Q1 A   True     True\n",
      " |         B   True     True\n",
      " |         C   True     True\n",
      " |      Q2 A  False     True\n",
      " |         B   True    False\n",
      " |         C   True    False\n",
      " |  \n",
      " |  eval(self, expr, inplace=None, **kwargs)\n",
      " |      Evaluate a string describing operations on DataFrame columns.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.eval.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Operates on columns only, not specific rows or elements.  This allows\n",
      " |      `eval` to run arbitrary code, which can make you vulnerable to code\n",
      " |      injection if you pass user input to this function.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      expr : str\n",
      " |          The expression string to evaluate.\n",
      " |      inplace : bool, default False\n",
      " |          If the expression contains an assignment, whether to perform the\n",
      " |          operation inplace and mutate the existing DataFrame. Otherwise,\n",
      " |          a new DataFrame is returned.\n",
      " |      **kwargs\n",
      " |          See the documentation for :func:`eval` for complete details\n",
      " |          on the keyword arguments accepted by\n",
      " |          :meth:`~pandas.DataFrame.query`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ndarray, scalar, pandas object, or None\n",
      " |          The result of the evaluation or None if ``inplace=True``.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.query : Evaluates a boolean expression to query the columns\n",
      " |          of a frame.\n",
      " |      DataFrame.assign : Can evaluate an expression or function to create new\n",
      " |          values for a column.\n",
      " |      eval : Evaluate a Python expression as a string using various\n",
      " |          backends.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For more details see the API documentation for :func:`~eval`.\n",
      " |      For detailed examples see :ref:`enhancing performance with eval\n",
      " |      <enhancingperf.eval>`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'A': range(1, 6), 'B': range(10, 0, -2)})  # doctest: +SKIP\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |         A   B\n",
      " |      0  1  10\n",
      " |      1  2   8\n",
      " |      2  3   6\n",
      " |      3  4   4\n",
      " |      4  5   2\n",
      " |      >>> df.eval('A + B')  # doctest: +SKIP\n",
      " |      0    11\n",
      " |      1    10\n",
      " |      2     9\n",
      " |      3     8\n",
      " |      4     7\n",
      " |      dtype: int64\n",
      " |      \n",
      " |      Assignment is allowed though by default the original DataFrame is not\n",
      " |      modified.\n",
      " |      \n",
      " |      >>> df.eval('C = A + B')  # doctest: +SKIP\n",
      " |         A   B   C\n",
      " |      0  1  10  11\n",
      " |      1  2   8  10\n",
      " |      2  3   6   9\n",
      " |      3  4   4   8\n",
      " |      4  5   2   7\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |         A   B\n",
      " |      0  1  10\n",
      " |      1  2   8\n",
      " |      2  3   6\n",
      " |      3  4   4\n",
      " |      4  5   2\n",
      " |      \n",
      " |      Use ``inplace=True`` to modify the original DataFrame.\n",
      " |      \n",
      " |      >>> df.eval('C = A + B', inplace=True)  # doctest: +SKIP\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |         A   B   C\n",
      " |      0  1  10  11\n",
      " |      1  2   8  10\n",
      " |      2  3   6   9\n",
      " |      3  4   4   8\n",
      " |      4  5   2   7\n",
      " |      \n",
      " |      Multiple columns can be assigned to using multi-line expressions:\n",
      " |      \n",
      " |      >>> df.eval(  # doctest: +SKIP\n",
      " |      ...     '''\n",
      " |      ... C = A + B\n",
      " |      ... D = A - B\n",
      " |      ... '''\n",
      " |      ... )\n",
      " |         A   B   C  D\n",
      " |      0  1  10  11 -9\n",
      " |      1  2   8  10 -6\n",
      " |      2  3   6   9 -3\n",
      " |      3  4   4   8  0\n",
      " |      4  5   2   7  3\n",
      " |  \n",
      " |  explode(self, column)\n",
      " |      Transform each element of a list-like to a row, replicating index values.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.explode.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      .. versionadded:: 0.25.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      column : IndexLabel\n",
      " |          Column(s) to explode.\n",
      " |          For multiple columns, specify a non-empty list with each element\n",
      " |          be str or tuple, and all specified columns their list-like data\n",
      " |          on same row of the frame must have matching length.\n",
      " |      \n",
      " |          .. versionadded:: 1.3.0\n",
      " |              Multi-column explode\n",
      " |      \n",
      " |      ignore_index : bool, default False  (Not supported in Dask)\n",
      " |          If True, the resulting index will be labeled 0, 1, …, n - 1.\n",
      " |      \n",
      " |          .. versionadded:: 1.1.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Exploded lists to rows of the subset columns;\n",
      " |          index will be duplicated for these rows.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ValueError :\n",
      " |          * If columns of the frame are not unique.\n",
      " |          * If specified columns to explode is empty list.\n",
      " |          * If specified columns to explode have not matching count of\n",
      " |            elements rowwise in the frame.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.unstack : Pivot a level of the (necessarily hierarchical)\n",
      " |          index labels.\n",
      " |      DataFrame.melt : Unpivot a DataFrame from wide format to long format.\n",
      " |      Series.explode : Explode a DataFrame from list-like columns to long format.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This routine will explode list-likes including lists, tuples, sets,\n",
      " |      Series, and np.ndarray. The result dtype of the subset rows will\n",
      " |      be object. Scalars will be returned unchanged, and empty list-likes will\n",
      " |      result in a np.nan for that row. In addition, the ordering of rows in the\n",
      " |      output will be non-deterministic when exploding sets.\n",
      " |      \n",
      " |      Reference :ref:`the user guide <reshaping.explode>` for more examples.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'A': [[0, 1, 2], 'foo', [], [3, 4]],  # doctest: +SKIP\n",
      " |      ...                    'B': 1,\n",
      " |      ...                    'C': [['a', 'b', 'c'], np.nan, [], ['d', 'e']]})\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |                 A  B          C\n",
      " |      0  [0, 1, 2]  1  [a, b, c]\n",
      " |      1        foo  1        NaN\n",
      " |      2         []  1         []\n",
      " |      3     [3, 4]  1     [d, e]\n",
      " |      \n",
      " |      Single-column explode.\n",
      " |      \n",
      " |      >>> df.explode('A')  # doctest: +SKIP\n",
      " |           A  B          C\n",
      " |      0    0  1  [a, b, c]\n",
      " |      0    1  1  [a, b, c]\n",
      " |      0    2  1  [a, b, c]\n",
      " |      1  foo  1        NaN\n",
      " |      2  NaN  1         []\n",
      " |      3    3  1     [d, e]\n",
      " |      3    4  1     [d, e]\n",
      " |      \n",
      " |      Multi-column explode.\n",
      " |      \n",
      " |      >>> df.explode(list('AC'))  # doctest: +SKIP\n",
      " |           A  B    C\n",
      " |      0    0  1    a\n",
      " |      0    1  1    b\n",
      " |      0    2  1    c\n",
      " |      1  foo  1  NaN\n",
      " |      2  NaN  1  NaN\n",
      " |      3    3  1    d\n",
      " |      3    4  1    e\n",
      " |  \n",
      " |  floordiv(self, other, axis='columns', level=None, fill_value=None)\n",
      " |      Get Integer division of dataframe and other, element-wise (binary operator `floordiv`).\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.floordiv.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Equivalent to ``dataframe // other``, but with support to substitute a fill_value\n",
      " |      for missing data in one of the inputs. With reverse version, `rfloordiv`.\n",
      " |      \n",
      " |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `mod`, `pow`) to\n",
      " |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}\n",
      " |          Whether to compare by the index (0 or 'index') or columns\n",
      " |          (1 or 'columns'). For Series input, axis to match Series index on.\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      fill_value : float or None, default None\n",
      " |          Fill existing missing (NaN) values, and any new element needed for\n",
      " |          successful DataFrame alignment, with this value before computation.\n",
      " |          If data in both corresponding DataFrame locations is missing\n",
      " |          the result will be missing.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Result of the arithmetic operation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.add : Add DataFrames.\n",
      " |      DataFrame.sub : Subtract DataFrames.\n",
      " |      DataFrame.mul : Multiply DataFrames.\n",
      " |      DataFrame.div : Divide DataFrames (float division).\n",
      " |      DataFrame.truediv : Divide DataFrames (float division).\n",
      " |      DataFrame.floordiv : Divide DataFrames (integer division).\n",
      " |      DataFrame.mod : Calculate modulo (remainder after division).\n",
      " |      DataFrame.pow : Calculate exponential power.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'angles': [0, 3, 4],  # doctest: +SKIP\n",
      " |      ...                    'degrees': [360, 180, 360]},\n",
      " |      ...                   index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      360\n",
      " |      triangle        3      180\n",
      " |      rectangle       4      360\n",
      " |      \n",
      " |      Add a scalar with operator version which return the same\n",
      " |      results.\n",
      " |      \n",
      " |      >>> df + 1  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      >>> df.add(1)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      Divide by constant with reverse version.\n",
      " |      \n",
      " |      >>> df.div(10)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle        0.0     36.0\n",
      " |      triangle      0.3     18.0\n",
      " |      rectangle     0.4     36.0\n",
      " |      \n",
      " |      >>> df.rdiv(10)  # doctest: +SKIP\n",
      " |                   angles   degrees\n",
      " |      circle          inf  0.027778\n",
      " |      triangle   3.333333  0.055556\n",
      " |      rectangle  2.500000  0.027778\n",
      " |      \n",
      " |      Subtract a list and Series by axis with operator version.\n",
      " |      \n",
      " |      >>> df - [1, 2]  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub([1, 2], axis='columns')  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),  # doctest: +SKIP\n",
      " |      ...        axis='index')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      359\n",
      " |      triangle        2      179\n",
      " |      rectangle       3      359\n",
      " |      \n",
      " |      Multiply a DataFrame of different shape with operator version.\n",
      " |      \n",
      " |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},  # doctest: +SKIP\n",
      " |      ...                      index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> other  # doctest: +SKIP\n",
      " |                 angles\n",
      " |      circle          0\n",
      " |      triangle        3\n",
      " |      rectangle       4\n",
      " |      \n",
      " |      >>> df * other  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      NaN\n",
      " |      triangle        9      NaN\n",
      " |      rectangle      16      NaN\n",
      " |      \n",
      " |      >>> df.mul(other, fill_value=0)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      0.0\n",
      " |      triangle        9      0.0\n",
      " |      rectangle      16      0.0\n",
      " |      \n",
      " |      Divide by a MultiIndex by level.\n",
      " |      \n",
      " |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],  # doctest: +SKIP\n",
      " |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n",
      " |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n",
      " |      ...                                    ['circle', 'triangle', 'rectangle',\n",
      " |      ...                                     'square', 'pentagon', 'hexagon']])\n",
      " |      >>> df_multindex  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle          0      360\n",
      " |        triangle        3      180\n",
      " |        rectangle       4      360\n",
      " |      B square          4      360\n",
      " |        pentagon        5      540\n",
      " |        hexagon         6      720\n",
      " |      \n",
      " |      >>> df.div(df_multindex, level=1, fill_value=0)  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle        NaN      1.0\n",
      " |        triangle      1.0      1.0\n",
      " |        rectangle     1.0      1.0\n",
      " |      B square        0.0      0.0\n",
      " |        pentagon      0.0      0.0\n",
      " |        hexagon       0.0      0.0\n",
      " |  \n",
      " |  ge(self, other, axis='columns', level=None)\n",
      " |      Get Greater than or equal to of dataframe and other, element-wise (binary operator `ge`).\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.ge.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Among flexible wrappers (`eq`, `ne`, `le`, `lt`, `ge`, `gt`) to comparison\n",
      " |      operators.\n",
      " |      \n",
      " |      Equivalent to `==`, `!=`, `<=`, `<`, `>=`, `>` with support to choose axis\n",
      " |      (rows or columns) and level for comparison.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 'columns'\n",
      " |          Whether to compare by the index (0 or 'index') or columns\n",
      " |          (1 or 'columns').\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the passed\n",
      " |          MultiIndex level.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame of bool\n",
      " |          Result of the comparison.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.eq : Compare DataFrames for equality elementwise.\n",
      " |      DataFrame.ne : Compare DataFrames for inequality elementwise.\n",
      " |      DataFrame.le : Compare DataFrames for less than inequality\n",
      " |          or equality elementwise.\n",
      " |      DataFrame.lt : Compare DataFrames for strictly less than\n",
      " |          inequality elementwise.\n",
      " |      DataFrame.ge : Compare DataFrames for greater than inequality\n",
      " |          or equality elementwise.\n",
      " |      DataFrame.gt : Compare DataFrames for strictly greater than\n",
      " |          inequality elementwise.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |      `NaN` values are considered different (i.e. `NaN` != `NaN`).\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'cost': [250, 150, 100],  # doctest: +SKIP\n",
      " |      ...                    'revenue': [100, 250, 300]},\n",
      " |      ...                   index=['A', 'B', 'C'])\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |         cost  revenue\n",
      " |      A   250      100\n",
      " |      B   150      250\n",
      " |      C   100      300\n",
      " |      \n",
      " |      Comparison with a scalar, using either the operator or method:\n",
      " |      \n",
      " |      >>> df == 100  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A  False     True\n",
      " |      B  False    False\n",
      " |      C   True    False\n",
      " |      \n",
      " |      >>> df.eq(100)  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A  False     True\n",
      " |      B  False    False\n",
      " |      C   True    False\n",
      " |      \n",
      " |      When `other` is a :class:`Series`, the columns of a DataFrame are aligned\n",
      " |      with the index of `other` and broadcast:\n",
      " |      \n",
      " |      >>> df != pd.Series([100, 250], index=[\"cost\", \"revenue\"])  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A   True     True\n",
      " |      B   True    False\n",
      " |      C  False     True\n",
      " |      \n",
      " |      Use the method to control the broadcast axis:\n",
      " |      \n",
      " |      >>> df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index')  # doctest: +SKIP\n",
      " |         cost  revenue\n",
      " |      A  True    False\n",
      " |      B  True     True\n",
      " |      C  True     True\n",
      " |      D  True     True\n",
      " |      \n",
      " |      When comparing to an arbitrary sequence, the number of columns must\n",
      " |      match the number elements in `other`:\n",
      " |      \n",
      " |      >>> df == [250, 100]  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A   True     True\n",
      " |      B  False    False\n",
      " |      C  False    False\n",
      " |      \n",
      " |      Use the method to control the axis:\n",
      " |      \n",
      " |      >>> df.eq([250, 250, 100], axis='index')  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A   True    False\n",
      " |      B  False     True\n",
      " |      C   True    False\n",
      " |      \n",
      " |      Compare to a DataFrame of different shape.\n",
      " |      \n",
      " |      >>> other = pd.DataFrame({'revenue': [300, 250, 100, 150]},  # doctest: +SKIP\n",
      " |      ...                      index=['A', 'B', 'C', 'D'])\n",
      " |      >>> other  # doctest: +SKIP\n",
      " |         revenue\n",
      " |      A      300\n",
      " |      B      250\n",
      " |      C      100\n",
      " |      D      150\n",
      " |      \n",
      " |      >>> df.gt(other)  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A  False    False\n",
      " |      B  False    False\n",
      " |      C  False     True\n",
      " |      D  False    False\n",
      " |      \n",
      " |      Compare to a MultiIndex by level.\n",
      " |      \n",
      " |      >>> df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220],  # doctest: +SKIP\n",
      " |      ...                              'revenue': [100, 250, 300, 200, 175, 225]},\n",
      " |      ...                             index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'],\n",
      " |      ...                                    ['A', 'B', 'C', 'A', 'B', 'C']])\n",
      " |      >>> df_multindex  # doctest: +SKIP\n",
      " |            cost  revenue\n",
      " |      Q1 A   250      100\n",
      " |         B   150      250\n",
      " |         C   100      300\n",
      " |      Q2 A   150      200\n",
      " |         B   300      175\n",
      " |         C   220      225\n",
      " |      \n",
      " |      >>> df.le(df_multindex, level=1)  # doctest: +SKIP\n",
      " |             cost  revenue\n",
      " |      Q1 A   True     True\n",
      " |         B   True     True\n",
      " |         C   True     True\n",
      " |      Q2 A  False     True\n",
      " |         B   True    False\n",
      " |         C   True    False\n",
      " |  \n",
      " |  get_dtype_counts(self)\n",
      " |  \n",
      " |  get_ftype_counts(self)\n",
      " |  \n",
      " |  groupby(self, by=None, group_keys=True, sort=None, observed=None, dropna=None, **kwargs)\n",
      " |      Group DataFrame using a mapper or by a Series of columns.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.groupby.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      A groupby operation involves some combination of splitting the\n",
      " |      object, applying a function, and combining the results. This can be\n",
      " |      used to group large amounts of data and compute operations on these\n",
      " |      groups.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      by : mapping, function, label, or list of labels\n",
      " |          Used to determine the groups for the groupby.\n",
      " |          If ``by`` is a function, it's called on each value of the object's\n",
      " |          index. If a dict or Series is passed, the Series or dict VALUES\n",
      " |          will be used to determine the groups (the Series' values are first\n",
      " |          aligned; see ``.align()`` method). If a list or ndarray of length\n",
      " |          equal to the selected axis is passed (see the `groupby user guide\n",
      " |          <https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#splitting-an-object-into-groups>`_),\n",
      " |          the values are used as-is to determine the groups. A label or list\n",
      " |          of labels may be passed to group by the columns in ``self``.\n",
      " |          Notice that a tuple is interpreted as a (single) key.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0  (Not supported in Dask)\n",
      " |          Split along rows (0) or columns (1).\n",
      " |      level : int, level name, or sequence of such, default None  (Not supported in Dask)\n",
      " |          If the axis is a MultiIndex (hierarchical), group by a particular\n",
      " |          level or levels.\n",
      " |      as_index : bool, default True  (Not supported in Dask)\n",
      " |          For aggregated output, return object with group labels as the\n",
      " |          index. Only relevant for DataFrame input. as_index=False is\n",
      " |          effectively \"SQL-style\" grouped output.\n",
      " |      sort : bool, default True\n",
      " |          Sort group keys. Get better performance by turning this off.\n",
      " |          Note this does not influence the order of observations within each\n",
      " |          group. Groupby preserves the order of rows within each group.\n",
      " |      group_keys : bool, default True\n",
      " |          When calling apply, add group keys to index to identify pieces.\n",
      " |      squeeze : bool, default False  (Not supported in Dask)\n",
      " |          Reduce the dimensionality of the return type if possible,\n",
      " |          otherwise return a consistent type.\n",
      " |      \n",
      " |          .. deprecated:: 1.1.0\n",
      " |      \n",
      " |      observed : bool, default False\n",
      " |          This only applies if any of the groupers are Categoricals.\n",
      " |          If True: only show observed values for categorical groupers.\n",
      " |          If False: show all values for categorical groupers.\n",
      " |      dropna : bool, default True\n",
      " |          If True, and if group keys contain NA values, NA values together\n",
      " |          with row/column will be dropped.\n",
      " |          If False, NA values will also be treated as the key in groups.\n",
      " |      \n",
      " |          .. versionadded:: 1.1.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrameGroupBy\n",
      " |          Returns a groupby object that contains information about the groups.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      resample : Convenience method for frequency conversion and resampling\n",
      " |          of time series.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      See the `user guide\n",
      " |      <https://pandas.pydata.org/pandas-docs/stable/groupby.html>`__ for more\n",
      " |      detailed usage and examples, including splitting an object into groups,\n",
      " |      iterating through groups, selecting a group, aggregation, and more.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'Animal': ['Falcon', 'Falcon',  # doctest: +SKIP\n",
      " |      ...                               'Parrot', 'Parrot'],\n",
      " |      ...                    'Max Speed': [380., 370., 24., 26.]})\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |         Animal  Max Speed\n",
      " |      0  Falcon      380.0\n",
      " |      1  Falcon      370.0\n",
      " |      2  Parrot       24.0\n",
      " |      3  Parrot       26.0\n",
      " |      >>> df.groupby(['Animal']).mean()  # doctest: +SKIP\n",
      " |              Max Speed\n",
      " |      Animal\n",
      " |      Falcon      375.0\n",
      " |      Parrot       25.0\n",
      " |      \n",
      " |      **Hierarchical Indexes**\n",
      " |      \n",
      " |      We can groupby different levels of a hierarchical index\n",
      " |      using the `level` parameter:\n",
      " |      \n",
      " |      >>> arrays = [['Falcon', 'Falcon', 'Parrot', 'Parrot'],  # doctest: +SKIP\n",
      " |      ...           ['Captive', 'Wild', 'Captive', 'Wild']]\n",
      " |      >>> index = pd.MultiIndex.from_arrays(arrays, names=('Animal', 'Type'))  # doctest: +SKIP\n",
      " |      >>> df = pd.DataFrame({'Max Speed': [390., 350., 30., 20.]},  # doctest: +SKIP\n",
      " |      ...                   index=index)\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |                      Max Speed\n",
      " |      Animal Type\n",
      " |      Falcon Captive      390.0\n",
      " |             Wild         350.0\n",
      " |      Parrot Captive       30.0\n",
      " |             Wild          20.0\n",
      " |      >>> df.groupby(level=0).mean()  # doctest: +SKIP\n",
      " |              Max Speed\n",
      " |      Animal\n",
      " |      Falcon      370.0\n",
      " |      Parrot       25.0\n",
      " |      >>> df.groupby(level=\"Type\").mean()  # doctest: +SKIP\n",
      " |               Max Speed\n",
      " |      Type\n",
      " |      Captive      210.0\n",
      " |      Wild         185.0\n",
      " |      \n",
      " |      We can also choose to include NA in group keys or not by setting\n",
      " |      `dropna` parameter, the default setting is `True`.\n",
      " |      \n",
      " |      >>> l = [[1, 2, 3], [1, None, 4], [2, 1, 3], [1, 2, 2]]  # doctest: +SKIP\n",
      " |      >>> df = pd.DataFrame(l, columns=[\"a\", \"b\", \"c\"])  # doctest: +SKIP\n",
      " |      \n",
      " |      >>> df.groupby(by=[\"b\"]).sum()  # doctest: +SKIP\n",
      " |          a   c\n",
      " |      b\n",
      " |      1.0 2   3\n",
      " |      2.0 2   5\n",
      " |      \n",
      " |      >>> df.groupby(by=[\"b\"], dropna=False).sum()  # doctest: +SKIP\n",
      " |          a   c\n",
      " |      b\n",
      " |      1.0 2   3\n",
      " |      2.0 2   5\n",
      " |      NaN 1   4\n",
      " |      \n",
      " |      >>> l = [[\"a\", 12, 12], [None, 12.3, 33.], [\"b\", 12.3, 123], [\"a\", 1, 1]]  # doctest: +SKIP\n",
      " |      >>> df = pd.DataFrame(l, columns=[\"a\", \"b\", \"c\"])  # doctest: +SKIP\n",
      " |      \n",
      " |      >>> df.groupby(by=\"a\").sum()  # doctest: +SKIP\n",
      " |          b     c\n",
      " |      a\n",
      " |      a   13.0   13.0\n",
      " |      b   12.3  123.0\n",
      " |      \n",
      " |      >>> df.groupby(by=\"a\", dropna=False).sum()  # doctest: +SKIP\n",
      " |          b     c\n",
      " |      a\n",
      " |      a   13.0   13.0\n",
      " |      b   12.3  123.0\n",
      " |      NaN 12.3   33.0\n",
      " |  \n",
      " |  gt(self, other, axis='columns', level=None)\n",
      " |      Get Greater than of dataframe and other, element-wise (binary operator `gt`).\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.gt.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Among flexible wrappers (`eq`, `ne`, `le`, `lt`, `ge`, `gt`) to comparison\n",
      " |      operators.\n",
      " |      \n",
      " |      Equivalent to `==`, `!=`, `<=`, `<`, `>=`, `>` with support to choose axis\n",
      " |      (rows or columns) and level for comparison.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 'columns'\n",
      " |          Whether to compare by the index (0 or 'index') or columns\n",
      " |          (1 or 'columns').\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the passed\n",
      " |          MultiIndex level.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame of bool\n",
      " |          Result of the comparison.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.eq : Compare DataFrames for equality elementwise.\n",
      " |      DataFrame.ne : Compare DataFrames for inequality elementwise.\n",
      " |      DataFrame.le : Compare DataFrames for less than inequality\n",
      " |          or equality elementwise.\n",
      " |      DataFrame.lt : Compare DataFrames for strictly less than\n",
      " |          inequality elementwise.\n",
      " |      DataFrame.ge : Compare DataFrames for greater than inequality\n",
      " |          or equality elementwise.\n",
      " |      DataFrame.gt : Compare DataFrames for strictly greater than\n",
      " |          inequality elementwise.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |      `NaN` values are considered different (i.e. `NaN` != `NaN`).\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'cost': [250, 150, 100],  # doctest: +SKIP\n",
      " |      ...                    'revenue': [100, 250, 300]},\n",
      " |      ...                   index=['A', 'B', 'C'])\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |         cost  revenue\n",
      " |      A   250      100\n",
      " |      B   150      250\n",
      " |      C   100      300\n",
      " |      \n",
      " |      Comparison with a scalar, using either the operator or method:\n",
      " |      \n",
      " |      >>> df == 100  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A  False     True\n",
      " |      B  False    False\n",
      " |      C   True    False\n",
      " |      \n",
      " |      >>> df.eq(100)  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A  False     True\n",
      " |      B  False    False\n",
      " |      C   True    False\n",
      " |      \n",
      " |      When `other` is a :class:`Series`, the columns of a DataFrame are aligned\n",
      " |      with the index of `other` and broadcast:\n",
      " |      \n",
      " |      >>> df != pd.Series([100, 250], index=[\"cost\", \"revenue\"])  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A   True     True\n",
      " |      B   True    False\n",
      " |      C  False     True\n",
      " |      \n",
      " |      Use the method to control the broadcast axis:\n",
      " |      \n",
      " |      >>> df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index')  # doctest: +SKIP\n",
      " |         cost  revenue\n",
      " |      A  True    False\n",
      " |      B  True     True\n",
      " |      C  True     True\n",
      " |      D  True     True\n",
      " |      \n",
      " |      When comparing to an arbitrary sequence, the number of columns must\n",
      " |      match the number elements in `other`:\n",
      " |      \n",
      " |      >>> df == [250, 100]  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A   True     True\n",
      " |      B  False    False\n",
      " |      C  False    False\n",
      " |      \n",
      " |      Use the method to control the axis:\n",
      " |      \n",
      " |      >>> df.eq([250, 250, 100], axis='index')  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A   True    False\n",
      " |      B  False     True\n",
      " |      C   True    False\n",
      " |      \n",
      " |      Compare to a DataFrame of different shape.\n",
      " |      \n",
      " |      >>> other = pd.DataFrame({'revenue': [300, 250, 100, 150]},  # doctest: +SKIP\n",
      " |      ...                      index=['A', 'B', 'C', 'D'])\n",
      " |      >>> other  # doctest: +SKIP\n",
      " |         revenue\n",
      " |      A      300\n",
      " |      B      250\n",
      " |      C      100\n",
      " |      D      150\n",
      " |      \n",
      " |      >>> df.gt(other)  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A  False    False\n",
      " |      B  False    False\n",
      " |      C  False     True\n",
      " |      D  False    False\n",
      " |      \n",
      " |      Compare to a MultiIndex by level.\n",
      " |      \n",
      " |      >>> df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220],  # doctest: +SKIP\n",
      " |      ...                              'revenue': [100, 250, 300, 200, 175, 225]},\n",
      " |      ...                             index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'],\n",
      " |      ...                                    ['A', 'B', 'C', 'A', 'B', 'C']])\n",
      " |      >>> df_multindex  # doctest: +SKIP\n",
      " |            cost  revenue\n",
      " |      Q1 A   250      100\n",
      " |         B   150      250\n",
      " |         C   100      300\n",
      " |      Q2 A   150      200\n",
      " |         B   300      175\n",
      " |         C   220      225\n",
      " |      \n",
      " |      >>> df.le(df_multindex, level=1)  # doctest: +SKIP\n",
      " |             cost  revenue\n",
      " |      Q1 A   True     True\n",
      " |         B   True     True\n",
      " |         C   True     True\n",
      " |      Q2 A  False     True\n",
      " |         B   True    False\n",
      " |         C   True    False\n",
      " |  \n",
      " |  info(self, buf=None, verbose=False, memory_usage=False)\n",
      " |      Concise summary of a Dask DataFrame.\n",
      " |  \n",
      " |  items(self)\n",
      " |      Iterate over (column name, Series) pairs.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.items.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Iterates over the DataFrame columns, returning a tuple with\n",
      " |      the column name and the content as a Series.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      label : object\n",
      " |          The column names for the DataFrame being iterated over.\n",
      " |      content : Series\n",
      " |          The column entries belonging to each label, as a Series.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.iterrows : Iterate over DataFrame rows as\n",
      " |          (index, Series) pairs.\n",
      " |      DataFrame.itertuples : Iterate over DataFrame rows as namedtuples\n",
      " |          of the values.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'species': ['bear', 'bear', 'marsupial'],  # doctest: +SKIP\n",
      " |      ...                   'population': [1864, 22000, 80000]},\n",
      " |      ...                   index=['panda', 'polar', 'koala'])\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |              species   population\n",
      " |      panda   bear      1864\n",
      " |      polar   bear      22000\n",
      " |      koala   marsupial 80000\n",
      " |      >>> for label, content in df.items():  # doctest: +SKIP\n",
      " |      ...     print(f'label: {label}')\n",
      " |      ...     print(f'content: {content}', sep='\\n')\n",
      " |      ...\n",
      " |      label: species\n",
      " |      content:\n",
      " |      panda         bear\n",
      " |      polar         bear\n",
      " |      koala    marsupial\n",
      " |      Name: species, dtype: object\n",
      " |      label: population\n",
      " |      content:\n",
      " |      panda     1864\n",
      " |      polar    22000\n",
      " |      koala    80000\n",
      " |      Name: population, dtype: int64\n",
      " |  \n",
      " |  iterrows(self)\n",
      " |      Iterate over DataFrame rows as (index, Series) pairs.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.iterrows.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      index : label or tuple of label\n",
      " |          The index of the row. A tuple for a `MultiIndex`.\n",
      " |      data : Series\n",
      " |          The data of the row as a Series.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.itertuples : Iterate over DataFrame rows as namedtuples of the values.\n",
      " |      DataFrame.items : Iterate over (column name, Series) pairs.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      1. Because ``iterrows`` returns a Series for each row,\n",
      " |         it does **not** preserve dtypes across the rows (dtypes are\n",
      " |         preserved across columns for DataFrames). For example,\n",
      " |      \n",
      " |         >>> df = pd.DataFrame([[1, 1.5]], columns=['int', 'float'])  # doctest: +SKIP\n",
      " |         >>> row = next(df.iterrows())[1]  # doctest: +SKIP\n",
      " |         >>> row  # doctest: +SKIP\n",
      " |         int      1.0\n",
      " |         float    1.5\n",
      " |         Name: 0, dtype: float64\n",
      " |         >>> print(row['int'].dtype)  # doctest: +SKIP\n",
      " |         float64\n",
      " |         >>> print(df['int'].dtype)  # doctest: +SKIP\n",
      " |         int64\n",
      " |      \n",
      " |         To preserve dtypes while iterating over the rows, it is better\n",
      " |         to use :meth:`itertuples` which returns namedtuples of the values\n",
      " |         and which is generally faster than ``iterrows``.\n",
      " |      \n",
      " |      2. You should **never modify** something you are iterating over.\n",
      " |         This is not guaranteed to work in all cases. Depending on the\n",
      " |         data types, the iterator returns a copy and not a view, and writing\n",
      " |         to it will have no effect.\n",
      " |  \n",
      " |  itertuples(self, index=True, name='Pandas')\n",
      " |      Iterate over DataFrame rows as namedtuples.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.itertuples.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      index : bool, default True\n",
      " |          If True, return the index as the first element of the tuple.\n",
      " |      name : str or None, default \"Pandas\"\n",
      " |          The name of the returned namedtuples or None to return regular\n",
      " |          tuples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      iterator\n",
      " |          An object to iterate over namedtuples for each row in the\n",
      " |          DataFrame with the first field possibly being the index and\n",
      " |          following fields being the column values.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.iterrows : Iterate over DataFrame rows as (index, Series)\n",
      " |          pairs.\n",
      " |      DataFrame.items : Iterate over (column name, Series) pairs.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The column names will be renamed to positional names if they are\n",
      " |      invalid Python identifiers, repeated, or start with an underscore.\n",
      " |      On python versions < 3.7 regular tuples are returned for DataFrames\n",
      " |      with a large number of columns (>254).\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'num_legs': [4, 2], 'num_wings': [0, 2]},  # doctest: +SKIP\n",
      " |      ...                   index=['dog', 'hawk'])\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |            num_legs  num_wings\n",
      " |      dog          4          0\n",
      " |      hawk         2          2\n",
      " |      >>> for row in df.itertuples():  # doctest: +SKIP\n",
      " |      ...     print(row)\n",
      " |      ...\n",
      " |      Pandas(Index='dog', num_legs=4, num_wings=0)\n",
      " |      Pandas(Index='hawk', num_legs=2, num_wings=2)\n",
      " |      \n",
      " |      By setting the `index` parameter to False we can remove the index\n",
      " |      as the first element of the tuple:\n",
      " |      \n",
      " |      >>> for row in df.itertuples(index=False):  # doctest: +SKIP\n",
      " |      ...     print(row)\n",
      " |      ...\n",
      " |      Pandas(num_legs=4, num_wings=0)\n",
      " |      Pandas(num_legs=2, num_wings=2)\n",
      " |      \n",
      " |      With the `name` parameter set we set a custom name for the yielded\n",
      " |      namedtuples:\n",
      " |      \n",
      " |      >>> for row in df.itertuples(name='Animal'):  # doctest: +SKIP\n",
      " |      ...     print(row)\n",
      " |      ...\n",
      " |      Animal(Index='dog', num_legs=4, num_wings=0)\n",
      " |      Animal(Index='hawk', num_legs=2, num_wings=2)\n",
      " |  \n",
      " |  join(self, other, on=None, how='left', lsuffix='', rsuffix='', npartitions=None, shuffle=None)\n",
      " |      Join columns of another DataFrame.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.join.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Join columns with `other` DataFrame either on index or on a key\n",
      " |      column. Efficiently join multiple DataFrame objects by index at once by\n",
      " |      passing a list.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : DataFrame, Series, or list of DataFrame\n",
      " |          Index should be similar to one of the columns in this one. If a\n",
      " |          Series is passed, its name attribute must be set, and that will be\n",
      " |          used as the column name in the resulting joined DataFrame.\n",
      " |      on : str, list of str, or array-like, optional\n",
      " |          Column or index level name(s) in the caller to join on the index\n",
      " |          in `other`, otherwise joins index-on-index. If multiple\n",
      " |          values given, the `other` DataFrame must have a MultiIndex. Can\n",
      " |          pass an array as the join key if it is not already contained in\n",
      " |          the calling DataFrame. Like an Excel VLOOKUP operation.\n",
      " |      how : {'left', 'right', 'outer', 'inner'}, default 'left'\n",
      " |          How to handle the operation of the two objects.\n",
      " |      \n",
      " |          * left: use calling frame's index (or column if on is specified)\n",
      " |          * right: use `other`'s index.\n",
      " |          * outer: form union of calling frame's index (or column if on is\n",
      " |            specified) with `other`'s index, and sort it.\n",
      " |            lexicographically.\n",
      " |          * inner: form intersection of calling frame's index (or column if\n",
      " |            on is specified) with `other`'s index, preserving the order\n",
      " |            of the calling's one.\n",
      " |          * cross: creates the cartesian product from both frames, preserves the order\n",
      " |            of the left keys.\n",
      " |      \n",
      " |            .. versionadded:: 1.2.0\n",
      " |      \n",
      " |      lsuffix : str, default ''\n",
      " |          Suffix to use from left frame's overlapping columns.\n",
      " |      rsuffix : str, default ''\n",
      " |          Suffix to use from right frame's overlapping columns.\n",
      " |      sort : bool, default False  (Not supported in Dask)\n",
      " |          Order result DataFrame lexicographically by the join key. If False,\n",
      " |          the order of the join key depends on the join type (how keyword).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          A dataframe containing columns from both the caller and `other`.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.merge : For column(s)-on-column(s) operations.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Parameters `on`, `lsuffix`, and `rsuffix` are not supported when\n",
      " |      passing a list of `DataFrame` objects.\n",
      " |      \n",
      " |      Support for specifying index levels as the `on` parameter was added\n",
      " |      in version 0.23.0.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3', 'K4', 'K5'],  # doctest: +SKIP\n",
      " |      ...                    'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']})\n",
      " |      \n",
      " |      >>> df  # doctest: +SKIP\n",
      " |        key   A\n",
      " |      0  K0  A0\n",
      " |      1  K1  A1\n",
      " |      2  K2  A2\n",
      " |      3  K3  A3\n",
      " |      4  K4  A4\n",
      " |      5  K5  A5\n",
      " |      \n",
      " |      >>> other = pd.DataFrame({'key': ['K0', 'K1', 'K2'],  # doctest: +SKIP\n",
      " |      ...                       'B': ['B0', 'B1', 'B2']})\n",
      " |      \n",
      " |      >>> other  # doctest: +SKIP\n",
      " |        key   B\n",
      " |      0  K0  B0\n",
      " |      1  K1  B1\n",
      " |      2  K2  B2\n",
      " |      \n",
      " |      Join DataFrames using their indexes.\n",
      " |      \n",
      " |      >>> df.join(other, lsuffix='_caller', rsuffix='_other')  # doctest: +SKIP\n",
      " |        key_caller   A key_other    B\n",
      " |      0         K0  A0        K0   B0\n",
      " |      1         K1  A1        K1   B1\n",
      " |      2         K2  A2        K2   B2\n",
      " |      3         K3  A3       NaN  NaN\n",
      " |      4         K4  A4       NaN  NaN\n",
      " |      5         K5  A5       NaN  NaN\n",
      " |      \n",
      " |      If we want to join using the key columns, we need to set key to be\n",
      " |      the index in both `df` and `other`. The joined DataFrame will have\n",
      " |      key as its index.\n",
      " |      \n",
      " |      >>> df.set_index('key').join(other.set_index('key'))  # doctest: +SKIP\n",
      " |            A    B\n",
      " |      key\n",
      " |      K0   A0   B0\n",
      " |      K1   A1   B1\n",
      " |      K2   A2   B2\n",
      " |      K3   A3  NaN\n",
      " |      K4   A4  NaN\n",
      " |      K5   A5  NaN\n",
      " |      \n",
      " |      Another option to join using the key columns is to use the `on`\n",
      " |      parameter. DataFrame.join always uses `other`'s index but we can use\n",
      " |      any column in `df`. This method preserves the original DataFrame's\n",
      " |      index in the result.\n",
      " |      \n",
      " |      >>> df.join(other.set_index('key'), on='key')  # doctest: +SKIP\n",
      " |        key   A    B\n",
      " |      0  K0  A0   B0\n",
      " |      1  K1  A1   B1\n",
      " |      2  K2  A2   B2\n",
      " |      3  K3  A3  NaN\n",
      " |      4  K4  A4  NaN\n",
      " |      5  K5  A5  NaN\n",
      " |      \n",
      " |      Using non-unique key values shows how they are matched.\n",
      " |      \n",
      " |      >>> df = pd.DataFrame({'key': ['K0', 'K1', 'K1', 'K3', 'K0', 'K1'],  # doctest: +SKIP\n",
      " |      ...                    'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']})\n",
      " |      \n",
      " |      >>> df  # doctest: +SKIP\n",
      " |        key   A\n",
      " |      0  K0  A0\n",
      " |      1  K1  A1\n",
      " |      2  K1  A2\n",
      " |      3  K3  A3\n",
      " |      4  K0  A4\n",
      " |      5  K1  A5\n",
      " |      \n",
      " |      >>> df.join(other.set_index('key'), on='key')  # doctest: +SKIP\n",
      " |        key   A    B\n",
      " |      0  K0  A0   B0\n",
      " |      1  K1  A1   B1\n",
      " |      2  K1  A2   B1\n",
      " |      3  K3  A3  NaN\n",
      " |      4  K0  A4   B0\n",
      " |      5  K1  A5   B1\n",
      " |  \n",
      " |  le(self, other, axis='columns', level=None)\n",
      " |      Get Less than or equal to of dataframe and other, element-wise (binary operator `le`).\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.le.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Among flexible wrappers (`eq`, `ne`, `le`, `lt`, `ge`, `gt`) to comparison\n",
      " |      operators.\n",
      " |      \n",
      " |      Equivalent to `==`, `!=`, `<=`, `<`, `>=`, `>` with support to choose axis\n",
      " |      (rows or columns) and level for comparison.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 'columns'\n",
      " |          Whether to compare by the index (0 or 'index') or columns\n",
      " |          (1 or 'columns').\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the passed\n",
      " |          MultiIndex level.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame of bool\n",
      " |          Result of the comparison.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.eq : Compare DataFrames for equality elementwise.\n",
      " |      DataFrame.ne : Compare DataFrames for inequality elementwise.\n",
      " |      DataFrame.le : Compare DataFrames for less than inequality\n",
      " |          or equality elementwise.\n",
      " |      DataFrame.lt : Compare DataFrames for strictly less than\n",
      " |          inequality elementwise.\n",
      " |      DataFrame.ge : Compare DataFrames for greater than inequality\n",
      " |          or equality elementwise.\n",
      " |      DataFrame.gt : Compare DataFrames for strictly greater than\n",
      " |          inequality elementwise.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |      `NaN` values are considered different (i.e. `NaN` != `NaN`).\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'cost': [250, 150, 100],  # doctest: +SKIP\n",
      " |      ...                    'revenue': [100, 250, 300]},\n",
      " |      ...                   index=['A', 'B', 'C'])\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |         cost  revenue\n",
      " |      A   250      100\n",
      " |      B   150      250\n",
      " |      C   100      300\n",
      " |      \n",
      " |      Comparison with a scalar, using either the operator or method:\n",
      " |      \n",
      " |      >>> df == 100  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A  False     True\n",
      " |      B  False    False\n",
      " |      C   True    False\n",
      " |      \n",
      " |      >>> df.eq(100)  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A  False     True\n",
      " |      B  False    False\n",
      " |      C   True    False\n",
      " |      \n",
      " |      When `other` is a :class:`Series`, the columns of a DataFrame are aligned\n",
      " |      with the index of `other` and broadcast:\n",
      " |      \n",
      " |      >>> df != pd.Series([100, 250], index=[\"cost\", \"revenue\"])  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A   True     True\n",
      " |      B   True    False\n",
      " |      C  False     True\n",
      " |      \n",
      " |      Use the method to control the broadcast axis:\n",
      " |      \n",
      " |      >>> df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index')  # doctest: +SKIP\n",
      " |         cost  revenue\n",
      " |      A  True    False\n",
      " |      B  True     True\n",
      " |      C  True     True\n",
      " |      D  True     True\n",
      " |      \n",
      " |      When comparing to an arbitrary sequence, the number of columns must\n",
      " |      match the number elements in `other`:\n",
      " |      \n",
      " |      >>> df == [250, 100]  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A   True     True\n",
      " |      B  False    False\n",
      " |      C  False    False\n",
      " |      \n",
      " |      Use the method to control the axis:\n",
      " |      \n",
      " |      >>> df.eq([250, 250, 100], axis='index')  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A   True    False\n",
      " |      B  False     True\n",
      " |      C   True    False\n",
      " |      \n",
      " |      Compare to a DataFrame of different shape.\n",
      " |      \n",
      " |      >>> other = pd.DataFrame({'revenue': [300, 250, 100, 150]},  # doctest: +SKIP\n",
      " |      ...                      index=['A', 'B', 'C', 'D'])\n",
      " |      >>> other  # doctest: +SKIP\n",
      " |         revenue\n",
      " |      A      300\n",
      " |      B      250\n",
      " |      C      100\n",
      " |      D      150\n",
      " |      \n",
      " |      >>> df.gt(other)  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A  False    False\n",
      " |      B  False    False\n",
      " |      C  False     True\n",
      " |      D  False    False\n",
      " |      \n",
      " |      Compare to a MultiIndex by level.\n",
      " |      \n",
      " |      >>> df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220],  # doctest: +SKIP\n",
      " |      ...                              'revenue': [100, 250, 300, 200, 175, 225]},\n",
      " |      ...                             index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'],\n",
      " |      ...                                    ['A', 'B', 'C', 'A', 'B', 'C']])\n",
      " |      >>> df_multindex  # doctest: +SKIP\n",
      " |            cost  revenue\n",
      " |      Q1 A   250      100\n",
      " |         B   150      250\n",
      " |         C   100      300\n",
      " |      Q2 A   150      200\n",
      " |         B   300      175\n",
      " |         C   220      225\n",
      " |      \n",
      " |      >>> df.le(df_multindex, level=1)  # doctest: +SKIP\n",
      " |             cost  revenue\n",
      " |      Q1 A   True     True\n",
      " |         B   True     True\n",
      " |         C   True     True\n",
      " |      Q2 A  False     True\n",
      " |         B   True    False\n",
      " |         C   True    False\n",
      " |  \n",
      " |  lt(self, other, axis='columns', level=None)\n",
      " |      Get Less than of dataframe and other, element-wise (binary operator `lt`).\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.lt.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Among flexible wrappers (`eq`, `ne`, `le`, `lt`, `ge`, `gt`) to comparison\n",
      " |      operators.\n",
      " |      \n",
      " |      Equivalent to `==`, `!=`, `<=`, `<`, `>=`, `>` with support to choose axis\n",
      " |      (rows or columns) and level for comparison.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 'columns'\n",
      " |          Whether to compare by the index (0 or 'index') or columns\n",
      " |          (1 or 'columns').\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the passed\n",
      " |          MultiIndex level.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame of bool\n",
      " |          Result of the comparison.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.eq : Compare DataFrames for equality elementwise.\n",
      " |      DataFrame.ne : Compare DataFrames for inequality elementwise.\n",
      " |      DataFrame.le : Compare DataFrames for less than inequality\n",
      " |          or equality elementwise.\n",
      " |      DataFrame.lt : Compare DataFrames for strictly less than\n",
      " |          inequality elementwise.\n",
      " |      DataFrame.ge : Compare DataFrames for greater than inequality\n",
      " |          or equality elementwise.\n",
      " |      DataFrame.gt : Compare DataFrames for strictly greater than\n",
      " |          inequality elementwise.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |      `NaN` values are considered different (i.e. `NaN` != `NaN`).\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'cost': [250, 150, 100],  # doctest: +SKIP\n",
      " |      ...                    'revenue': [100, 250, 300]},\n",
      " |      ...                   index=['A', 'B', 'C'])\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |         cost  revenue\n",
      " |      A   250      100\n",
      " |      B   150      250\n",
      " |      C   100      300\n",
      " |      \n",
      " |      Comparison with a scalar, using either the operator or method:\n",
      " |      \n",
      " |      >>> df == 100  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A  False     True\n",
      " |      B  False    False\n",
      " |      C   True    False\n",
      " |      \n",
      " |      >>> df.eq(100)  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A  False     True\n",
      " |      B  False    False\n",
      " |      C   True    False\n",
      " |      \n",
      " |      When `other` is a :class:`Series`, the columns of a DataFrame are aligned\n",
      " |      with the index of `other` and broadcast:\n",
      " |      \n",
      " |      >>> df != pd.Series([100, 250], index=[\"cost\", \"revenue\"])  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A   True     True\n",
      " |      B   True    False\n",
      " |      C  False     True\n",
      " |      \n",
      " |      Use the method to control the broadcast axis:\n",
      " |      \n",
      " |      >>> df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index')  # doctest: +SKIP\n",
      " |         cost  revenue\n",
      " |      A  True    False\n",
      " |      B  True     True\n",
      " |      C  True     True\n",
      " |      D  True     True\n",
      " |      \n",
      " |      When comparing to an arbitrary sequence, the number of columns must\n",
      " |      match the number elements in `other`:\n",
      " |      \n",
      " |      >>> df == [250, 100]  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A   True     True\n",
      " |      B  False    False\n",
      " |      C  False    False\n",
      " |      \n",
      " |      Use the method to control the axis:\n",
      " |      \n",
      " |      >>> df.eq([250, 250, 100], axis='index')  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A   True    False\n",
      " |      B  False     True\n",
      " |      C   True    False\n",
      " |      \n",
      " |      Compare to a DataFrame of different shape.\n",
      " |      \n",
      " |      >>> other = pd.DataFrame({'revenue': [300, 250, 100, 150]},  # doctest: +SKIP\n",
      " |      ...                      index=['A', 'B', 'C', 'D'])\n",
      " |      >>> other  # doctest: +SKIP\n",
      " |         revenue\n",
      " |      A      300\n",
      " |      B      250\n",
      " |      C      100\n",
      " |      D      150\n",
      " |      \n",
      " |      >>> df.gt(other)  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A  False    False\n",
      " |      B  False    False\n",
      " |      C  False     True\n",
      " |      D  False    False\n",
      " |      \n",
      " |      Compare to a MultiIndex by level.\n",
      " |      \n",
      " |      >>> df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220],  # doctest: +SKIP\n",
      " |      ...                              'revenue': [100, 250, 300, 200, 175, 225]},\n",
      " |      ...                             index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'],\n",
      " |      ...                                    ['A', 'B', 'C', 'A', 'B', 'C']])\n",
      " |      >>> df_multindex  # doctest: +SKIP\n",
      " |            cost  revenue\n",
      " |      Q1 A   250      100\n",
      " |         B   150      250\n",
      " |         C   100      300\n",
      " |      Q2 A   150      200\n",
      " |         B   300      175\n",
      " |         C   220      225\n",
      " |      \n",
      " |      >>> df.le(df_multindex, level=1)  # doctest: +SKIP\n",
      " |             cost  revenue\n",
      " |      Q1 A   True     True\n",
      " |         B   True     True\n",
      " |         C   True     True\n",
      " |      Q2 A  False     True\n",
      " |         B   True    False\n",
      " |         C   True    False\n",
      " |  \n",
      " |  melt(self, id_vars=None, value_vars=None, var_name=None, value_name='value', col_level=None)\n",
      " |      Unpivots a DataFrame from wide format to long format,\n",
      " |      optionally leaving identifier variables set.\n",
      " |      \n",
      " |      This function is useful to massage a DataFrame into a format where\n",
      " |      one or more columns are identifier variables (``id_vars``), while\n",
      " |      all other columns, considered measured variables (``value_vars``),\n",
      " |      are \"unpivoted\" to the row axis, leaving just two non-identifier\n",
      " |      columns, 'variable' and 'value'.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      frame : DataFrame\n",
      " |      id_vars : tuple, list, or ndarray, optional\n",
      " |          Column(s) to use as identifier variables.\n",
      " |      value_vars : tuple, list, or ndarray, optional\n",
      " |          Column(s) to unpivot. If not specified, uses all columns that\n",
      " |          are not set as `id_vars`.\n",
      " |      var_name : scalar\n",
      " |          Name to use for the 'variable' column. If None it uses\n",
      " |          ``frame.columns.name`` or 'variable'.\n",
      " |      value_name : scalar, default 'value'\n",
      " |          Name to use for the 'value' column.\n",
      " |      col_level : int or string, optional\n",
      " |          If columns are a MultiIndex then use this level to melt.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Unpivoted DataFrame.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pandas.DataFrame.melt\n",
      " |  \n",
      " |  memory_usage(self, index=True, deep=False)\n",
      " |      Return the memory usage of each column in bytes.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.memory_usage.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      The memory usage can optionally include the contribution of\n",
      " |      the index and elements of `object` dtype.\n",
      " |      \n",
      " |      This value is displayed in `DataFrame.info` by default. This can be\n",
      " |      suppressed by setting ``pandas.options.display.memory_usage`` to False.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      index : bool, default True\n",
      " |          Specifies whether to include the memory usage of the DataFrame's\n",
      " |          index in returned Series. If ``index=True``, the memory usage of\n",
      " |          the index is the first item in the output.\n",
      " |      deep : bool, default False\n",
      " |          If True, introspect the data deeply by interrogating\n",
      " |          `object` dtypes for system-level memory consumption, and include\n",
      " |          it in the returned values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series\n",
      " |          A Series whose index is the original column names and whose values\n",
      " |          is the memory usage of each column in bytes.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.ndarray.nbytes : Total bytes consumed by the elements of an\n",
      " |          ndarray.\n",
      " |      Series.memory_usage : Bytes consumed by a Series.\n",
      " |      Categorical : Memory-efficient array for string values with\n",
      " |          many repeated values.\n",
      " |      DataFrame.info : Concise summary of a DataFrame.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> dtypes = ['int64', 'float64', 'complex128', 'object', 'bool']  # doctest: +SKIP\n",
      " |      >>> data = dict([(t, np.ones(shape=5000, dtype=int).astype(t))  # doctest: +SKIP\n",
      " |      ...              for t in dtypes])\n",
      " |      >>> df = pd.DataFrame(data)  # doctest: +SKIP\n",
      " |      >>> df.head()  # doctest: +SKIP\n",
      " |         int64  float64            complex128  object  bool\n",
      " |      0      1      1.0              1.0+0.0j       1  True\n",
      " |      1      1      1.0              1.0+0.0j       1  True\n",
      " |      2      1      1.0              1.0+0.0j       1  True\n",
      " |      3      1      1.0              1.0+0.0j       1  True\n",
      " |      4      1      1.0              1.0+0.0j       1  True\n",
      " |      \n",
      " |      >>> df.memory_usage()  # doctest: +SKIP\n",
      " |      Index           128\n",
      " |      int64         40000\n",
      " |      float64       40000\n",
      " |      complex128    80000\n",
      " |      object        40000\n",
      " |      bool           5000\n",
      " |      dtype: int64\n",
      " |      \n",
      " |      >>> df.memory_usage(index=False)  # doctest: +SKIP\n",
      " |      int64         40000\n",
      " |      float64       40000\n",
      " |      complex128    80000\n",
      " |      object        40000\n",
      " |      bool           5000\n",
      " |      dtype: int64\n",
      " |      \n",
      " |      The memory footprint of `object` dtype columns is ignored by default:\n",
      " |      \n",
      " |      >>> df.memory_usage(deep=True)  # doctest: +SKIP\n",
      " |      Index            128\n",
      " |      int64          40000\n",
      " |      float64        40000\n",
      " |      complex128     80000\n",
      " |      object        180000\n",
      " |      bool            5000\n",
      " |      dtype: int64\n",
      " |      \n",
      " |      Use a Categorical for efficient storage of an object-dtype column with\n",
      " |      many repeated values.\n",
      " |      \n",
      " |      >>> df['object'].astype('category').memory_usage(deep=True)  # doctest: +SKIP\n",
      " |      5244\n",
      " |  \n",
      " |  merge(self, right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, suffixes=('_x', '_y'), indicator=False, npartitions=None, shuffle=None, broadcast=None)\n",
      " |      Merge the DataFrame with another DataFrame\n",
      " |      \n",
      " |      This will merge the two datasets, either on the indices, a certain column\n",
      " |      in each dataset or the index in one dataset and the column in another.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      right: dask.dataframe.DataFrame\n",
      " |      how : {'left', 'right', 'outer', 'inner'}, default: 'inner'\n",
      " |          How to handle the operation of the two objects:\n",
      " |      \n",
      " |          - left: use calling frame's index (or column if on is specified)\n",
      " |          - right: use other frame's index\n",
      " |          - outer: form union of calling frame's index (or column if on is\n",
      " |            specified) with other frame's index, and sort it\n",
      " |            lexicographically\n",
      " |          - inner: form intersection of calling frame's index (or column if\n",
      " |            on is specified) with other frame's index, preserving the order\n",
      " |            of the calling's one\n",
      " |      \n",
      " |      on : label or list\n",
      " |          Column or index level names to join on. These must be found in both\n",
      " |          DataFrames. If on is None and not merging on indexes then this\n",
      " |          defaults to the intersection of the columns in both DataFrames.\n",
      " |      left_on : label or list, or array-like\n",
      " |          Column to join on in the left DataFrame. Other than in pandas\n",
      " |          arrays and lists are only support if their length is 1.\n",
      " |      right_on : label or list, or array-like\n",
      " |          Column to join on in the right DataFrame. Other than in pandas\n",
      " |          arrays and lists are only support if their length is 1.\n",
      " |      left_index : boolean, default False\n",
      " |          Use the index from the left DataFrame as the join key.\n",
      " |      right_index : boolean, default False\n",
      " |          Use the index from the right DataFrame as the join key.\n",
      " |      suffixes : 2-length sequence (tuple, list, ...)\n",
      " |          Suffix to apply to overlapping column names in the left and\n",
      " |          right side, respectively\n",
      " |      indicator : boolean or string, default False\n",
      " |          If True, adds a column to output DataFrame called \"_merge\" with\n",
      " |          information on the source of each row. If string, column with\n",
      " |          information on source of each row will be added to output DataFrame,\n",
      " |          and column will be named value of string. Information column is\n",
      " |          Categorical-type and takes on a value of \"left_only\" for observations\n",
      " |          whose merge key only appears in `left` DataFrame, \"right_only\" for\n",
      " |          observations whose merge key only appears in `right` DataFrame,\n",
      " |          and \"both\" if the observation’s merge key is found in both.\n",
      " |      npartitions: int or None, optional\n",
      " |          The ideal number of output partitions. This is only utilised when\n",
      " |          performing a hash_join (merging on columns only). If ``None`` then\n",
      " |          ``npartitions = max(lhs.npartitions, rhs.npartitions)``.\n",
      " |          Default is ``None``.\n",
      " |      shuffle: {'disk', 'tasks'}, optional\n",
      " |          Either ``'disk'`` for single-node operation or ``'tasks'`` for\n",
      " |          distributed operation.  Will be inferred by your current scheduler.\n",
      " |      broadcast: boolean or float, optional\n",
      " |          Whether to use a broadcast-based join in lieu of a shuffle-based\n",
      " |          join for supported cases.  By default, a simple heuristic will be\n",
      " |          used to select the underlying algorithm. If a floating-point value\n",
      " |          is specified, that number will be used as the ``broadcast_bias``\n",
      " |          within the simple heuristic (a large number makes Dask more likely\n",
      " |          to choose the ``broacast_join`` code path). See ``broadcast_join``\n",
      " |          for more information.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      \n",
      " |      There are three ways to join dataframes:\n",
      " |      \n",
      " |      1. Joining on indices. In this case the divisions are\n",
      " |         aligned using the function ``dask.dataframe.multi.align_partitions``.\n",
      " |         Afterwards, each partition is merged with the pandas merge function.\n",
      " |      \n",
      " |      2. Joining one on index and one on column. In this case the divisions of\n",
      " |         dataframe merged by index (:math:`d_i`) are used to divide the column\n",
      " |         merged dataframe (:math:`d_c`) one using\n",
      " |         ``dask.dataframe.multi.rearrange_by_divisions``. In this case the\n",
      " |         merged dataframe (:math:`d_m`) has the exact same divisions\n",
      " |         as (:math:`d_i`). This can lead to issues if you merge multiple rows from\n",
      " |         (:math:`d_c`) to one row in (:math:`d_i`).\n",
      " |      \n",
      " |      3. Joining both on columns. In this case a hash join is performed using\n",
      " |         ``dask.dataframe.multi.hash_join``.\n",
      " |      \n",
      " |      In some cases, you may see a ``MemoryError`` if the ``merge`` operation requires\n",
      " |      an internal ``shuffle``, because shuffling places all rows that have the same\n",
      " |      index in the same partition. To avoid this error, make sure all rows with the\n",
      " |      same ``on``-column value can fit on a single partition.\n",
      " |  \n",
      " |  mod(self, other, axis='columns', level=None, fill_value=None)\n",
      " |      Get Modulo of dataframe and other, element-wise (binary operator `mod`).\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.mod.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Equivalent to ``dataframe % other``, but with support to substitute a fill_value\n",
      " |      for missing data in one of the inputs. With reverse version, `rmod`.\n",
      " |      \n",
      " |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `mod`, `pow`) to\n",
      " |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}\n",
      " |          Whether to compare by the index (0 or 'index') or columns\n",
      " |          (1 or 'columns'). For Series input, axis to match Series index on.\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      fill_value : float or None, default None\n",
      " |          Fill existing missing (NaN) values, and any new element needed for\n",
      " |          successful DataFrame alignment, with this value before computation.\n",
      " |          If data in both corresponding DataFrame locations is missing\n",
      " |          the result will be missing.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Result of the arithmetic operation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.add : Add DataFrames.\n",
      " |      DataFrame.sub : Subtract DataFrames.\n",
      " |      DataFrame.mul : Multiply DataFrames.\n",
      " |      DataFrame.div : Divide DataFrames (float division).\n",
      " |      DataFrame.truediv : Divide DataFrames (float division).\n",
      " |      DataFrame.floordiv : Divide DataFrames (integer division).\n",
      " |      DataFrame.mod : Calculate modulo (remainder after division).\n",
      " |      DataFrame.pow : Calculate exponential power.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'angles': [0, 3, 4],  # doctest: +SKIP\n",
      " |      ...                    'degrees': [360, 180, 360]},\n",
      " |      ...                   index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      360\n",
      " |      triangle        3      180\n",
      " |      rectangle       4      360\n",
      " |      \n",
      " |      Add a scalar with operator version which return the same\n",
      " |      results.\n",
      " |      \n",
      " |      >>> df + 1  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      >>> df.add(1)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      Divide by constant with reverse version.\n",
      " |      \n",
      " |      >>> df.div(10)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle        0.0     36.0\n",
      " |      triangle      0.3     18.0\n",
      " |      rectangle     0.4     36.0\n",
      " |      \n",
      " |      >>> df.rdiv(10)  # doctest: +SKIP\n",
      " |                   angles   degrees\n",
      " |      circle          inf  0.027778\n",
      " |      triangle   3.333333  0.055556\n",
      " |      rectangle  2.500000  0.027778\n",
      " |      \n",
      " |      Subtract a list and Series by axis with operator version.\n",
      " |      \n",
      " |      >>> df - [1, 2]  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub([1, 2], axis='columns')  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),  # doctest: +SKIP\n",
      " |      ...        axis='index')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      359\n",
      " |      triangle        2      179\n",
      " |      rectangle       3      359\n",
      " |      \n",
      " |      Multiply a DataFrame of different shape with operator version.\n",
      " |      \n",
      " |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},  # doctest: +SKIP\n",
      " |      ...                      index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> other  # doctest: +SKIP\n",
      " |                 angles\n",
      " |      circle          0\n",
      " |      triangle        3\n",
      " |      rectangle       4\n",
      " |      \n",
      " |      >>> df * other  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      NaN\n",
      " |      triangle        9      NaN\n",
      " |      rectangle      16      NaN\n",
      " |      \n",
      " |      >>> df.mul(other, fill_value=0)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      0.0\n",
      " |      triangle        9      0.0\n",
      " |      rectangle      16      0.0\n",
      " |      \n",
      " |      Divide by a MultiIndex by level.\n",
      " |      \n",
      " |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],  # doctest: +SKIP\n",
      " |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n",
      " |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n",
      " |      ...                                    ['circle', 'triangle', 'rectangle',\n",
      " |      ...                                     'square', 'pentagon', 'hexagon']])\n",
      " |      >>> df_multindex  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle          0      360\n",
      " |        triangle        3      180\n",
      " |        rectangle       4      360\n",
      " |      B square          4      360\n",
      " |        pentagon        5      540\n",
      " |        hexagon         6      720\n",
      " |      \n",
      " |      >>> df.div(df_multindex, level=1, fill_value=0)  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle        NaN      1.0\n",
      " |        triangle      1.0      1.0\n",
      " |        rectangle     1.0      1.0\n",
      " |      B square        0.0      0.0\n",
      " |        pentagon      0.0      0.0\n",
      " |        hexagon       0.0      0.0\n",
      " |  \n",
      " |  mode(self, dropna=True, split_every=False)\n",
      " |      Get the mode(s) of each element along the selected axis.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.mode.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      The mode of a set of values is the value that appears most often.\n",
      " |      It can be multiple values.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0  (Not supported in Dask)\n",
      " |          The axis to iterate over while searching for the mode:\n",
      " |      \n",
      " |          * 0 or 'index' : get mode of each column\n",
      " |          * 1 or 'columns' : get mode of each row.\n",
      " |      \n",
      " |      numeric_only : bool, default False  (Not supported in Dask)\n",
      " |          If True, only apply to numeric columns.\n",
      " |      dropna : bool, default True\n",
      " |          Don't consider counts of NaN/NaT.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          The modes of each column or row.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.mode : Return the highest frequency value in a Series.\n",
      " |      Series.value_counts : Return the counts of values in a Series.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame([('bird', 2, 2),  # doctest: +SKIP\n",
      " |      ...                    ('mammal', 4, np.nan),\n",
      " |      ...                    ('arthropod', 8, 0),\n",
      " |      ...                    ('bird', 2, np.nan)],\n",
      " |      ...                   index=('falcon', 'horse', 'spider', 'ostrich'),\n",
      " |      ...                   columns=('species', 'legs', 'wings'))\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |                 species  legs  wings\n",
      " |      falcon        bird     2    2.0\n",
      " |      horse       mammal     4    NaN\n",
      " |      spider   arthropod     8    0.0\n",
      " |      ostrich       bird     2    NaN\n",
      " |      \n",
      " |      By default, missing values are not considered, and the mode of wings\n",
      " |      are both 0 and 2. Because the resulting DataFrame has two rows,\n",
      " |      the second row of ``species`` and ``legs`` contains ``NaN``.\n",
      " |      \n",
      " |      >>> df.mode()  # doctest: +SKIP\n",
      " |        species  legs  wings\n",
      " |      0    bird   2.0    0.0\n",
      " |      1     NaN   NaN    2.0\n",
      " |      \n",
      " |      Setting ``dropna=False`` ``NaN`` values are considered and they can be\n",
      " |      the mode (like for wings).\n",
      " |      \n",
      " |      >>> df.mode(dropna=False)  # doctest: +SKIP\n",
      " |        species  legs  wings\n",
      " |      0    bird     2    NaN\n",
      " |      \n",
      " |      Setting ``numeric_only=True``, only the mode of numeric columns is\n",
      " |      computed, and columns of other types are ignored.\n",
      " |      \n",
      " |      >>> df.mode(numeric_only=True)  # doctest: +SKIP\n",
      " |         legs  wings\n",
      " |      0   2.0    0.0\n",
      " |      1   NaN    2.0\n",
      " |      \n",
      " |      To compute the mode over columns and not rows, use the axis parameter:\n",
      " |      \n",
      " |      >>> df.mode(axis='columns', numeric_only=True)  # doctest: +SKIP\n",
      " |                 0    1\n",
      " |      falcon   2.0  NaN\n",
      " |      horse    4.0  NaN\n",
      " |      spider   0.0  8.0\n",
      " |      ostrich  2.0  NaN\n",
      " |  \n",
      " |  mul(self, other, axis='columns', level=None, fill_value=None)\n",
      " |      Get Multiplication of dataframe and other, element-wise (binary operator `mul`).\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.mul.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Equivalent to ``dataframe * other``, but with support to substitute a fill_value\n",
      " |      for missing data in one of the inputs. With reverse version, `rmul`.\n",
      " |      \n",
      " |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `mod`, `pow`) to\n",
      " |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}\n",
      " |          Whether to compare by the index (0 or 'index') or columns\n",
      " |          (1 or 'columns'). For Series input, axis to match Series index on.\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      fill_value : float or None, default None\n",
      " |          Fill existing missing (NaN) values, and any new element needed for\n",
      " |          successful DataFrame alignment, with this value before computation.\n",
      " |          If data in both corresponding DataFrame locations is missing\n",
      " |          the result will be missing.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Result of the arithmetic operation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.add : Add DataFrames.\n",
      " |      DataFrame.sub : Subtract DataFrames.\n",
      " |      DataFrame.mul : Multiply DataFrames.\n",
      " |      DataFrame.div : Divide DataFrames (float division).\n",
      " |      DataFrame.truediv : Divide DataFrames (float division).\n",
      " |      DataFrame.floordiv : Divide DataFrames (integer division).\n",
      " |      DataFrame.mod : Calculate modulo (remainder after division).\n",
      " |      DataFrame.pow : Calculate exponential power.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'angles': [0, 3, 4],  # doctest: +SKIP\n",
      " |      ...                    'degrees': [360, 180, 360]},\n",
      " |      ...                   index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      360\n",
      " |      triangle        3      180\n",
      " |      rectangle       4      360\n",
      " |      \n",
      " |      Add a scalar with operator version which return the same\n",
      " |      results.\n",
      " |      \n",
      " |      >>> df + 1  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      >>> df.add(1)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      Divide by constant with reverse version.\n",
      " |      \n",
      " |      >>> df.div(10)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle        0.0     36.0\n",
      " |      triangle      0.3     18.0\n",
      " |      rectangle     0.4     36.0\n",
      " |      \n",
      " |      >>> df.rdiv(10)  # doctest: +SKIP\n",
      " |                   angles   degrees\n",
      " |      circle          inf  0.027778\n",
      " |      triangle   3.333333  0.055556\n",
      " |      rectangle  2.500000  0.027778\n",
      " |      \n",
      " |      Subtract a list and Series by axis with operator version.\n",
      " |      \n",
      " |      >>> df - [1, 2]  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub([1, 2], axis='columns')  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),  # doctest: +SKIP\n",
      " |      ...        axis='index')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      359\n",
      " |      triangle        2      179\n",
      " |      rectangle       3      359\n",
      " |      \n",
      " |      Multiply a DataFrame of different shape with operator version.\n",
      " |      \n",
      " |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},  # doctest: +SKIP\n",
      " |      ...                      index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> other  # doctest: +SKIP\n",
      " |                 angles\n",
      " |      circle          0\n",
      " |      triangle        3\n",
      " |      rectangle       4\n",
      " |      \n",
      " |      >>> df * other  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      NaN\n",
      " |      triangle        9      NaN\n",
      " |      rectangle      16      NaN\n",
      " |      \n",
      " |      >>> df.mul(other, fill_value=0)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      0.0\n",
      " |      triangle        9      0.0\n",
      " |      rectangle      16      0.0\n",
      " |      \n",
      " |      Divide by a MultiIndex by level.\n",
      " |      \n",
      " |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],  # doctest: +SKIP\n",
      " |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n",
      " |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n",
      " |      ...                                    ['circle', 'triangle', 'rectangle',\n",
      " |      ...                                     'square', 'pentagon', 'hexagon']])\n",
      " |      >>> df_multindex  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle          0      360\n",
      " |        triangle        3      180\n",
      " |        rectangle       4      360\n",
      " |      B square          4      360\n",
      " |        pentagon        5      540\n",
      " |        hexagon         6      720\n",
      " |      \n",
      " |      >>> df.div(df_multindex, level=1, fill_value=0)  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle        NaN      1.0\n",
      " |        triangle      1.0      1.0\n",
      " |        rectangle     1.0      1.0\n",
      " |      B square        0.0      0.0\n",
      " |        pentagon      0.0      0.0\n",
      " |        hexagon       0.0      0.0\n",
      " |  \n",
      " |  ne(self, other, axis='columns', level=None)\n",
      " |      Get Not equal to of dataframe and other, element-wise (binary operator `ne`).\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.ne.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Among flexible wrappers (`eq`, `ne`, `le`, `lt`, `ge`, `gt`) to comparison\n",
      " |      operators.\n",
      " |      \n",
      " |      Equivalent to `==`, `!=`, `<=`, `<`, `>=`, `>` with support to choose axis\n",
      " |      (rows or columns) and level for comparison.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 'columns'\n",
      " |          Whether to compare by the index (0 or 'index') or columns\n",
      " |          (1 or 'columns').\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the passed\n",
      " |          MultiIndex level.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame of bool\n",
      " |          Result of the comparison.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.eq : Compare DataFrames for equality elementwise.\n",
      " |      DataFrame.ne : Compare DataFrames for inequality elementwise.\n",
      " |      DataFrame.le : Compare DataFrames for less than inequality\n",
      " |          or equality elementwise.\n",
      " |      DataFrame.lt : Compare DataFrames for strictly less than\n",
      " |          inequality elementwise.\n",
      " |      DataFrame.ge : Compare DataFrames for greater than inequality\n",
      " |          or equality elementwise.\n",
      " |      DataFrame.gt : Compare DataFrames for strictly greater than\n",
      " |          inequality elementwise.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |      `NaN` values are considered different (i.e. `NaN` != `NaN`).\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'cost': [250, 150, 100],  # doctest: +SKIP\n",
      " |      ...                    'revenue': [100, 250, 300]},\n",
      " |      ...                   index=['A', 'B', 'C'])\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |         cost  revenue\n",
      " |      A   250      100\n",
      " |      B   150      250\n",
      " |      C   100      300\n",
      " |      \n",
      " |      Comparison with a scalar, using either the operator or method:\n",
      " |      \n",
      " |      >>> df == 100  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A  False     True\n",
      " |      B  False    False\n",
      " |      C   True    False\n",
      " |      \n",
      " |      >>> df.eq(100)  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A  False     True\n",
      " |      B  False    False\n",
      " |      C   True    False\n",
      " |      \n",
      " |      When `other` is a :class:`Series`, the columns of a DataFrame are aligned\n",
      " |      with the index of `other` and broadcast:\n",
      " |      \n",
      " |      >>> df != pd.Series([100, 250], index=[\"cost\", \"revenue\"])  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A   True     True\n",
      " |      B   True    False\n",
      " |      C  False     True\n",
      " |      \n",
      " |      Use the method to control the broadcast axis:\n",
      " |      \n",
      " |      >>> df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index')  # doctest: +SKIP\n",
      " |         cost  revenue\n",
      " |      A  True    False\n",
      " |      B  True     True\n",
      " |      C  True     True\n",
      " |      D  True     True\n",
      " |      \n",
      " |      When comparing to an arbitrary sequence, the number of columns must\n",
      " |      match the number elements in `other`:\n",
      " |      \n",
      " |      >>> df == [250, 100]  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A   True     True\n",
      " |      B  False    False\n",
      " |      C  False    False\n",
      " |      \n",
      " |      Use the method to control the axis:\n",
      " |      \n",
      " |      >>> df.eq([250, 250, 100], axis='index')  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A   True    False\n",
      " |      B  False     True\n",
      " |      C   True    False\n",
      " |      \n",
      " |      Compare to a DataFrame of different shape.\n",
      " |      \n",
      " |      >>> other = pd.DataFrame({'revenue': [300, 250, 100, 150]},  # doctest: +SKIP\n",
      " |      ...                      index=['A', 'B', 'C', 'D'])\n",
      " |      >>> other  # doctest: +SKIP\n",
      " |         revenue\n",
      " |      A      300\n",
      " |      B      250\n",
      " |      C      100\n",
      " |      D      150\n",
      " |      \n",
      " |      >>> df.gt(other)  # doctest: +SKIP\n",
      " |          cost  revenue\n",
      " |      A  False    False\n",
      " |      B  False    False\n",
      " |      C  False     True\n",
      " |      D  False    False\n",
      " |      \n",
      " |      Compare to a MultiIndex by level.\n",
      " |      \n",
      " |      >>> df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220],  # doctest: +SKIP\n",
      " |      ...                              'revenue': [100, 250, 300, 200, 175, 225]},\n",
      " |      ...                             index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'],\n",
      " |      ...                                    ['A', 'B', 'C', 'A', 'B', 'C']])\n",
      " |      >>> df_multindex  # doctest: +SKIP\n",
      " |            cost  revenue\n",
      " |      Q1 A   250      100\n",
      " |         B   150      250\n",
      " |         C   100      300\n",
      " |      Q2 A   150      200\n",
      " |         B   300      175\n",
      " |         C   220      225\n",
      " |      \n",
      " |      >>> df.le(df_multindex, level=1)  # doctest: +SKIP\n",
      " |             cost  revenue\n",
      " |      Q1 A   True     True\n",
      " |         B   True     True\n",
      " |         C   True     True\n",
      " |      Q2 A  False     True\n",
      " |         B   True    False\n",
      " |         C   True    False\n",
      " |  \n",
      " |  nlargest(self, n=5, columns=None, split_every=None)\n",
      " |      Return the first `n` rows ordered by `columns` in descending order.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.nlargest.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Return the first `n` rows with the largest values in `columns`, in\n",
      " |      descending order. The columns that are not specified are returned as\n",
      " |      well, but not used for ordering.\n",
      " |      \n",
      " |      This method is equivalent to\n",
      " |      ``df.sort_values(columns, ascending=False).head(n)``, but more\n",
      " |      performant.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : int\n",
      " |          Number of rows to return.\n",
      " |      columns : label or list of labels\n",
      " |          Column label(s) to order by.\n",
      " |      keep : {'first', 'last', 'all'}, default 'first'  (Not supported in Dask)\n",
      " |          Where there are duplicate values:\n",
      " |      \n",
      " |          - ``first`` : prioritize the first occurrence(s)\n",
      " |          - ``last`` : prioritize the last occurrence(s)\n",
      " |          - ``all`` : do not drop any duplicates, even it means\n",
      " |            selecting more than `n` items.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          The first `n` rows ordered by the given columns in descending\n",
      " |          order.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.nsmallest : Return the first `n` rows ordered by `columns` in\n",
      " |          ascending order.\n",
      " |      DataFrame.sort_values : Sort DataFrame by the values.\n",
      " |      DataFrame.head : Return the first `n` rows without re-ordering.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This function cannot be used with all column types. For example, when\n",
      " |      specifying columns with `object` or `category` dtypes, ``TypeError`` is\n",
      " |      raised.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'population': [59000000, 65000000, 434000,  # doctest: +SKIP\n",
      " |      ...                                   434000, 434000, 337000, 11300,\n",
      " |      ...                                   11300, 11300],\n",
      " |      ...                    'GDP': [1937894, 2583560 , 12011, 4520, 12128,\n",
      " |      ...                            17036, 182, 38, 311],\n",
      " |      ...                    'alpha-2': [\"IT\", \"FR\", \"MT\", \"MV\", \"BN\",\n",
      " |      ...                                \"IS\", \"NR\", \"TV\", \"AI\"]},\n",
      " |      ...                   index=[\"Italy\", \"France\", \"Malta\",\n",
      " |      ...                          \"Maldives\", \"Brunei\", \"Iceland\",\n",
      " |      ...                          \"Nauru\", \"Tuvalu\", \"Anguilla\"])\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |                population      GDP alpha-2\n",
      " |      Italy       59000000  1937894      IT\n",
      " |      France      65000000  2583560      FR\n",
      " |      Malta         434000    12011      MT\n",
      " |      Maldives      434000     4520      MV\n",
      " |      Brunei        434000    12128      BN\n",
      " |      Iceland       337000    17036      IS\n",
      " |      Nauru          11300      182      NR\n",
      " |      Tuvalu         11300       38      TV\n",
      " |      Anguilla       11300      311      AI\n",
      " |      \n",
      " |      In the following example, we will use ``nlargest`` to select the three\n",
      " |      rows having the largest values in column \"population\".\n",
      " |      \n",
      " |      >>> df.nlargest(3, 'population')  # doctest: +SKIP\n",
      " |              population      GDP alpha-2\n",
      " |      France    65000000  2583560      FR\n",
      " |      Italy     59000000  1937894      IT\n",
      " |      Malta       434000    12011      MT\n",
      " |      \n",
      " |      When using ``keep='last'``, ties are resolved in reverse order:\n",
      " |      \n",
      " |      >>> df.nlargest(3, 'population', keep='last')  # doctest: +SKIP\n",
      " |              population      GDP alpha-2\n",
      " |      France    65000000  2583560      FR\n",
      " |      Italy     59000000  1937894      IT\n",
      " |      Brunei      434000    12128      BN\n",
      " |      \n",
      " |      When using ``keep='all'``, all duplicate items are maintained:\n",
      " |      \n",
      " |      >>> df.nlargest(3, 'population', keep='all')  # doctest: +SKIP\n",
      " |                population      GDP alpha-2\n",
      " |      France      65000000  2583560      FR\n",
      " |      Italy       59000000  1937894      IT\n",
      " |      Malta         434000    12011      MT\n",
      " |      Maldives      434000     4520      MV\n",
      " |      Brunei        434000    12128      BN\n",
      " |      \n",
      " |      To order by the largest values in column \"population\" and then \"GDP\",\n",
      " |      we can specify multiple columns like in the next example.\n",
      " |      \n",
      " |      >>> df.nlargest(3, ['population', 'GDP'])  # doctest: +SKIP\n",
      " |              population      GDP alpha-2\n",
      " |      France    65000000  2583560      FR\n",
      " |      Italy     59000000  1937894      IT\n",
      " |      Brunei      434000    12128      BN\n",
      " |  \n",
      " |  nsmallest(self, n=5, columns=None, split_every=None)\n",
      " |      Return the first `n` rows ordered by `columns` in ascending order.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.nsmallest.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Return the first `n` rows with the smallest values in `columns`, in\n",
      " |      ascending order. The columns that are not specified are returned as\n",
      " |      well, but not used for ordering.\n",
      " |      \n",
      " |      This method is equivalent to\n",
      " |      ``df.sort_values(columns, ascending=True).head(n)``, but more\n",
      " |      performant.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : int\n",
      " |          Number of items to retrieve.\n",
      " |      columns : list or str\n",
      " |          Column name or names to order by.\n",
      " |      keep : {'first', 'last', 'all'}, default 'first'  (Not supported in Dask)\n",
      " |          Where there are duplicate values:\n",
      " |      \n",
      " |          - ``first`` : take the first occurrence.\n",
      " |          - ``last`` : take the last occurrence.\n",
      " |          - ``all`` : do not drop any duplicates, even it means\n",
      " |            selecting more than `n` items.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.nlargest : Return the first `n` rows ordered by `columns` in\n",
      " |          descending order.\n",
      " |      DataFrame.sort_values : Sort DataFrame by the values.\n",
      " |      DataFrame.head : Return the first `n` rows without re-ordering.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'population': [59000000, 65000000, 434000,  # doctest: +SKIP\n",
      " |      ...                                   434000, 434000, 337000, 337000,\n",
      " |      ...                                   11300, 11300],\n",
      " |      ...                    'GDP': [1937894, 2583560 , 12011, 4520, 12128,\n",
      " |      ...                            17036, 182, 38, 311],\n",
      " |      ...                    'alpha-2': [\"IT\", \"FR\", \"MT\", \"MV\", \"BN\",\n",
      " |      ...                                \"IS\", \"NR\", \"TV\", \"AI\"]},\n",
      " |      ...                   index=[\"Italy\", \"France\", \"Malta\",\n",
      " |      ...                          \"Maldives\", \"Brunei\", \"Iceland\",\n",
      " |      ...                          \"Nauru\", \"Tuvalu\", \"Anguilla\"])\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |                population      GDP alpha-2\n",
      " |      Italy       59000000  1937894      IT\n",
      " |      France      65000000  2583560      FR\n",
      " |      Malta         434000    12011      MT\n",
      " |      Maldives      434000     4520      MV\n",
      " |      Brunei        434000    12128      BN\n",
      " |      Iceland       337000    17036      IS\n",
      " |      Nauru         337000      182      NR\n",
      " |      Tuvalu         11300       38      TV\n",
      " |      Anguilla       11300      311      AI\n",
      " |      \n",
      " |      In the following example, we will use ``nsmallest`` to select the\n",
      " |      three rows having the smallest values in column \"population\".\n",
      " |      \n",
      " |      >>> df.nsmallest(3, 'population')  # doctest: +SKIP\n",
      " |                population    GDP alpha-2\n",
      " |      Tuvalu         11300     38      TV\n",
      " |      Anguilla       11300    311      AI\n",
      " |      Iceland       337000  17036      IS\n",
      " |      \n",
      " |      When using ``keep='last'``, ties are resolved in reverse order:\n",
      " |      \n",
      " |      >>> df.nsmallest(3, 'population', keep='last')  # doctest: +SKIP\n",
      " |                population  GDP alpha-2\n",
      " |      Anguilla       11300  311      AI\n",
      " |      Tuvalu         11300   38      TV\n",
      " |      Nauru         337000  182      NR\n",
      " |      \n",
      " |      When using ``keep='all'``, all duplicate items are maintained:\n",
      " |      \n",
      " |      >>> df.nsmallest(3, 'population', keep='all')  # doctest: +SKIP\n",
      " |                population    GDP alpha-2\n",
      " |      Tuvalu         11300     38      TV\n",
      " |      Anguilla       11300    311      AI\n",
      " |      Iceland       337000  17036      IS\n",
      " |      Nauru         337000    182      NR\n",
      " |      \n",
      " |      To order by the smallest values in column \"population\" and then \"GDP\", we can\n",
      " |      specify multiple columns like in the next example.\n",
      " |      \n",
      " |      >>> df.nsmallest(3, ['population', 'GDP'])  # doctest: +SKIP\n",
      " |                population  GDP alpha-2\n",
      " |      Tuvalu         11300   38      TV\n",
      " |      Anguilla       11300  311      AI\n",
      " |      Nauru         337000  182      NR\n",
      " |  \n",
      " |  nunique(self, split_every=False, dropna=True, axis=0)\n",
      " |      Count number of distinct elements in specified axis.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.nunique.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Return Series with number of distinct elements. Can ignore NaN\n",
      " |      values.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for\n",
      " |          column-wise.\n",
      " |      dropna : bool, default True\n",
      " |          Don't include NaN in the counts.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.nunique: Method nunique for Series.\n",
      " |      DataFrame.count: Count non-NA cells for each column or row.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'A': [4, 5, 6], 'B': [4, 1, 1]})  # doctest: +SKIP\n",
      " |      >>> df.nunique()  # doctest: +SKIP\n",
      " |      A    3\n",
      " |      B    2\n",
      " |      dtype: int64\n",
      " |      \n",
      " |      >>> df.nunique(axis=1)  # doctest: +SKIP\n",
      " |      0    1\n",
      " |      1    2\n",
      " |      2    2\n",
      " |      dtype: int64\n",
      " |  \n",
      " |  pivot_table(self, index=None, columns=None, values=None, aggfunc='mean')\n",
      " |      Create a spreadsheet-style pivot table as a DataFrame. Target ``columns``\n",
      " |      must have category dtype to infer result's ``columns``.\n",
      " |      ``index``, ``columns``, ``values`` and ``aggfunc`` must be all scalar.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      values : scalar\n",
      " |          column to aggregate\n",
      " |      index : scalar\n",
      " |          column to be index\n",
      " |      columns : scalar\n",
      " |          column to be columns\n",
      " |      aggfunc : {'mean', 'sum', 'count'}, default 'mean'\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      table : DataFrame\n",
      " |  \n",
      " |  pop(self, item)\n",
      " |      Return item and drop from frame. Raise KeyError if not found.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.pop.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      item : label\n",
      " |          Label of column to be popped.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame([('falcon', 'bird', 389.0),  # doctest: +SKIP\n",
      " |      ...                    ('parrot', 'bird', 24.0),\n",
      " |      ...                    ('lion', 'mammal', 80.5),\n",
      " |      ...                    ('monkey', 'mammal', np.nan)],\n",
      " |      ...                   columns=('name', 'class', 'max_speed'))\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |           name   class  max_speed\n",
      " |      0  falcon    bird      389.0\n",
      " |      1  parrot    bird       24.0\n",
      " |      2    lion  mammal       80.5\n",
      " |      3  monkey  mammal        NaN\n",
      " |      \n",
      " |      >>> df.pop('class')  # doctest: +SKIP\n",
      " |      0      bird\n",
      " |      1      bird\n",
      " |      2    mammal\n",
      " |      3    mammal\n",
      " |      Name: class, dtype: object\n",
      " |      \n",
      " |      >>> df  # doctest: +SKIP\n",
      " |           name  max_speed\n",
      " |      0  falcon      389.0\n",
      " |      1  parrot       24.0\n",
      " |      2    lion       80.5\n",
      " |      3  monkey        NaN\n",
      " |  \n",
      " |  pow(self, other, axis='columns', level=None, fill_value=None)\n",
      " |      Get Exponential power of dataframe and other, element-wise (binary operator `pow`).\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.pow.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Equivalent to ``dataframe ** other``, but with support to substitute a fill_value\n",
      " |      for missing data in one of the inputs. With reverse version, `rpow`.\n",
      " |      \n",
      " |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `mod`, `pow`) to\n",
      " |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}\n",
      " |          Whether to compare by the index (0 or 'index') or columns\n",
      " |          (1 or 'columns'). For Series input, axis to match Series index on.\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      fill_value : float or None, default None\n",
      " |          Fill existing missing (NaN) values, and any new element needed for\n",
      " |          successful DataFrame alignment, with this value before computation.\n",
      " |          If data in both corresponding DataFrame locations is missing\n",
      " |          the result will be missing.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Result of the arithmetic operation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.add : Add DataFrames.\n",
      " |      DataFrame.sub : Subtract DataFrames.\n",
      " |      DataFrame.mul : Multiply DataFrames.\n",
      " |      DataFrame.div : Divide DataFrames (float division).\n",
      " |      DataFrame.truediv : Divide DataFrames (float division).\n",
      " |      DataFrame.floordiv : Divide DataFrames (integer division).\n",
      " |      DataFrame.mod : Calculate modulo (remainder after division).\n",
      " |      DataFrame.pow : Calculate exponential power.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'angles': [0, 3, 4],  # doctest: +SKIP\n",
      " |      ...                    'degrees': [360, 180, 360]},\n",
      " |      ...                   index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      360\n",
      " |      triangle        3      180\n",
      " |      rectangle       4      360\n",
      " |      \n",
      " |      Add a scalar with operator version which return the same\n",
      " |      results.\n",
      " |      \n",
      " |      >>> df + 1  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      >>> df.add(1)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      Divide by constant with reverse version.\n",
      " |      \n",
      " |      >>> df.div(10)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle        0.0     36.0\n",
      " |      triangle      0.3     18.0\n",
      " |      rectangle     0.4     36.0\n",
      " |      \n",
      " |      >>> df.rdiv(10)  # doctest: +SKIP\n",
      " |                   angles   degrees\n",
      " |      circle          inf  0.027778\n",
      " |      triangle   3.333333  0.055556\n",
      " |      rectangle  2.500000  0.027778\n",
      " |      \n",
      " |      Subtract a list and Series by axis with operator version.\n",
      " |      \n",
      " |      >>> df - [1, 2]  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub([1, 2], axis='columns')  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),  # doctest: +SKIP\n",
      " |      ...        axis='index')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      359\n",
      " |      triangle        2      179\n",
      " |      rectangle       3      359\n",
      " |      \n",
      " |      Multiply a DataFrame of different shape with operator version.\n",
      " |      \n",
      " |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},  # doctest: +SKIP\n",
      " |      ...                      index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> other  # doctest: +SKIP\n",
      " |                 angles\n",
      " |      circle          0\n",
      " |      triangle        3\n",
      " |      rectangle       4\n",
      " |      \n",
      " |      >>> df * other  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      NaN\n",
      " |      triangle        9      NaN\n",
      " |      rectangle      16      NaN\n",
      " |      \n",
      " |      >>> df.mul(other, fill_value=0)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      0.0\n",
      " |      triangle        9      0.0\n",
      " |      rectangle      16      0.0\n",
      " |      \n",
      " |      Divide by a MultiIndex by level.\n",
      " |      \n",
      " |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],  # doctest: +SKIP\n",
      " |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n",
      " |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n",
      " |      ...                                    ['circle', 'triangle', 'rectangle',\n",
      " |      ...                                     'square', 'pentagon', 'hexagon']])\n",
      " |      >>> df_multindex  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle          0      360\n",
      " |        triangle        3      180\n",
      " |        rectangle       4      360\n",
      " |      B square          4      360\n",
      " |        pentagon        5      540\n",
      " |        hexagon         6      720\n",
      " |      \n",
      " |      >>> df.div(df_multindex, level=1, fill_value=0)  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle        NaN      1.0\n",
      " |        triangle      1.0      1.0\n",
      " |        rectangle     1.0      1.0\n",
      " |      B square        0.0      0.0\n",
      " |        pentagon      0.0      0.0\n",
      " |        hexagon       0.0      0.0\n",
      " |  \n",
      " |  query(self, expr, **kwargs)\n",
      " |      Filter dataframe with complex expression\n",
      " |      \n",
      " |      Blocked version of pd.DataFrame.query\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      expr: str\n",
      " |          The query string to evaluate.\n",
      " |          You can refer to column names that are not valid Python variable names\n",
      " |          by surrounding them in backticks.\n",
      " |          Dask does not fully support referring to variables using the '@' character,\n",
      " |          use f-strings or the ``local_dict`` keyword argument instead.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This is like the sequential version except that this will also happen\n",
      " |      in many threads.  This may conflict with ``numexpr`` which will use\n",
      " |      multiple threads itself.  We recommend that you set ``numexpr`` to use a\n",
      " |      single thread:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          import numexpr\n",
      " |          numexpr.set_num_threads(1)\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      pandas.DataFrame.query\n",
      " |      pandas.eval\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import pandas as pd\n",
      " |      >>> import dask.dataframe as dd\n",
      " |      >>> df = pd.DataFrame({'x': [1, 2, 1, 2],\n",
      " |      ...                    'y': [1, 2, 3, 4],\n",
      " |      ...                    'z z': [4, 3, 2, 1]})\n",
      " |      >>> ddf = dd.from_pandas(df, npartitions=2)\n",
      " |      \n",
      " |      Refer to column names directly:\n",
      " |      \n",
      " |      >>> ddf.query('y > x').compute()\n",
      " |         x  y  z z\n",
      " |      2  1  3    2\n",
      " |      3  2  4    1\n",
      " |      \n",
      " |      Refer to column name using backticks:\n",
      " |      \n",
      " |      >>> ddf.query('`z z` > x').compute()\n",
      " |         x  y  z z\n",
      " |      0  1  1    4\n",
      " |      1  2  2    3\n",
      " |      2  1  3    2\n",
      " |      \n",
      " |      Refer to variable name using f-strings:\n",
      " |      \n",
      " |      >>> value = 1\n",
      " |      >>> ddf.query(f'x == {value}').compute()\n",
      " |         x  y  z z\n",
      " |      0  1  1    4\n",
      " |      2  1  3    2\n",
      " |      \n",
      " |      Refer to variable name using ``local_dict``:\n",
      " |      \n",
      " |      >>> ddf.query('x == @value', local_dict={\"value\": value}).compute()\n",
      " |         x  y  z z\n",
      " |      0  1  1    4\n",
      " |      2  1  3    2\n",
      " |  \n",
      " |  radd(self, other, axis='columns', level=None, fill_value=None)\n",
      " |      Get Addition of dataframe and other, element-wise (binary operator `radd`).\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.radd.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Equivalent to ``other + dataframe``, but with support to substitute a fill_value\n",
      " |      for missing data in one of the inputs. With reverse version, `add`.\n",
      " |      \n",
      " |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `mod`, `pow`) to\n",
      " |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}\n",
      " |          Whether to compare by the index (0 or 'index') or columns\n",
      " |          (1 or 'columns'). For Series input, axis to match Series index on.\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      fill_value : float or None, default None\n",
      " |          Fill existing missing (NaN) values, and any new element needed for\n",
      " |          successful DataFrame alignment, with this value before computation.\n",
      " |          If data in both corresponding DataFrame locations is missing\n",
      " |          the result will be missing.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Result of the arithmetic operation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.add : Add DataFrames.\n",
      " |      DataFrame.sub : Subtract DataFrames.\n",
      " |      DataFrame.mul : Multiply DataFrames.\n",
      " |      DataFrame.div : Divide DataFrames (float division).\n",
      " |      DataFrame.truediv : Divide DataFrames (float division).\n",
      " |      DataFrame.floordiv : Divide DataFrames (integer division).\n",
      " |      DataFrame.mod : Calculate modulo (remainder after division).\n",
      " |      DataFrame.pow : Calculate exponential power.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'angles': [0, 3, 4],  # doctest: +SKIP\n",
      " |      ...                    'degrees': [360, 180, 360]},\n",
      " |      ...                   index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      360\n",
      " |      triangle        3      180\n",
      " |      rectangle       4      360\n",
      " |      \n",
      " |      Add a scalar with operator version which return the same\n",
      " |      results.\n",
      " |      \n",
      " |      >>> df + 1  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      >>> df.add(1)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      Divide by constant with reverse version.\n",
      " |      \n",
      " |      >>> df.div(10)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle        0.0     36.0\n",
      " |      triangle      0.3     18.0\n",
      " |      rectangle     0.4     36.0\n",
      " |      \n",
      " |      >>> df.rdiv(10)  # doctest: +SKIP\n",
      " |                   angles   degrees\n",
      " |      circle          inf  0.027778\n",
      " |      triangle   3.333333  0.055556\n",
      " |      rectangle  2.500000  0.027778\n",
      " |      \n",
      " |      Subtract a list and Series by axis with operator version.\n",
      " |      \n",
      " |      >>> df - [1, 2]  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub([1, 2], axis='columns')  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),  # doctest: +SKIP\n",
      " |      ...        axis='index')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      359\n",
      " |      triangle        2      179\n",
      " |      rectangle       3      359\n",
      " |      \n",
      " |      Multiply a DataFrame of different shape with operator version.\n",
      " |      \n",
      " |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},  # doctest: +SKIP\n",
      " |      ...                      index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> other  # doctest: +SKIP\n",
      " |                 angles\n",
      " |      circle          0\n",
      " |      triangle        3\n",
      " |      rectangle       4\n",
      " |      \n",
      " |      >>> df * other  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      NaN\n",
      " |      triangle        9      NaN\n",
      " |      rectangle      16      NaN\n",
      " |      \n",
      " |      >>> df.mul(other, fill_value=0)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      0.0\n",
      " |      triangle        9      0.0\n",
      " |      rectangle      16      0.0\n",
      " |      \n",
      " |      Divide by a MultiIndex by level.\n",
      " |      \n",
      " |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],  # doctest: +SKIP\n",
      " |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n",
      " |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n",
      " |      ...                                    ['circle', 'triangle', 'rectangle',\n",
      " |      ...                                     'square', 'pentagon', 'hexagon']])\n",
      " |      >>> df_multindex  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle          0      360\n",
      " |        triangle        3      180\n",
      " |        rectangle       4      360\n",
      " |      B square          4      360\n",
      " |        pentagon        5      540\n",
      " |        hexagon         6      720\n",
      " |      \n",
      " |      >>> df.div(df_multindex, level=1, fill_value=0)  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle        NaN      1.0\n",
      " |        triangle      1.0      1.0\n",
      " |        rectangle     1.0      1.0\n",
      " |      B square        0.0      0.0\n",
      " |        pentagon      0.0      0.0\n",
      " |        hexagon       0.0      0.0\n",
      " |  \n",
      " |  rdiv(self, other, axis='columns', level=None, fill_value=None)\n",
      " |      Get Floating division of dataframe and other, element-wise (binary operator `rtruediv`).\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.rdiv.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Equivalent to ``other / dataframe``, but with support to substitute a fill_value\n",
      " |      for missing data in one of the inputs. With reverse version, `truediv`.\n",
      " |      \n",
      " |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `mod`, `pow`) to\n",
      " |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}\n",
      " |          Whether to compare by the index (0 or 'index') or columns\n",
      " |          (1 or 'columns'). For Series input, axis to match Series index on.\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      fill_value : float or None, default None\n",
      " |          Fill existing missing (NaN) values, and any new element needed for\n",
      " |          successful DataFrame alignment, with this value before computation.\n",
      " |          If data in both corresponding DataFrame locations is missing\n",
      " |          the result will be missing.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Result of the arithmetic operation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.add : Add DataFrames.\n",
      " |      DataFrame.sub : Subtract DataFrames.\n",
      " |      DataFrame.mul : Multiply DataFrames.\n",
      " |      DataFrame.div : Divide DataFrames (float division).\n",
      " |      DataFrame.truediv : Divide DataFrames (float division).\n",
      " |      DataFrame.floordiv : Divide DataFrames (integer division).\n",
      " |      DataFrame.mod : Calculate modulo (remainder after division).\n",
      " |      DataFrame.pow : Calculate exponential power.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'angles': [0, 3, 4],  # doctest: +SKIP\n",
      " |      ...                    'degrees': [360, 180, 360]},\n",
      " |      ...                   index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      360\n",
      " |      triangle        3      180\n",
      " |      rectangle       4      360\n",
      " |      \n",
      " |      Add a scalar with operator version which return the same\n",
      " |      results.\n",
      " |      \n",
      " |      >>> df + 1  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      >>> df.add(1)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      Divide by constant with reverse version.\n",
      " |      \n",
      " |      >>> df.div(10)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle        0.0     36.0\n",
      " |      triangle      0.3     18.0\n",
      " |      rectangle     0.4     36.0\n",
      " |      \n",
      " |      >>> df.rdiv(10)  # doctest: +SKIP\n",
      " |                   angles   degrees\n",
      " |      circle          inf  0.027778\n",
      " |      triangle   3.333333  0.055556\n",
      " |      rectangle  2.500000  0.027778\n",
      " |      \n",
      " |      Subtract a list and Series by axis with operator version.\n",
      " |      \n",
      " |      >>> df - [1, 2]  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub([1, 2], axis='columns')  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),  # doctest: +SKIP\n",
      " |      ...        axis='index')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      359\n",
      " |      triangle        2      179\n",
      " |      rectangle       3      359\n",
      " |      \n",
      " |      Multiply a DataFrame of different shape with operator version.\n",
      " |      \n",
      " |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},  # doctest: +SKIP\n",
      " |      ...                      index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> other  # doctest: +SKIP\n",
      " |                 angles\n",
      " |      circle          0\n",
      " |      triangle        3\n",
      " |      rectangle       4\n",
      " |      \n",
      " |      >>> df * other  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      NaN\n",
      " |      triangle        9      NaN\n",
      " |      rectangle      16      NaN\n",
      " |      \n",
      " |      >>> df.mul(other, fill_value=0)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      0.0\n",
      " |      triangle        9      0.0\n",
      " |      rectangle      16      0.0\n",
      " |      \n",
      " |      Divide by a MultiIndex by level.\n",
      " |      \n",
      " |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],  # doctest: +SKIP\n",
      " |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n",
      " |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n",
      " |      ...                                    ['circle', 'triangle', 'rectangle',\n",
      " |      ...                                     'square', 'pentagon', 'hexagon']])\n",
      " |      >>> df_multindex  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle          0      360\n",
      " |        triangle        3      180\n",
      " |        rectangle       4      360\n",
      " |      B square          4      360\n",
      " |        pentagon        5      540\n",
      " |        hexagon         6      720\n",
      " |      \n",
      " |      >>> df.div(df_multindex, level=1, fill_value=0)  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle        NaN      1.0\n",
      " |        triangle      1.0      1.0\n",
      " |        rectangle     1.0      1.0\n",
      " |      B square        0.0      0.0\n",
      " |        pentagon      0.0      0.0\n",
      " |        hexagon       0.0      0.0\n",
      " |  \n",
      " |  rename(self, index=None, columns=None)\n",
      " |      Alter axes labels.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.rename.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Function / dict values must be unique (1-to-1). Labels not contained in\n",
      " |      a dict / Series will be left as-is. Extra labels listed don't throw an\n",
      " |      error.\n",
      " |      \n",
      " |      See the :ref:`user guide <basics.rename>` for more.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      mapper : dict-like or function  (Not supported in Dask)\n",
      " |          Dict-like or function transformations to apply to\n",
      " |          that axis' values. Use either ``mapper`` and ``axis`` to\n",
      " |          specify the axis to target with ``mapper``, or ``index`` and\n",
      " |          ``columns``.\n",
      " |      index : dict-like or function  (Not supported in Dask)\n",
      " |          Alternative to specifying axis (``mapper, axis=0``\n",
      " |          is equivalent to ``index=mapper``).\n",
      " |      columns : dict-like or function\n",
      " |          Alternative to specifying axis (``mapper, axis=1``\n",
      " |          is equivalent to ``columns=mapper``).\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0  (Not supported in Dask)\n",
      " |          Axis to target with ``mapper``. Can be either the axis name\n",
      " |          ('index', 'columns') or number (0, 1). The default is 'index'.\n",
      " |      copy : bool, default True  (Not supported in Dask)\n",
      " |          Also copy underlying data.\n",
      " |      inplace : bool, default False  (Not supported in Dask)\n",
      " |          Whether to return a new DataFrame. If True then value of copy is\n",
      " |          ignored.\n",
      " |      level : int or level name, default None  (Not supported in Dask)\n",
      " |          In case of a MultiIndex, only rename labels in the specified\n",
      " |          level.\n",
      " |      errors : {'ignore', 'raise'}, default 'ignore'  (Not supported in Dask)\n",
      " |          If 'raise', raise a `KeyError` when a dict-like `mapper`, `index`,\n",
      " |          or `columns` contains labels that are not present in the Index\n",
      " |          being transformed.\n",
      " |          If 'ignore', existing keys will be renamed and extra keys will be\n",
      " |          ignored.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame or None\n",
      " |          DataFrame with the renamed axis labels or None if ``inplace=True``.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If any of the labels is not found in the selected axis and\n",
      " |          \"errors='raise'\".\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.rename_axis : Set the name of the axis.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      ``DataFrame.rename`` supports two calling conventions\n",
      " |      \n",
      " |      * ``(index=index_mapper, columns=columns_mapper, ...)``\n",
      " |      * ``(mapper, axis={'index', 'columns'}, ...)``\n",
      " |      \n",
      " |      We *highly* recommend using keyword arguments to clarify your\n",
      " |      intent.\n",
      " |      \n",
      " |      Rename columns using a mapping:\n",
      " |      \n",
      " |      >>> df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})  # doctest: +SKIP\n",
      " |      >>> df.rename(columns={\"A\": \"a\", \"B\": \"c\"})  # doctest: +SKIP\n",
      " |         a  c\n",
      " |      0  1  4\n",
      " |      1  2  5\n",
      " |      2  3  6\n",
      " |      \n",
      " |      Rename index using a mapping:\n",
      " |      \n",
      " |      >>> df.rename(index={0: \"x\", 1: \"y\", 2: \"z\"})  # doctest: +SKIP\n",
      " |         A  B\n",
      " |      x  1  4\n",
      " |      y  2  5\n",
      " |      z  3  6\n",
      " |      \n",
      " |      Cast index labels to a different type:\n",
      " |      \n",
      " |      >>> df.index  # doctest: +SKIP\n",
      " |      RangeIndex(start=0, stop=3, step=1)\n",
      " |      >>> df.rename(index=str).index  # doctest: +SKIP\n",
      " |      Index(['0', '1', '2'], dtype='object')\n",
      " |      \n",
      " |      >>> df.rename(columns={\"A\": \"a\", \"B\": \"b\", \"C\": \"c\"}, errors=\"raise\")  # doctest: +SKIP\n",
      " |      Traceback (most recent call last):\n",
      " |      KeyError: ['C'] not found in axis\n",
      " |      \n",
      " |      Using axis-style parameters:\n",
      " |      \n",
      " |      >>> df.rename(str.lower, axis='columns')  # doctest: +SKIP\n",
      " |         a  b\n",
      " |      0  1  4\n",
      " |      1  2  5\n",
      " |      2  3  6\n",
      " |      \n",
      " |      >>> df.rename({1: 2, 2: 4}, axis='index')  # doctest: +SKIP\n",
      " |         A  B\n",
      " |      0  1  4\n",
      " |      2  2  5\n",
      " |      4  3  6\n",
      " |  \n",
      " |  rfloordiv(self, other, axis='columns', level=None, fill_value=None)\n",
      " |      Get Integer division of dataframe and other, element-wise (binary operator `rfloordiv`).\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.rfloordiv.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Equivalent to ``other // dataframe``, but with support to substitute a fill_value\n",
      " |      for missing data in one of the inputs. With reverse version, `floordiv`.\n",
      " |      \n",
      " |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `mod`, `pow`) to\n",
      " |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}\n",
      " |          Whether to compare by the index (0 or 'index') or columns\n",
      " |          (1 or 'columns'). For Series input, axis to match Series index on.\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      fill_value : float or None, default None\n",
      " |          Fill existing missing (NaN) values, and any new element needed for\n",
      " |          successful DataFrame alignment, with this value before computation.\n",
      " |          If data in both corresponding DataFrame locations is missing\n",
      " |          the result will be missing.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Result of the arithmetic operation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.add : Add DataFrames.\n",
      " |      DataFrame.sub : Subtract DataFrames.\n",
      " |      DataFrame.mul : Multiply DataFrames.\n",
      " |      DataFrame.div : Divide DataFrames (float division).\n",
      " |      DataFrame.truediv : Divide DataFrames (float division).\n",
      " |      DataFrame.floordiv : Divide DataFrames (integer division).\n",
      " |      DataFrame.mod : Calculate modulo (remainder after division).\n",
      " |      DataFrame.pow : Calculate exponential power.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'angles': [0, 3, 4],  # doctest: +SKIP\n",
      " |      ...                    'degrees': [360, 180, 360]},\n",
      " |      ...                   index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      360\n",
      " |      triangle        3      180\n",
      " |      rectangle       4      360\n",
      " |      \n",
      " |      Add a scalar with operator version which return the same\n",
      " |      results.\n",
      " |      \n",
      " |      >>> df + 1  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      >>> df.add(1)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      Divide by constant with reverse version.\n",
      " |      \n",
      " |      >>> df.div(10)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle        0.0     36.0\n",
      " |      triangle      0.3     18.0\n",
      " |      rectangle     0.4     36.0\n",
      " |      \n",
      " |      >>> df.rdiv(10)  # doctest: +SKIP\n",
      " |                   angles   degrees\n",
      " |      circle          inf  0.027778\n",
      " |      triangle   3.333333  0.055556\n",
      " |      rectangle  2.500000  0.027778\n",
      " |      \n",
      " |      Subtract a list and Series by axis with operator version.\n",
      " |      \n",
      " |      >>> df - [1, 2]  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub([1, 2], axis='columns')  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),  # doctest: +SKIP\n",
      " |      ...        axis='index')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      359\n",
      " |      triangle        2      179\n",
      " |      rectangle       3      359\n",
      " |      \n",
      " |      Multiply a DataFrame of different shape with operator version.\n",
      " |      \n",
      " |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},  # doctest: +SKIP\n",
      " |      ...                      index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> other  # doctest: +SKIP\n",
      " |                 angles\n",
      " |      circle          0\n",
      " |      triangle        3\n",
      " |      rectangle       4\n",
      " |      \n",
      " |      >>> df * other  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      NaN\n",
      " |      triangle        9      NaN\n",
      " |      rectangle      16      NaN\n",
      " |      \n",
      " |      >>> df.mul(other, fill_value=0)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      0.0\n",
      " |      triangle        9      0.0\n",
      " |      rectangle      16      0.0\n",
      " |      \n",
      " |      Divide by a MultiIndex by level.\n",
      " |      \n",
      " |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],  # doctest: +SKIP\n",
      " |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n",
      " |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n",
      " |      ...                                    ['circle', 'triangle', 'rectangle',\n",
      " |      ...                                     'square', 'pentagon', 'hexagon']])\n",
      " |      >>> df_multindex  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle          0      360\n",
      " |        triangle        3      180\n",
      " |        rectangle       4      360\n",
      " |      B square          4      360\n",
      " |        pentagon        5      540\n",
      " |        hexagon         6      720\n",
      " |      \n",
      " |      >>> df.div(df_multindex, level=1, fill_value=0)  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle        NaN      1.0\n",
      " |        triangle      1.0      1.0\n",
      " |        rectangle     1.0      1.0\n",
      " |      B square        0.0      0.0\n",
      " |        pentagon      0.0      0.0\n",
      " |        hexagon       0.0      0.0\n",
      " |  \n",
      " |  rmod(self, other, axis='columns', level=None, fill_value=None)\n",
      " |      Get Modulo of dataframe and other, element-wise (binary operator `rmod`).\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.rmod.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Equivalent to ``other % dataframe``, but with support to substitute a fill_value\n",
      " |      for missing data in one of the inputs. With reverse version, `mod`.\n",
      " |      \n",
      " |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `mod`, `pow`) to\n",
      " |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}\n",
      " |          Whether to compare by the index (0 or 'index') or columns\n",
      " |          (1 or 'columns'). For Series input, axis to match Series index on.\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      fill_value : float or None, default None\n",
      " |          Fill existing missing (NaN) values, and any new element needed for\n",
      " |          successful DataFrame alignment, with this value before computation.\n",
      " |          If data in both corresponding DataFrame locations is missing\n",
      " |          the result will be missing.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Result of the arithmetic operation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.add : Add DataFrames.\n",
      " |      DataFrame.sub : Subtract DataFrames.\n",
      " |      DataFrame.mul : Multiply DataFrames.\n",
      " |      DataFrame.div : Divide DataFrames (float division).\n",
      " |      DataFrame.truediv : Divide DataFrames (float division).\n",
      " |      DataFrame.floordiv : Divide DataFrames (integer division).\n",
      " |      DataFrame.mod : Calculate modulo (remainder after division).\n",
      " |      DataFrame.pow : Calculate exponential power.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'angles': [0, 3, 4],  # doctest: +SKIP\n",
      " |      ...                    'degrees': [360, 180, 360]},\n",
      " |      ...                   index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      360\n",
      " |      triangle        3      180\n",
      " |      rectangle       4      360\n",
      " |      \n",
      " |      Add a scalar with operator version which return the same\n",
      " |      results.\n",
      " |      \n",
      " |      >>> df + 1  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      >>> df.add(1)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      Divide by constant with reverse version.\n",
      " |      \n",
      " |      >>> df.div(10)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle        0.0     36.0\n",
      " |      triangle      0.3     18.0\n",
      " |      rectangle     0.4     36.0\n",
      " |      \n",
      " |      >>> df.rdiv(10)  # doctest: +SKIP\n",
      " |                   angles   degrees\n",
      " |      circle          inf  0.027778\n",
      " |      triangle   3.333333  0.055556\n",
      " |      rectangle  2.500000  0.027778\n",
      " |      \n",
      " |      Subtract a list and Series by axis with operator version.\n",
      " |      \n",
      " |      >>> df - [1, 2]  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub([1, 2], axis='columns')  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),  # doctest: +SKIP\n",
      " |      ...        axis='index')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      359\n",
      " |      triangle        2      179\n",
      " |      rectangle       3      359\n",
      " |      \n",
      " |      Multiply a DataFrame of different shape with operator version.\n",
      " |      \n",
      " |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},  # doctest: +SKIP\n",
      " |      ...                      index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> other  # doctest: +SKIP\n",
      " |                 angles\n",
      " |      circle          0\n",
      " |      triangle        3\n",
      " |      rectangle       4\n",
      " |      \n",
      " |      >>> df * other  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      NaN\n",
      " |      triangle        9      NaN\n",
      " |      rectangle      16      NaN\n",
      " |      \n",
      " |      >>> df.mul(other, fill_value=0)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      0.0\n",
      " |      triangle        9      0.0\n",
      " |      rectangle      16      0.0\n",
      " |      \n",
      " |      Divide by a MultiIndex by level.\n",
      " |      \n",
      " |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],  # doctest: +SKIP\n",
      " |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n",
      " |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n",
      " |      ...                                    ['circle', 'triangle', 'rectangle',\n",
      " |      ...                                     'square', 'pentagon', 'hexagon']])\n",
      " |      >>> df_multindex  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle          0      360\n",
      " |        triangle        3      180\n",
      " |        rectangle       4      360\n",
      " |      B square          4      360\n",
      " |        pentagon        5      540\n",
      " |        hexagon         6      720\n",
      " |      \n",
      " |      >>> df.div(df_multindex, level=1, fill_value=0)  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle        NaN      1.0\n",
      " |        triangle      1.0      1.0\n",
      " |        rectangle     1.0      1.0\n",
      " |      B square        0.0      0.0\n",
      " |        pentagon      0.0      0.0\n",
      " |        hexagon       0.0      0.0\n",
      " |  \n",
      " |  rmul(self, other, axis='columns', level=None, fill_value=None)\n",
      " |      Get Multiplication of dataframe and other, element-wise (binary operator `rmul`).\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.rmul.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Equivalent to ``other * dataframe``, but with support to substitute a fill_value\n",
      " |      for missing data in one of the inputs. With reverse version, `mul`.\n",
      " |      \n",
      " |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `mod`, `pow`) to\n",
      " |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}\n",
      " |          Whether to compare by the index (0 or 'index') or columns\n",
      " |          (1 or 'columns'). For Series input, axis to match Series index on.\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      fill_value : float or None, default None\n",
      " |          Fill existing missing (NaN) values, and any new element needed for\n",
      " |          successful DataFrame alignment, with this value before computation.\n",
      " |          If data in both corresponding DataFrame locations is missing\n",
      " |          the result will be missing.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Result of the arithmetic operation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.add : Add DataFrames.\n",
      " |      DataFrame.sub : Subtract DataFrames.\n",
      " |      DataFrame.mul : Multiply DataFrames.\n",
      " |      DataFrame.div : Divide DataFrames (float division).\n",
      " |      DataFrame.truediv : Divide DataFrames (float division).\n",
      " |      DataFrame.floordiv : Divide DataFrames (integer division).\n",
      " |      DataFrame.mod : Calculate modulo (remainder after division).\n",
      " |      DataFrame.pow : Calculate exponential power.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'angles': [0, 3, 4],  # doctest: +SKIP\n",
      " |      ...                    'degrees': [360, 180, 360]},\n",
      " |      ...                   index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      360\n",
      " |      triangle        3      180\n",
      " |      rectangle       4      360\n",
      " |      \n",
      " |      Add a scalar with operator version which return the same\n",
      " |      results.\n",
      " |      \n",
      " |      >>> df + 1  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      >>> df.add(1)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      Divide by constant with reverse version.\n",
      " |      \n",
      " |      >>> df.div(10)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle        0.0     36.0\n",
      " |      triangle      0.3     18.0\n",
      " |      rectangle     0.4     36.0\n",
      " |      \n",
      " |      >>> df.rdiv(10)  # doctest: +SKIP\n",
      " |                   angles   degrees\n",
      " |      circle          inf  0.027778\n",
      " |      triangle   3.333333  0.055556\n",
      " |      rectangle  2.500000  0.027778\n",
      " |      \n",
      " |      Subtract a list and Series by axis with operator version.\n",
      " |      \n",
      " |      >>> df - [1, 2]  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub([1, 2], axis='columns')  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),  # doctest: +SKIP\n",
      " |      ...        axis='index')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      359\n",
      " |      triangle        2      179\n",
      " |      rectangle       3      359\n",
      " |      \n",
      " |      Multiply a DataFrame of different shape with operator version.\n",
      " |      \n",
      " |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},  # doctest: +SKIP\n",
      " |      ...                      index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> other  # doctest: +SKIP\n",
      " |                 angles\n",
      " |      circle          0\n",
      " |      triangle        3\n",
      " |      rectangle       4\n",
      " |      \n",
      " |      >>> df * other  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      NaN\n",
      " |      triangle        9      NaN\n",
      " |      rectangle      16      NaN\n",
      " |      \n",
      " |      >>> df.mul(other, fill_value=0)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      0.0\n",
      " |      triangle        9      0.0\n",
      " |      rectangle      16      0.0\n",
      " |      \n",
      " |      Divide by a MultiIndex by level.\n",
      " |      \n",
      " |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],  # doctest: +SKIP\n",
      " |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n",
      " |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n",
      " |      ...                                    ['circle', 'triangle', 'rectangle',\n",
      " |      ...                                     'square', 'pentagon', 'hexagon']])\n",
      " |      >>> df_multindex  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle          0      360\n",
      " |        triangle        3      180\n",
      " |        rectangle       4      360\n",
      " |      B square          4      360\n",
      " |        pentagon        5      540\n",
      " |        hexagon         6      720\n",
      " |      \n",
      " |      >>> df.div(df_multindex, level=1, fill_value=0)  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle        NaN      1.0\n",
      " |        triangle      1.0      1.0\n",
      " |        rectangle     1.0      1.0\n",
      " |      B square        0.0      0.0\n",
      " |        pentagon      0.0      0.0\n",
      " |        hexagon       0.0      0.0\n",
      " |  \n",
      " |  round(self, decimals=0)\n",
      " |      Round a DataFrame to a variable number of decimal places.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.round.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      decimals : int, dict, Series\n",
      " |          Number of decimal places to round each column to. If an int is\n",
      " |          given, round each column to the same number of places.\n",
      " |          Otherwise dict and Series round to variable numbers of places.\n",
      " |          Column names should be in the keys if `decimals` is a\n",
      " |          dict-like, or in the index if `decimals` is a Series. Any\n",
      " |          columns not included in `decimals` will be left as is. Elements\n",
      " |          of `decimals` which are not columns of the input will be\n",
      " |          ignored.\n",
      " |      *args\n",
      " |          Additional keywords have no effect but might be accepted for\n",
      " |          compatibility with numpy.\n",
      " |      **kwargs\n",
      " |          Additional keywords have no effect but might be accepted for\n",
      " |          compatibility with numpy.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          A DataFrame with the affected columns rounded to the specified\n",
      " |          number of decimal places.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.around : Round a numpy array to the given number of decimals.\n",
      " |      Series.round : Round a Series to the given number of decimals.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame([(.21, .32), (.01, .67), (.66, .03), (.21, .18)],  # doctest: +SKIP\n",
      " |      ...                   columns=['dogs', 'cats'])\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |          dogs  cats\n",
      " |      0  0.21  0.32\n",
      " |      1  0.01  0.67\n",
      " |      2  0.66  0.03\n",
      " |      3  0.21  0.18\n",
      " |      \n",
      " |      By providing an integer each column is rounded to the same number\n",
      " |      of decimal places\n",
      " |      \n",
      " |      >>> df.round(1)  # doctest: +SKIP\n",
      " |          dogs  cats\n",
      " |      0   0.2   0.3\n",
      " |      1   0.0   0.7\n",
      " |      2   0.7   0.0\n",
      " |      3   0.2   0.2\n",
      " |      \n",
      " |      With a dict, the number of places for specific columns can be\n",
      " |      specified with the column names as key and the number of decimal\n",
      " |      places as value\n",
      " |      \n",
      " |      >>> df.round({'dogs': 1, 'cats': 0})  # doctest: +SKIP\n",
      " |          dogs  cats\n",
      " |      0   0.2   0.0\n",
      " |      1   0.0   1.0\n",
      " |      2   0.7   0.0\n",
      " |      3   0.2   0.0\n",
      " |      \n",
      " |      Using a Series, the number of places for specific columns can be\n",
      " |      specified with the column names as index and the number of\n",
      " |      decimal places as value\n",
      " |      \n",
      " |      >>> decimals = pd.Series([0, 1], index=['cats', 'dogs'])  # doctest: +SKIP\n",
      " |      >>> df.round(decimals)  # doctest: +SKIP\n",
      " |          dogs  cats\n",
      " |      0   0.2   0.0\n",
      " |      1   0.0   1.0\n",
      " |      2   0.7   0.0\n",
      " |      3   0.2   0.0\n",
      " |  \n",
      " |  rpow(self, other, axis='columns', level=None, fill_value=None)\n",
      " |      Get Exponential power of dataframe and other, element-wise (binary operator `rpow`).\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.rpow.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Equivalent to ``other ** dataframe``, but with support to substitute a fill_value\n",
      " |      for missing data in one of the inputs. With reverse version, `pow`.\n",
      " |      \n",
      " |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `mod`, `pow`) to\n",
      " |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}\n",
      " |          Whether to compare by the index (0 or 'index') or columns\n",
      " |          (1 or 'columns'). For Series input, axis to match Series index on.\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      fill_value : float or None, default None\n",
      " |          Fill existing missing (NaN) values, and any new element needed for\n",
      " |          successful DataFrame alignment, with this value before computation.\n",
      " |          If data in both corresponding DataFrame locations is missing\n",
      " |          the result will be missing.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Result of the arithmetic operation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.add : Add DataFrames.\n",
      " |      DataFrame.sub : Subtract DataFrames.\n",
      " |      DataFrame.mul : Multiply DataFrames.\n",
      " |      DataFrame.div : Divide DataFrames (float division).\n",
      " |      DataFrame.truediv : Divide DataFrames (float division).\n",
      " |      DataFrame.floordiv : Divide DataFrames (integer division).\n",
      " |      DataFrame.mod : Calculate modulo (remainder after division).\n",
      " |      DataFrame.pow : Calculate exponential power.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'angles': [0, 3, 4],  # doctest: +SKIP\n",
      " |      ...                    'degrees': [360, 180, 360]},\n",
      " |      ...                   index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      360\n",
      " |      triangle        3      180\n",
      " |      rectangle       4      360\n",
      " |      \n",
      " |      Add a scalar with operator version which return the same\n",
      " |      results.\n",
      " |      \n",
      " |      >>> df + 1  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      >>> df.add(1)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      Divide by constant with reverse version.\n",
      " |      \n",
      " |      >>> df.div(10)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle        0.0     36.0\n",
      " |      triangle      0.3     18.0\n",
      " |      rectangle     0.4     36.0\n",
      " |      \n",
      " |      >>> df.rdiv(10)  # doctest: +SKIP\n",
      " |                   angles   degrees\n",
      " |      circle          inf  0.027778\n",
      " |      triangle   3.333333  0.055556\n",
      " |      rectangle  2.500000  0.027778\n",
      " |      \n",
      " |      Subtract a list and Series by axis with operator version.\n",
      " |      \n",
      " |      >>> df - [1, 2]  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub([1, 2], axis='columns')  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),  # doctest: +SKIP\n",
      " |      ...        axis='index')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      359\n",
      " |      triangle        2      179\n",
      " |      rectangle       3      359\n",
      " |      \n",
      " |      Multiply a DataFrame of different shape with operator version.\n",
      " |      \n",
      " |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},  # doctest: +SKIP\n",
      " |      ...                      index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> other  # doctest: +SKIP\n",
      " |                 angles\n",
      " |      circle          0\n",
      " |      triangle        3\n",
      " |      rectangle       4\n",
      " |      \n",
      " |      >>> df * other  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      NaN\n",
      " |      triangle        9      NaN\n",
      " |      rectangle      16      NaN\n",
      " |      \n",
      " |      >>> df.mul(other, fill_value=0)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      0.0\n",
      " |      triangle        9      0.0\n",
      " |      rectangle      16      0.0\n",
      " |      \n",
      " |      Divide by a MultiIndex by level.\n",
      " |      \n",
      " |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],  # doctest: +SKIP\n",
      " |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n",
      " |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n",
      " |      ...                                    ['circle', 'triangle', 'rectangle',\n",
      " |      ...                                     'square', 'pentagon', 'hexagon']])\n",
      " |      >>> df_multindex  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle          0      360\n",
      " |        triangle        3      180\n",
      " |        rectangle       4      360\n",
      " |      B square          4      360\n",
      " |        pentagon        5      540\n",
      " |        hexagon         6      720\n",
      " |      \n",
      " |      >>> df.div(df_multindex, level=1, fill_value=0)  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle        NaN      1.0\n",
      " |        triangle      1.0      1.0\n",
      " |        rectangle     1.0      1.0\n",
      " |      B square        0.0      0.0\n",
      " |        pentagon      0.0      0.0\n",
      " |        hexagon       0.0      0.0\n",
      " |  \n",
      " |  rsub(self, other, axis='columns', level=None, fill_value=None)\n",
      " |      Get Subtraction of dataframe and other, element-wise (binary operator `rsub`).\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.rsub.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Equivalent to ``other - dataframe``, but with support to substitute a fill_value\n",
      " |      for missing data in one of the inputs. With reverse version, `sub`.\n",
      " |      \n",
      " |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `mod`, `pow`) to\n",
      " |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}\n",
      " |          Whether to compare by the index (0 or 'index') or columns\n",
      " |          (1 or 'columns'). For Series input, axis to match Series index on.\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      fill_value : float or None, default None\n",
      " |          Fill existing missing (NaN) values, and any new element needed for\n",
      " |          successful DataFrame alignment, with this value before computation.\n",
      " |          If data in both corresponding DataFrame locations is missing\n",
      " |          the result will be missing.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Result of the arithmetic operation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.add : Add DataFrames.\n",
      " |      DataFrame.sub : Subtract DataFrames.\n",
      " |      DataFrame.mul : Multiply DataFrames.\n",
      " |      DataFrame.div : Divide DataFrames (float division).\n",
      " |      DataFrame.truediv : Divide DataFrames (float division).\n",
      " |      DataFrame.floordiv : Divide DataFrames (integer division).\n",
      " |      DataFrame.mod : Calculate modulo (remainder after division).\n",
      " |      DataFrame.pow : Calculate exponential power.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'angles': [0, 3, 4],  # doctest: +SKIP\n",
      " |      ...                    'degrees': [360, 180, 360]},\n",
      " |      ...                   index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      360\n",
      " |      triangle        3      180\n",
      " |      rectangle       4      360\n",
      " |      \n",
      " |      Add a scalar with operator version which return the same\n",
      " |      results.\n",
      " |      \n",
      " |      >>> df + 1  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      >>> df.add(1)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      Divide by constant with reverse version.\n",
      " |      \n",
      " |      >>> df.div(10)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle        0.0     36.0\n",
      " |      triangle      0.3     18.0\n",
      " |      rectangle     0.4     36.0\n",
      " |      \n",
      " |      >>> df.rdiv(10)  # doctest: +SKIP\n",
      " |                   angles   degrees\n",
      " |      circle          inf  0.027778\n",
      " |      triangle   3.333333  0.055556\n",
      " |      rectangle  2.500000  0.027778\n",
      " |      \n",
      " |      Subtract a list and Series by axis with operator version.\n",
      " |      \n",
      " |      >>> df - [1, 2]  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub([1, 2], axis='columns')  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),  # doctest: +SKIP\n",
      " |      ...        axis='index')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      359\n",
      " |      triangle        2      179\n",
      " |      rectangle       3      359\n",
      " |      \n",
      " |      Multiply a DataFrame of different shape with operator version.\n",
      " |      \n",
      " |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},  # doctest: +SKIP\n",
      " |      ...                      index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> other  # doctest: +SKIP\n",
      " |                 angles\n",
      " |      circle          0\n",
      " |      triangle        3\n",
      " |      rectangle       4\n",
      " |      \n",
      " |      >>> df * other  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      NaN\n",
      " |      triangle        9      NaN\n",
      " |      rectangle      16      NaN\n",
      " |      \n",
      " |      >>> df.mul(other, fill_value=0)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      0.0\n",
      " |      triangle        9      0.0\n",
      " |      rectangle      16      0.0\n",
      " |      \n",
      " |      Divide by a MultiIndex by level.\n",
      " |      \n",
      " |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],  # doctest: +SKIP\n",
      " |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n",
      " |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n",
      " |      ...                                    ['circle', 'triangle', 'rectangle',\n",
      " |      ...                                     'square', 'pentagon', 'hexagon']])\n",
      " |      >>> df_multindex  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle          0      360\n",
      " |        triangle        3      180\n",
      " |        rectangle       4      360\n",
      " |      B square          4      360\n",
      " |        pentagon        5      540\n",
      " |        hexagon         6      720\n",
      " |      \n",
      " |      >>> df.div(df_multindex, level=1, fill_value=0)  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle        NaN      1.0\n",
      " |        triangle      1.0      1.0\n",
      " |        rectangle     1.0      1.0\n",
      " |      B square        0.0      0.0\n",
      " |        pentagon      0.0      0.0\n",
      " |        hexagon       0.0      0.0\n",
      " |  \n",
      " |  rtruediv(self, other, axis='columns', level=None, fill_value=None)\n",
      " |      Get Floating division of dataframe and other, element-wise (binary operator `rtruediv`).\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.rtruediv.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Equivalent to ``other / dataframe``, but with support to substitute a fill_value\n",
      " |      for missing data in one of the inputs. With reverse version, `truediv`.\n",
      " |      \n",
      " |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `mod`, `pow`) to\n",
      " |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}\n",
      " |          Whether to compare by the index (0 or 'index') or columns\n",
      " |          (1 or 'columns'). For Series input, axis to match Series index on.\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      fill_value : float or None, default None\n",
      " |          Fill existing missing (NaN) values, and any new element needed for\n",
      " |          successful DataFrame alignment, with this value before computation.\n",
      " |          If data in both corresponding DataFrame locations is missing\n",
      " |          the result will be missing.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Result of the arithmetic operation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.add : Add DataFrames.\n",
      " |      DataFrame.sub : Subtract DataFrames.\n",
      " |      DataFrame.mul : Multiply DataFrames.\n",
      " |      DataFrame.div : Divide DataFrames (float division).\n",
      " |      DataFrame.truediv : Divide DataFrames (float division).\n",
      " |      DataFrame.floordiv : Divide DataFrames (integer division).\n",
      " |      DataFrame.mod : Calculate modulo (remainder after division).\n",
      " |      DataFrame.pow : Calculate exponential power.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'angles': [0, 3, 4],  # doctest: +SKIP\n",
      " |      ...                    'degrees': [360, 180, 360]},\n",
      " |      ...                   index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      360\n",
      " |      triangle        3      180\n",
      " |      rectangle       4      360\n",
      " |      \n",
      " |      Add a scalar with operator version which return the same\n",
      " |      results.\n",
      " |      \n",
      " |      >>> df + 1  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      >>> df.add(1)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      Divide by constant with reverse version.\n",
      " |      \n",
      " |      >>> df.div(10)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle        0.0     36.0\n",
      " |      triangle      0.3     18.0\n",
      " |      rectangle     0.4     36.0\n",
      " |      \n",
      " |      >>> df.rdiv(10)  # doctest: +SKIP\n",
      " |                   angles   degrees\n",
      " |      circle          inf  0.027778\n",
      " |      triangle   3.333333  0.055556\n",
      " |      rectangle  2.500000  0.027778\n",
      " |      \n",
      " |      Subtract a list and Series by axis with operator version.\n",
      " |      \n",
      " |      >>> df - [1, 2]  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub([1, 2], axis='columns')  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),  # doctest: +SKIP\n",
      " |      ...        axis='index')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      359\n",
      " |      triangle        2      179\n",
      " |      rectangle       3      359\n",
      " |      \n",
      " |      Multiply a DataFrame of different shape with operator version.\n",
      " |      \n",
      " |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},  # doctest: +SKIP\n",
      " |      ...                      index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> other  # doctest: +SKIP\n",
      " |                 angles\n",
      " |      circle          0\n",
      " |      triangle        3\n",
      " |      rectangle       4\n",
      " |      \n",
      " |      >>> df * other  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      NaN\n",
      " |      triangle        9      NaN\n",
      " |      rectangle      16      NaN\n",
      " |      \n",
      " |      >>> df.mul(other, fill_value=0)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      0.0\n",
      " |      triangle        9      0.0\n",
      " |      rectangle      16      0.0\n",
      " |      \n",
      " |      Divide by a MultiIndex by level.\n",
      " |      \n",
      " |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],  # doctest: +SKIP\n",
      " |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n",
      " |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n",
      " |      ...                                    ['circle', 'triangle', 'rectangle',\n",
      " |      ...                                     'square', 'pentagon', 'hexagon']])\n",
      " |      >>> df_multindex  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle          0      360\n",
      " |        triangle        3      180\n",
      " |        rectangle       4      360\n",
      " |      B square          4      360\n",
      " |        pentagon        5      540\n",
      " |        hexagon         6      720\n",
      " |      \n",
      " |      >>> df.div(df_multindex, level=1, fill_value=0)  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle        NaN      1.0\n",
      " |        triangle      1.0      1.0\n",
      " |        rectangle     1.0      1.0\n",
      " |      B square        0.0      0.0\n",
      " |        pentagon      0.0      0.0\n",
      " |        hexagon       0.0      0.0\n",
      " |  \n",
      " |  select_dtypes(self, include=None, exclude=None)\n",
      " |      Return a subset of the DataFrame's columns based on the column dtypes.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.select_dtypes.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      include, exclude : scalar or list-like\n",
      " |          A selection of dtypes or strings to be included/excluded. At least\n",
      " |          one of these parameters must be supplied.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          The subset of the frame including the dtypes in ``include`` and\n",
      " |          excluding the dtypes in ``exclude``.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ValueError\n",
      " |          * If both of ``include`` and ``exclude`` are empty\n",
      " |          * If ``include`` and ``exclude`` have overlapping elements\n",
      " |          * If any kind of string dtype is passed in.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.dtypes: Return Series with the data type of each column.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      * To select all *numeric* types, use ``np.number`` or ``'number'``\n",
      " |      * To select strings you must use the ``object`` dtype, but note that\n",
      " |        this will return *all* object dtype columns\n",
      " |      * See the `numpy dtype hierarchy\n",
      " |        <https://numpy.org/doc/stable/reference/arrays.scalars.html>`__\n",
      " |      * To select datetimes, use ``np.datetime64``, ``'datetime'`` or\n",
      " |        ``'datetime64'``\n",
      " |      * To select timedeltas, use ``np.timedelta64``, ``'timedelta'`` or\n",
      " |        ``'timedelta64'``\n",
      " |      * To select Pandas categorical dtypes, use ``'category'``\n",
      " |      * To select Pandas datetimetz dtypes, use ``'datetimetz'`` (new in\n",
      " |        0.20.0) or ``'datetime64[ns, tz]'``\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'a': [1, 2] * 3,  # doctest: +SKIP\n",
      " |      ...                    'b': [True, False] * 3,\n",
      " |      ...                    'c': [1.0, 2.0] * 3})\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |              a      b  c\n",
      " |      0       1   True  1.0\n",
      " |      1       2  False  2.0\n",
      " |      2       1   True  1.0\n",
      " |      3       2  False  2.0\n",
      " |      4       1   True  1.0\n",
      " |      5       2  False  2.0\n",
      " |      \n",
      " |      >>> df.select_dtypes(include='bool')  # doctest: +SKIP\n",
      " |         b\n",
      " |      0  True\n",
      " |      1  False\n",
      " |      2  True\n",
      " |      3  False\n",
      " |      4  True\n",
      " |      5  False\n",
      " |      \n",
      " |      >>> df.select_dtypes(include=['float64'])  # doctest: +SKIP\n",
      " |         c\n",
      " |      0  1.0\n",
      " |      1  2.0\n",
      " |      2  1.0\n",
      " |      3  2.0\n",
      " |      4  1.0\n",
      " |      5  2.0\n",
      " |      \n",
      " |      >>> df.select_dtypes(exclude=['int64'])  # doctest: +SKIP\n",
      " |             b    c\n",
      " |      0   True  1.0\n",
      " |      1  False  2.0\n",
      " |      2   True  1.0\n",
      " |      3  False  2.0\n",
      " |      4   True  1.0\n",
      " |      5  False  2.0\n",
      " |  \n",
      " |  set_index(self, other: 'str | Series', drop: 'bool' = True, sorted: 'bool' = False, npartitions: \"int | Literal['auto'] | None\" = None, divisions: 'Sequence | None' = None, inplace: 'bool' = False, **kwargs)\n",
      " |      Set the DataFrame index (row labels) using an existing column.\n",
      " |      \n",
      " |      This realigns the dataset to be sorted by a new column. This can have a\n",
      " |      significant impact on performance, because joins, groupbys, lookups, etc.\n",
      " |      are all much faster on that column. However, this performance increase\n",
      " |      comes with a cost, sorting a parallel dataset requires expensive shuffles.\n",
      " |      Often we ``set_index`` once directly after data ingest and filtering and\n",
      " |      then perform many cheap computations off of the sorted dataset.\n",
      " |      \n",
      " |      This function operates exactly like ``pandas.set_index`` except with\n",
      " |      different performance costs (dask dataframe ``set_index`` is much more expensive).\n",
      " |      Under normal operation this function does an initial pass over the index column\n",
      " |      to compute approximate quantiles to serve as future divisions. It then passes\n",
      " |      over the data a second time, splitting up each input partition into several\n",
      " |      pieces and sharing those pieces to all of the output partitions now in\n",
      " |      sorted order.\n",
      " |      \n",
      " |      In some cases we can alleviate those costs, for example if your dataset is\n",
      " |      sorted already then we can avoid making many small pieces or if you know\n",
      " |      good values to split the new index column then we can avoid the initial\n",
      " |      pass over the data. For example if your new index is a datetime index and\n",
      " |      your data is already sorted by day then this entire operation can be done\n",
      " |      for free. You can control these options with the following parameters.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other: string or Dask Series\n",
      " |          Column to use as index.\n",
      " |      drop: boolean, default True\n",
      " |          Delete column to be used as the new index.\n",
      " |      sorted: bool, optional\n",
      " |          If the index column is already sorted in increasing order.\n",
      " |          Defaults to False\n",
      " |      npartitions: int, None, or 'auto'\n",
      " |          The ideal number of output partitions. If None, use the same as\n",
      " |          the input. If 'auto' then decide by memory use.\n",
      " |          Only used when ``divisions`` is not given. If ``divisions`` is given,\n",
      " |          the number of output partitions will be ``len(divisions) - 1``.\n",
      " |      divisions: list, optional\n",
      " |          The \"dividing lines\" used to split the new index into partitions.\n",
      " |          For ``divisions=[0, 10, 50, 100]``, there would be three output partitions,\n",
      " |          where the new index contained [0, 10), [10, 50), and [50, 100), respectively.\n",
      " |          See https://docs.dask.org/en/latest/dataframe-design.html#partitions.\n",
      " |          If not given (default), good divisions are calculated by immediately computing\n",
      " |          the data and looking at the distribution of its values. For large datasets,\n",
      " |          this can be expensive.\n",
      " |          Note that if ``sorted=True``, specified divisions are assumed to match\n",
      " |          the existing partitions in the data; if this is untrue you should\n",
      " |          leave divisions empty and call ``repartition`` after ``set_index``.\n",
      " |      inplace: bool, optional\n",
      " |          Modifying the DataFrame in place is not supported by Dask.\n",
      " |          Defaults to False.\n",
      " |      shuffle: string, 'disk' or 'tasks', optional\n",
      " |          Either ``'disk'`` for single-node operation or ``'tasks'`` for\n",
      " |          distributed operation.  Will be inferred by your current scheduler.\n",
      " |      compute: bool, default False\n",
      " |          Whether or not to trigger an immediate computation. Defaults to False.\n",
      " |          Note, that even if you set ``compute=False``, an immediate computation\n",
      " |          will still be triggered if ``divisions`` is ``None``.\n",
      " |      partition_size: int, optional\n",
      " |          Desired size of each partitions in bytes.\n",
      " |          Only used when ``npartitions='auto'``\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import dask\n",
      " |      >>> ddf = dask.datasets.timeseries(start=\"2021-01-01\", end=\"2021-01-07\", freq=\"1H\").reset_index()\n",
      " |      >>> ddf2 = ddf.set_index(\"x\")\n",
      " |      >>> ddf2 = ddf.set_index(ddf.x)\n",
      " |      >>> ddf2 = ddf.set_index(ddf.timestamp, sorted=True)\n",
      " |      \n",
      " |      A common case is when we have a datetime column that we know to be\n",
      " |      sorted and is cleanly divided by day.  We can set this index for free\n",
      " |      by specifying both that the column is pre-sorted and the particular\n",
      " |      divisions along which is is separated\n",
      " |      \n",
      " |      >>> import pandas as pd\n",
      " |      >>> divisions = pd.date_range(start=\"2021-01-01\", end=\"2021-01-07\", freq='1D')\n",
      " |      >>> divisions\n",
      " |      DatetimeIndex(['2021-01-01', '2021-01-02', '2021-01-03', '2021-01-04',\n",
      " |                     '2021-01-05', '2021-01-06', '2021-01-07'],\n",
      " |                    dtype='datetime64[ns]', freq='D')\n",
      " |      \n",
      " |      Note that ``len(divisons)`` is equal to ``npartitions + 1``. This is because ``divisions``\n",
      " |      represents the upper and lower bounds of each partition. The first item is the\n",
      " |      lower bound of the first partition, the second item is the lower bound of the\n",
      " |      second partition and the upper bound of the first partition, and so on.\n",
      " |      The second-to-last item is the lower bound of the last partition, and the last\n",
      " |      (extra) item is the upper bound of the last partition.\n",
      " |      \n",
      " |      >>> ddf2 = ddf.set_index(\"timestamp\", sorted=True, divisions=divisions.tolist())\n",
      " |      \n",
      " |      If you'll be running `set_index` on the same (or similar) datasets repeatedly,\n",
      " |      you could save time by letting Dask calculate good divisions once, then copy-pasting\n",
      " |      them to reuse. This is especially helpful running in a Jupyter notebook:\n",
      " |      \n",
      " |      >>> ddf2 = ddf.set_index(\"name\")  # slow, calculates data distribution\n",
      " |      >>> ddf2.divisions  # doctest: +SKIP\n",
      " |      [\"Alice\", \"Laura\", \"Ursula\", \"Zelda\"]\n",
      " |      >>> # ^ Now copy-paste this and edit the line above to:\n",
      " |      >>> # ddf2 = ddf.set_index(\"name\", divisions=[\"Alice\", \"Laura\", \"Ursula\", \"Zelda\"])\n",
      " |  \n",
      " |  sort_values(self, by: 'str', npartitions: \"int | Literal['auto'] | None\" = None, ascending: 'bool' = True, na_position: \"Literal['first'] | Literal['last']\" = 'last', sort_function: 'Callable[[pd.DataFrame], pd.DataFrame] | None' = None, sort_function_kwargs: 'Mapping[str, Any] | None' = None, **kwargs) -> 'DataFrame'\n",
      " |      Sort the dataset by a single column.\n",
      " |      \n",
      " |      Sorting a parallel dataset requires expensive shuffles and is generally\n",
      " |      not recommended. See ``set_index`` for implementation details.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      by: string\n",
      " |      npartitions: int, None, or 'auto'\n",
      " |          The ideal number of output partitions. If None, use the same as\n",
      " |          the input. If 'auto' then decide by memory use.\n",
      " |      ascending: bool, optional\n",
      " |          Sort ascending vs. descending.\n",
      " |          Defaults to True.\n",
      " |      na_position: {'last', 'first'}, optional\n",
      " |          Puts NaNs at the beginning if 'first', puts NaN at the end if 'last'.\n",
      " |          Defaults to 'last'.\n",
      " |      sort_function: function, optional\n",
      " |          Sorting function to use when sorting underlying partitions.\n",
      " |          If None, defaults to ``M.sort_values`` (the partition library's\n",
      " |          implementation of ``sort_values``).\n",
      " |      sort_function_kwargs: dict, optional\n",
      " |          Additional keyword arguments to pass to the partition sorting function.\n",
      " |          By default, ``by``, ``ascending``, and ``na_position`` are provided.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df2 = df.sort_values('x')  # doctest: +SKIP\n",
      " |  \n",
      " |  squeeze(self, axis=None)\n",
      " |      Squeeze 1 dimensional axis objects into scalars.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.squeeze.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Series or DataFrames with a single element are squeezed to a scalar.\n",
      " |      DataFrames with a single column or a single row are squeezed to a\n",
      " |      Series. Otherwise the object is unchanged.\n",
      " |      \n",
      " |      This method is most useful when you don't know if your\n",
      " |      object is a Series or DataFrame, but you do know it has just a single\n",
      " |      column. In that case you can safely call `squeeze` to ensure you have a\n",
      " |      Series.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {0 or 'index', 1 or 'columns', None}, default None\n",
      " |          A specific axis to squeeze. By default, all length-1 axes are\n",
      " |          squeezed.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame, Series, or scalar\n",
      " |          The projection after squeezing `axis` or all the axes.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.iloc : Integer-location based indexing for selecting scalars.\n",
      " |      DataFrame.iloc : Integer-location based indexing for selecting Series.\n",
      " |      Series.to_frame : Inverse of DataFrame.squeeze for a\n",
      " |          single-column DataFrame.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> primes = pd.Series([2, 3, 5, 7])  # doctest: +SKIP\n",
      " |      \n",
      " |      Slicing might produce a Series with a single value:\n",
      " |      \n",
      " |      >>> even_primes = primes[primes % 2 == 0]  # doctest: +SKIP\n",
      " |      >>> even_primes  # doctest: +SKIP\n",
      " |      0    2\n",
      " |      dtype: int64\n",
      " |      \n",
      " |      >>> even_primes.squeeze()  # doctest: +SKIP\n",
      " |      2\n",
      " |      \n",
      " |      Squeezing objects with more than one value in every axis does nothing:\n",
      " |      \n",
      " |      >>> odd_primes = primes[primes % 2 == 1]  # doctest: +SKIP\n",
      " |      >>> odd_primes  # doctest: +SKIP\n",
      " |      1    3\n",
      " |      2    5\n",
      " |      3    7\n",
      " |      dtype: int64\n",
      " |      \n",
      " |      >>> odd_primes.squeeze()  # doctest: +SKIP\n",
      " |      1    3\n",
      " |      2    5\n",
      " |      3    7\n",
      " |      dtype: int64\n",
      " |      \n",
      " |      Squeezing is even more effective when used with DataFrames.\n",
      " |      \n",
      " |      >>> df = pd.DataFrame([[1, 2], [3, 4]], columns=['a', 'b'])  # doctest: +SKIP\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |         a  b\n",
      " |      0  1  2\n",
      " |      1  3  4\n",
      " |      \n",
      " |      Slicing a single column will produce a DataFrame with the columns\n",
      " |      having only one value:\n",
      " |      \n",
      " |      >>> df_a = df[['a']]  # doctest: +SKIP\n",
      " |      >>> df_a  # doctest: +SKIP\n",
      " |         a\n",
      " |      0  1\n",
      " |      1  3\n",
      " |      \n",
      " |      So the columns can be squeezed down, resulting in a Series:\n",
      " |      \n",
      " |      >>> df_a.squeeze('columns')  # doctest: +SKIP\n",
      " |      0    1\n",
      " |      1    3\n",
      " |      Name: a, dtype: int64\n",
      " |      \n",
      " |      Slicing a single row from a single column will produce a single\n",
      " |      scalar DataFrame:\n",
      " |      \n",
      " |      >>> df_0a = df.loc[df.index < 1, ['a']]  # doctest: +SKIP\n",
      " |      >>> df_0a  # doctest: +SKIP\n",
      " |         a\n",
      " |      0  1\n",
      " |      \n",
      " |      Squeezing the rows produces a single scalar Series:\n",
      " |      \n",
      " |      >>> df_0a.squeeze('rows')  # doctest: +SKIP\n",
      " |      a    1\n",
      " |      Name: 0, dtype: int64\n",
      " |      \n",
      " |      Squeezing all axes will project directly into a scalar:\n",
      " |      \n",
      " |      >>> df_0a.squeeze()  # doctest: +SKIP\n",
      " |      1\n",
      " |  \n",
      " |  sub(self, other, axis='columns', level=None, fill_value=None)\n",
      " |      Get Subtraction of dataframe and other, element-wise (binary operator `sub`).\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.sub.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Equivalent to ``dataframe - other``, but with support to substitute a fill_value\n",
      " |      for missing data in one of the inputs. With reverse version, `rsub`.\n",
      " |      \n",
      " |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `mod`, `pow`) to\n",
      " |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}\n",
      " |          Whether to compare by the index (0 or 'index') or columns\n",
      " |          (1 or 'columns'). For Series input, axis to match Series index on.\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      fill_value : float or None, default None\n",
      " |          Fill existing missing (NaN) values, and any new element needed for\n",
      " |          successful DataFrame alignment, with this value before computation.\n",
      " |          If data in both corresponding DataFrame locations is missing\n",
      " |          the result will be missing.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Result of the arithmetic operation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.add : Add DataFrames.\n",
      " |      DataFrame.sub : Subtract DataFrames.\n",
      " |      DataFrame.mul : Multiply DataFrames.\n",
      " |      DataFrame.div : Divide DataFrames (float division).\n",
      " |      DataFrame.truediv : Divide DataFrames (float division).\n",
      " |      DataFrame.floordiv : Divide DataFrames (integer division).\n",
      " |      DataFrame.mod : Calculate modulo (remainder after division).\n",
      " |      DataFrame.pow : Calculate exponential power.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'angles': [0, 3, 4],  # doctest: +SKIP\n",
      " |      ...                    'degrees': [360, 180, 360]},\n",
      " |      ...                   index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      360\n",
      " |      triangle        3      180\n",
      " |      rectangle       4      360\n",
      " |      \n",
      " |      Add a scalar with operator version which return the same\n",
      " |      results.\n",
      " |      \n",
      " |      >>> df + 1  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      >>> df.add(1)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      Divide by constant with reverse version.\n",
      " |      \n",
      " |      >>> df.div(10)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle        0.0     36.0\n",
      " |      triangle      0.3     18.0\n",
      " |      rectangle     0.4     36.0\n",
      " |      \n",
      " |      >>> df.rdiv(10)  # doctest: +SKIP\n",
      " |                   angles   degrees\n",
      " |      circle          inf  0.027778\n",
      " |      triangle   3.333333  0.055556\n",
      " |      rectangle  2.500000  0.027778\n",
      " |      \n",
      " |      Subtract a list and Series by axis with operator version.\n",
      " |      \n",
      " |      >>> df - [1, 2]  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub([1, 2], axis='columns')  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),  # doctest: +SKIP\n",
      " |      ...        axis='index')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      359\n",
      " |      triangle        2      179\n",
      " |      rectangle       3      359\n",
      " |      \n",
      " |      Multiply a DataFrame of different shape with operator version.\n",
      " |      \n",
      " |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},  # doctest: +SKIP\n",
      " |      ...                      index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> other  # doctest: +SKIP\n",
      " |                 angles\n",
      " |      circle          0\n",
      " |      triangle        3\n",
      " |      rectangle       4\n",
      " |      \n",
      " |      >>> df * other  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      NaN\n",
      " |      triangle        9      NaN\n",
      " |      rectangle      16      NaN\n",
      " |      \n",
      " |      >>> df.mul(other, fill_value=0)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      0.0\n",
      " |      triangle        9      0.0\n",
      " |      rectangle      16      0.0\n",
      " |      \n",
      " |      Divide by a MultiIndex by level.\n",
      " |      \n",
      " |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],  # doctest: +SKIP\n",
      " |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n",
      " |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n",
      " |      ...                                    ['circle', 'triangle', 'rectangle',\n",
      " |      ...                                     'square', 'pentagon', 'hexagon']])\n",
      " |      >>> df_multindex  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle          0      360\n",
      " |        triangle        3      180\n",
      " |        rectangle       4      360\n",
      " |      B square          4      360\n",
      " |        pentagon        5      540\n",
      " |        hexagon         6      720\n",
      " |      \n",
      " |      >>> df.div(df_multindex, level=1, fill_value=0)  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle        NaN      1.0\n",
      " |        triangle      1.0      1.0\n",
      " |        rectangle     1.0      1.0\n",
      " |      B square        0.0      0.0\n",
      " |        pentagon      0.0      0.0\n",
      " |        hexagon       0.0      0.0\n",
      " |  \n",
      " |  to_bag(self, index=False, format='tuple')\n",
      " |      Create Dask Bag from a Dask DataFrame\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      index : bool, optional\n",
      " |          If True, the elements are tuples of ``(index, value)``, otherwise\n",
      " |          they're just the ``value``.  Default is False.\n",
      " |      format : {\"tuple\", \"dict\", \"frame\"}, optional\n",
      " |          Whether to return a bag of tuples, dictionaries, or\n",
      " |          dataframe-like objects. Default is \"tuple\". If \"frame\",\n",
      " |          the original partitions of ``df`` will not be transformed\n",
      " |          in any way.\n",
      " |      \n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> bag = df.to_bag()  # doctest: +SKIP\n",
      " |  \n",
      " |  to_html(self, max_rows=5)\n",
      " |      Render a DataFrame as an HTML table.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      buf : str, Path or StringIO-like, optional, default None  (Not supported in Dask)\n",
      " |          Buffer to write to. If None, the output is returned as a string.\n",
      " |      columns : sequence, optional, default None  (Not supported in Dask)\n",
      " |          The subset of columns to write. Writes all columns by default.\n",
      " |      col_space : str or int, list or dict of int or str, optional  (Not supported in Dask)\n",
      " |          The minimum width of each column in CSS length units.  An int is assumed to be px units.\n",
      " |      \n",
      " |          This docstring was copied from pandas.core.frame.DataFrame.to_html.\n",
      " |      \n",
      " |          Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |          .. versionadded:: 0.25.0\n",
      " |              Ability to use str.\n",
      " |      header : bool, optional  (Not supported in Dask)\n",
      " |          Whether to print column labels, default True.\n",
      " |      index : bool, optional, default True  (Not supported in Dask)\n",
      " |          Whether to print index (row) labels.\n",
      " |      na_rep : str, optional, default 'NaN'  (Not supported in Dask)\n",
      " |          String representation of ``NaN`` to use.\n",
      " |      formatters : list, tuple or dict of one-param. functions, optional  (Not supported in Dask)\n",
      " |          Formatter functions to apply to columns' elements by position or\n",
      " |          name.\n",
      " |          The result of each function must be a unicode string.\n",
      " |          List/tuple must be of length equal to the number of columns.\n",
      " |      float_format : one-parameter function, optional, default None  (Not supported in Dask)\n",
      " |          Formatter function to apply to columns' elements if they are\n",
      " |          floats. This function must return a unicode string and will be\n",
      " |          applied only to the non-``NaN`` elements, with ``NaN`` being\n",
      " |          handled by ``na_rep``.\n",
      " |      \n",
      " |          .. versionchanged:: 1.2.0\n",
      " |      \n",
      " |      sparsify : bool, optional, default True  (Not supported in Dask)\n",
      " |          Set to False for a DataFrame with a hierarchical index to print\n",
      " |          every multiindex key at each row.\n",
      " |      index_names : bool, optional, default True  (Not supported in Dask)\n",
      " |          Prints the names of the indexes.\n",
      " |      justify : str, default None  (Not supported in Dask)\n",
      " |          How to justify the column labels. If None uses the option from\n",
      " |          the print configuration (controlled by set_option), 'right' out\n",
      " |          of the box. Valid values are\n",
      " |      \n",
      " |          * left\n",
      " |          * right\n",
      " |          * center\n",
      " |          * justify\n",
      " |          * justify-all\n",
      " |          * start\n",
      " |          * end\n",
      " |          * inherit\n",
      " |          * match-parent\n",
      " |          * initial\n",
      " |          * unset.\n",
      " |      max_rows : int, optional\n",
      " |          Maximum number of rows to display in the console.\n",
      " |      max_cols : int, optional  (Not supported in Dask)\n",
      " |          Maximum number of columns to display in the console.\n",
      " |      show_dimensions : bool, default False  (Not supported in Dask)\n",
      " |          Display DataFrame dimensions (number of rows by number of columns).\n",
      " |      decimal : str, default '.'  (Not supported in Dask)\n",
      " |          Character recognized as decimal separator, e.g. ',' in Europe.\n",
      " |      \n",
      " |      bold_rows : bool, default True  (Not supported in Dask)\n",
      " |          Make the row labels bold in the output.\n",
      " |      classes : str or list or tuple, default None  (Not supported in Dask)\n",
      " |          CSS class(es) to apply to the resulting html table.\n",
      " |      escape : bool, default True  (Not supported in Dask)\n",
      " |          Convert the characters <, >, and & to HTML-safe sequences.\n",
      " |      notebook : {True, False}, default False  (Not supported in Dask)\n",
      " |          Whether the generated HTML is for IPython Notebook.\n",
      " |      border : int  (Not supported in Dask)\n",
      " |          A ``border=border`` attribute is included in the opening\n",
      " |          `<table>` tag. Default ``pd.options.display.html.border``.\n",
      " |      table_id : str, optional  (Not supported in Dask)\n",
      " |          A css id is included in the opening `<table>` tag if specified.\n",
      " |      render_links : bool, default False  (Not supported in Dask)\n",
      " |          Convert URLs to HTML links.\n",
      " |      encoding : str, default \"utf-8\"  (Not supported in Dask)\n",
      " |          Set character encoding.\n",
      " |      \n",
      " |          .. versionadded:: 1.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str or None\n",
      " |          If buf is None, returns the result as a string. Otherwise returns\n",
      " |          None.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      to_string : Convert DataFrame to a string.\n",
      " |  \n",
      " |  to_orc(self, path, *args, **kwargs)\n",
      " |      See dd.to_orc docstring for more information\n",
      " |  \n",
      " |  to_parquet(self, path, *args, **kwargs)\n",
      " |      Store Dask.dataframe to Parquet files\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Each partition will be written to a separate file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      df : dask.dataframe.DataFrame\n",
      " |      path : string or pathlib.Path\n",
      " |          Destination directory for data.  Prepend with protocol like ``s3://``\n",
      " |          or ``hdfs://`` for remote data.\n",
      " |      engine : {'auto', 'pyarrow', 'fastparquet'}, default 'auto'\n",
      " |          Parquet library to use. Defaults to 'auto', which uses ``pyarrow`` if\n",
      " |          it is installed, and falls back to ``fastparquet`` otherwise.\n",
      " |      compression : string or dict, default 'snappy'\n",
      " |          Either a string like ``\"snappy\"`` or a dictionary mapping column names\n",
      " |          to compressors like ``{\"name\": \"gzip\", \"values\": \"snappy\"}``. Defaults\n",
      " |          to ``\"snappy\"``.\n",
      " |      write_index : boolean, default True\n",
      " |          Whether or not to write the index. Defaults to True.\n",
      " |      append : bool, default False\n",
      " |          If False (default), construct data-set from scratch. If True, add new\n",
      " |          row-group(s) to an existing data-set. In the latter case, the data-set\n",
      " |          must exist, and the schema must match the input data.\n",
      " |      overwrite : bool, default False\n",
      " |          Whether or not to remove the contents of `path` before writing the dataset.\n",
      " |          The default is False.  If True, the specified path must correspond to\n",
      " |          a directory (but not the current working directory).  This option cannot\n",
      " |          be set to True if `append=True`.\n",
      " |          NOTE: `overwrite=True` will remove the original data even if the current\n",
      " |          write operation fails.  Use at your own risk.\n",
      " |      ignore_divisions : bool, default False\n",
      " |          If False (default) raises error when previous divisions overlap with\n",
      " |          the new appended divisions. Ignored if append=False.\n",
      " |      partition_on : list, default None\n",
      " |          Construct directory-based partitioning by splitting on these fields'\n",
      " |          values. Each dask partition will result in one or more datafiles,\n",
      " |          there will be no global groupby.\n",
      " |      storage_options : dict, default None\n",
      " |          Key/value pairs to be passed on to the file-system backend, if any.\n",
      " |      custom_metadata : dict, default None\n",
      " |          Custom key/value metadata to include in all footer metadata (and\n",
      " |          in the global \"_metadata\" file, if applicable).  Note that the custom\n",
      " |          metadata may not contain the reserved b\"pandas\" key.\n",
      " |      write_metadata_file : bool or None, default None\n",
      " |          Whether to write the special ``_metadata`` file. If ``None`` (the\n",
      " |          default), a ``_metadata`` file will only be written if ``append=True``\n",
      " |          and the dataset already has a ``_metadata`` file.\n",
      " |      compute : bool, default True\n",
      " |          If ``True`` (default) then the result is computed immediately. If\n",
      " |          ``False`` then a ``dask.dataframe.Scalar`` object is returned for\n",
      " |          future computation.\n",
      " |      compute_kwargs : dict, default True\n",
      " |          Options to be passed in to the compute method\n",
      " |      schema : pyarrow.Schema, dict, \"infer\", or None, default \"infer\"\n",
      " |          Global schema to use for the output dataset. Defaults to \"infer\", which\n",
      " |          will infer the schema from the dask dataframe metadata. This is usually\n",
      " |          sufficient for common schemas, but notably will fail for ``object``\n",
      " |          dtype columns that contain things other than strings. These columns\n",
      " |          will require an explicit schema be specified. The schema for a subset\n",
      " |          of columns can be overridden by passing in a dict of column names to\n",
      " |          pyarrow types (for example ``schema={\"field\": pa.string()}``); columns\n",
      " |          not present in this dict will still be automatically inferred.\n",
      " |          Alternatively, a full ``pyarrow.Schema`` may be passed, in which case\n",
      " |          no schema inference will be done. Passing in ``schema=None`` will\n",
      " |          disable the use of a global file schema - each written file may use a\n",
      " |          different schema dependent on the dtypes of the corresponding\n",
      " |          partition. Note that this argument is ignored by the \"fastparquet\"\n",
      " |          engine.\n",
      " |      name_function : callable, default None\n",
      " |          Function to generate the filename for each output partition.\n",
      " |          The function should accept an integer (partition index) as input and\n",
      " |          return a string which will be used as the filename for the corresponding\n",
      " |          partition. Should preserve the lexicographic order of partitions.\n",
      " |          If not specified, files will created using the convention\n",
      " |          ``part.0.parquet``, ``part.1.parquet``, ``part.2.parquet``, ...\n",
      " |          and so on for each partition in the DataFrame.\n",
      " |      **kwargs :\n",
      " |          Extra options to be passed on to the specific backend.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = dd.read_csv(...)  # doctest: +SKIP\n",
      " |      >>> df.to_parquet('/path/to/output/', ...)  # doctest: +SKIP\n",
      " |      \n",
      " |      By default, files will be created in the specified output directory using the\n",
      " |      convention ``part.0.parquet``, ``part.1.parquet``, ``part.2.parquet``, ... and so on for\n",
      " |      each partition in the DataFrame. To customize the names of each file, you can use the\n",
      " |      ``name_function=`` keyword argument. The function passed to ``name_function`` will be\n",
      " |      used to generate the filename for each partition and should expect a partition's index\n",
      " |      integer as input and return a string which will be used as the filename for the corresponding\n",
      " |      partition. Strings produced by ``name_function`` must preserve the order of their respective\n",
      " |      partition indices.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      >>> name_function = lambda x: f\"data-{x}.parquet\"\n",
      " |      >>> df.to_parquet('/path/to/output/', name_function=name_function)  # doctest: +SKIP\n",
      " |      \n",
      " |      will result in the following files being created::\n",
      " |      \n",
      " |          /path/to/output/\n",
      " |              ├── data-0.parquet\n",
      " |              ├── data-1.parquet\n",
      " |              ├── data-2.parquet\n",
      " |              └── ...\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      read_parquet: Read parquet data to dask.dataframe\n",
      " |  \n",
      " |  to_records(self, index=False, lengths=None)\n",
      " |      Create Dask Array from a Dask Dataframe\n",
      " |      \n",
      " |      Warning: This creates a dask.array without precise shape information.\n",
      " |      Operations that depend on shape information, like slicing or reshaping,\n",
      " |      will not work.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.to_records()  # doctest: +SKIP\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      dask.dataframe._Frame.values\n",
      " |      dask.dataframe.from_dask_array\n",
      " |  \n",
      " |  to_string(self, max_rows=5)\n",
      " |      Render a DataFrame to a console-friendly tabular output.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      buf : str, Path or StringIO-like, optional, default None  (Not supported in Dask)\n",
      " |          Buffer to write to. If None, the output is returned as a string.\n",
      " |      columns : sequence, optional, default None  (Not supported in Dask)\n",
      " |          The subset of columns to write. Writes all columns by default.\n",
      " |      col_space : int, list or dict of int, optional  (Not supported in Dask)\n",
      " |          The minimum width of each column. If a list of ints is given every integers corresponds with one column. If a dict is given, the key references the column, while the value defines the space to use..\n",
      " |      header : bool or sequence of str, optional  (Not supported in Dask)\n",
      " |          Write out the column names. If a list of strings is given, it is assumed to be aliases for the column names.\n",
      " |      index : bool, optional, default True  (Not supported in Dask)\n",
      " |          Whether to print index (row) labels.\n",
      " |      na_rep : str, optional, default 'NaN'  (Not supported in Dask)\n",
      " |          String representation of ``NaN`` to use.\n",
      " |      formatters : list, tuple or dict of one-param. functions, optional  (Not supported in Dask)\n",
      " |          Formatter functions to apply to columns' elements by position or\n",
      " |          name.\n",
      " |          The result of each function must be a unicode string.\n",
      " |          List/tuple must be of length equal to the number of columns.\n",
      " |      float_format : one-parameter function, optional, default None  (Not supported in Dask)\n",
      " |          Formatter function to apply to columns' elements if they are\n",
      " |          floats. This function must return a unicode string and will be\n",
      " |          applied only to the non-``NaN`` elements, with ``NaN`` being\n",
      " |          handled by ``na_rep``.\n",
      " |      \n",
      " |          This docstring was copied from pandas.core.frame.DataFrame.to_string.\n",
      " |      \n",
      " |          Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |          .. versionchanged:: 1.2.0\n",
      " |      \n",
      " |      sparsify : bool, optional, default True  (Not supported in Dask)\n",
      " |          Set to False for a DataFrame with a hierarchical index to print\n",
      " |          every multiindex key at each row.\n",
      " |      index_names : bool, optional, default True  (Not supported in Dask)\n",
      " |          Prints the names of the indexes.\n",
      " |      justify : str, default None  (Not supported in Dask)\n",
      " |          How to justify the column labels. If None uses the option from\n",
      " |          the print configuration (controlled by set_option), 'right' out\n",
      " |          of the box. Valid values are\n",
      " |      \n",
      " |          * left\n",
      " |          * right\n",
      " |          * center\n",
      " |          * justify\n",
      " |          * justify-all\n",
      " |          * start\n",
      " |          * end\n",
      " |          * inherit\n",
      " |          * match-parent\n",
      " |          * initial\n",
      " |          * unset.\n",
      " |      max_rows : int, optional\n",
      " |          Maximum number of rows to display in the console.\n",
      " |      max_cols : int, optional  (Not supported in Dask)\n",
      " |          Maximum number of columns to display in the console.\n",
      " |      show_dimensions : bool, default False  (Not supported in Dask)\n",
      " |          Display DataFrame dimensions (number of rows by number of columns).\n",
      " |      decimal : str, default '.'  (Not supported in Dask)\n",
      " |          Character recognized as decimal separator, e.g. ',' in Europe.\n",
      " |      \n",
      " |      line_width : int, optional  (Not supported in Dask)\n",
      " |          Width to wrap a line in characters.\n",
      " |      min_rows : int, optional  (Not supported in Dask)\n",
      " |          The number of rows to display in the console in a truncated repr\n",
      " |          (when number of rows is above `max_rows`).\n",
      " |      max_colwidth : int, optional  (Not supported in Dask)\n",
      " |          Max width to truncate each column in characters. By default, no limit.\n",
      " |      \n",
      " |          .. versionadded:: 1.0.0\n",
      " |      encoding : str, default \"utf-8\"  (Not supported in Dask)\n",
      " |          Set character encoding.\n",
      " |      \n",
      " |          .. versionadded:: 1.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str or None\n",
      " |          If buf is None, returns the result as a string. Otherwise returns\n",
      " |          None.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      to_html : Convert DataFrame to HTML.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> d = {'col1': [1, 2, 3], 'col2': [4, 5, 6]}  # doctest: +SKIP\n",
      " |      >>> df = pd.DataFrame(d)  # doctest: +SKIP\n",
      " |      >>> print(df.to_string())  # doctest: +SKIP\n",
      " |         col1  col2\n",
      " |      0     1     4\n",
      " |      1     2     5\n",
      " |      2     3     6\n",
      " |  \n",
      " |  to_timestamp(self, freq=None, how='start', axis=0)\n",
      " |      Cast to DatetimeIndex of timestamps, at *beginning* of period.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.to_timestamp.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      freq : str, default frequency of PeriodIndex\n",
      " |          Desired frequency.\n",
      " |      how : {'s', 'e', 'start', 'end'}\n",
      " |          Convention for converting period to timestamp; start of period\n",
      " |          vs. end.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          The axis to convert (the index by default).\n",
      " |      copy : bool, default True  (Not supported in Dask)\n",
      " |          If False then underlying input data is not copied.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame with DatetimeIndex\n",
      " |  \n",
      " |  truediv(self, other, axis='columns', level=None, fill_value=None)\n",
      " |      Get Floating division of dataframe and other, element-wise (binary operator `truediv`).\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.truediv.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Equivalent to ``dataframe / other``, but with support to substitute a fill_value\n",
      " |      for missing data in one of the inputs. With reverse version, `rtruediv`.\n",
      " |      \n",
      " |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `mod`, `pow`) to\n",
      " |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : scalar, sequence, Series, or DataFrame\n",
      " |          Any single or multiple element data structure, or list-like object.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}\n",
      " |          Whether to compare by the index (0 or 'index') or columns\n",
      " |          (1 or 'columns'). For Series input, axis to match Series index on.\n",
      " |      level : int or label\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      fill_value : float or None, default None\n",
      " |          Fill existing missing (NaN) values, and any new element needed for\n",
      " |          successful DataFrame alignment, with this value before computation.\n",
      " |          If data in both corresponding DataFrame locations is missing\n",
      " |          the result will be missing.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Result of the arithmetic operation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.add : Add DataFrames.\n",
      " |      DataFrame.sub : Subtract DataFrames.\n",
      " |      DataFrame.mul : Multiply DataFrames.\n",
      " |      DataFrame.div : Divide DataFrames (float division).\n",
      " |      DataFrame.truediv : Divide DataFrames (float division).\n",
      " |      DataFrame.floordiv : Divide DataFrames (integer division).\n",
      " |      DataFrame.mod : Calculate modulo (remainder after division).\n",
      " |      DataFrame.pow : Calculate exponential power.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Mismatched indices will be unioned together.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'angles': [0, 3, 4],  # doctest: +SKIP\n",
      " |      ...                    'degrees': [360, 180, 360]},\n",
      " |      ...                   index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      360\n",
      " |      triangle        3      180\n",
      " |      rectangle       4      360\n",
      " |      \n",
      " |      Add a scalar with operator version which return the same\n",
      " |      results.\n",
      " |      \n",
      " |      >>> df + 1  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      >>> df.add(1)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          1      361\n",
      " |      triangle        4      181\n",
      " |      rectangle       5      361\n",
      " |      \n",
      " |      Divide by constant with reverse version.\n",
      " |      \n",
      " |      >>> df.div(10)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle        0.0     36.0\n",
      " |      triangle      0.3     18.0\n",
      " |      rectangle     0.4     36.0\n",
      " |      \n",
      " |      >>> df.rdiv(10)  # doctest: +SKIP\n",
      " |                   angles   degrees\n",
      " |      circle          inf  0.027778\n",
      " |      triangle   3.333333  0.055556\n",
      " |      rectangle  2.500000  0.027778\n",
      " |      \n",
      " |      Subtract a list and Series by axis with operator version.\n",
      " |      \n",
      " |      >>> df - [1, 2]  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub([1, 2], axis='columns')  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle         -1      358\n",
      " |      triangle        2      178\n",
      " |      rectangle       3      358\n",
      " |      \n",
      " |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),  # doctest: +SKIP\n",
      " |      ...        axis='index')\n",
      " |                 angles  degrees\n",
      " |      circle         -1      359\n",
      " |      triangle        2      179\n",
      " |      rectangle       3      359\n",
      " |      \n",
      " |      Multiply a DataFrame of different shape with operator version.\n",
      " |      \n",
      " |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},  # doctest: +SKIP\n",
      " |      ...                      index=['circle', 'triangle', 'rectangle'])\n",
      " |      >>> other  # doctest: +SKIP\n",
      " |                 angles\n",
      " |      circle          0\n",
      " |      triangle        3\n",
      " |      rectangle       4\n",
      " |      \n",
      " |      >>> df * other  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      NaN\n",
      " |      triangle        9      NaN\n",
      " |      rectangle      16      NaN\n",
      " |      \n",
      " |      >>> df.mul(other, fill_value=0)  # doctest: +SKIP\n",
      " |                 angles  degrees\n",
      " |      circle          0      0.0\n",
      " |      triangle        9      0.0\n",
      " |      rectangle      16      0.0\n",
      " |      \n",
      " |      Divide by a MultiIndex by level.\n",
      " |      \n",
      " |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],  # doctest: +SKIP\n",
      " |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n",
      " |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n",
      " |      ...                                    ['circle', 'triangle', 'rectangle',\n",
      " |      ...                                     'square', 'pentagon', 'hexagon']])\n",
      " |      >>> df_multindex  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle          0      360\n",
      " |        triangle        3      180\n",
      " |        rectangle       4      360\n",
      " |      B square          4      360\n",
      " |        pentagon        5      540\n",
      " |        hexagon         6      720\n",
      " |      \n",
      " |      >>> df.div(df_multindex, level=1, fill_value=0)  # doctest: +SKIP\n",
      " |                   angles  degrees\n",
      " |      A circle        NaN      1.0\n",
      " |        triangle      1.0      1.0\n",
      " |        rectangle     1.0      1.0\n",
      " |      B square        0.0      0.0\n",
      " |        pentagon      0.0      0.0\n",
      " |        hexagon       0.0      0.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_dict(data, *, npartitions, orient='columns', dtype=None, columns=None) from builtins.type\n",
      " |      Construct a Dask DataFrame from a Python Dictionary\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : dict\n",
      " |          Of the form {field : array-like} or {field : dict}.\n",
      " |      npartitions : int\n",
      " |          The number of partitions of the index to create. Note that depending on\n",
      " |          the size and index of the dataframe, the output may have fewer\n",
      " |          partitions than requested.\n",
      " |      orient : {'columns', 'index', 'tight'}, default 'columns'\n",
      " |          The \"orientation\" of the data. If the keys of the passed dict\n",
      " |          should be the columns of the resulting DataFrame, pass 'columns'\n",
      " |          (default). Otherwise if the keys should be rows, pass 'index'.\n",
      " |          If 'tight', assume a dict with keys\n",
      " |          ['index', 'columns', 'data', 'index_names', 'column_names'].\n",
      " |      dtype: bool\n",
      " |          Data type to force, otherwise infer.\n",
      " |      columns: string, optional\n",
      " |          Column labels to use when ``orient='index'``. Raises a ValueError\n",
      " |          if used with ``orient='columns'`` or ``orient='tight'``.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import dask.dataframe as dd\n",
      " |      >>> ddf = dd.DataFrame.from_dict({\"num1\": [1, 2, 3, 4], \"num2\": [7, 8, 9, 10]}, npartitions=2)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  axes\n",
      " |  \n",
      " |  dtypes\n",
      " |      Return data types\n",
      " |  \n",
      " |  empty\n",
      " |  \n",
      " |  iloc\n",
      " |      Purely integer-location based indexing for selection by position.\n",
      " |      \n",
      " |      Only indexing the column positions is supported. Trying to select\n",
      " |      row positions will raise a ValueError.\n",
      " |      \n",
      " |      See :ref:`dataframe.indexing` for more.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.iloc[:, [2, 0, 1]]  # doctest: +SKIP\n",
      " |  \n",
      " |  ndim\n",
      " |      Return dimensionality\n",
      " |  \n",
      " |  shape\n",
      " |      Return a tuple representing the dimensionality of the DataFrame.\n",
      " |      \n",
      " |      The number of rows is a Delayed result. The number of columns\n",
      " |      is a concrete integer.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.size  # doctest: +SKIP\n",
      " |      (Delayed('int-07f06075-5ecc-4d77-817e-63c69a9188a8'), 2)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  columns\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'_accessors': 'ClassVar[set[str]]'}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _Frame:\n",
      " |  \n",
      " |  __abs__ lambda self\n",
      " |  \n",
      " |  __add__ lambda self, other\n",
      " |  \n",
      " |  __and__ lambda self, other\n",
      " |  \n",
      " |  __array__(self, dtype=None, **kwargs)\n",
      " |  \n",
      " |  __array_ufunc__(self, numpy_ufunc, method, *inputs, **kwargs)\n",
      " |  \n",
      " |  __bool__(self)\n",
      " |  \n",
      " |  __complex__(self)\n",
      " |  \n",
      " |  __dask_graph__(self)\n",
      " |  \n",
      " |  __dask_keys__(self) -> 'list[Hashable]'\n",
      " |  \n",
      " |  __dask_layers__(self)\n",
      " |  \n",
      " |  __dask_optimize__ = optimize(dsk, keys, **kwargs)\n",
      " |  \n",
      " |  __dask_postcompute__(self)\n",
      " |  \n",
      " |  __dask_postpersist__(self)\n",
      " |  \n",
      " |  __dask_tokenize__(self)\n",
      " |  \n",
      " |  __eq__ lambda self, other\n",
      " |  \n",
      " |  __float__(self)\n",
      " |  \n",
      " |  __floordiv__ lambda self, other\n",
      " |  \n",
      " |  __ge__ lambda self, other\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __gt__ lambda self, other\n",
      " |  \n",
      " |  __int__(self)\n",
      " |  \n",
      " |  __invert__ lambda self\n",
      " |  \n",
      " |  __le__ lambda self, other\n",
      " |  \n",
      " |  __long__ = __int__(self)\n",
      " |  \n",
      " |  __lt__ lambda self, other\n",
      " |  \n",
      " |  __mod__ lambda self, other\n",
      " |  \n",
      " |  __mul__ lambda self, other\n",
      " |  \n",
      " |  __ne__ lambda self, other\n",
      " |  \n",
      " |  __neg__ lambda self\n",
      " |  \n",
      " |  __nonzero__ = __bool__(self)\n",
      " |  \n",
      " |  __or__ lambda self, other\n",
      " |  \n",
      " |  __pow__ lambda self, other\n",
      " |  \n",
      " |  __radd__ lambda self, other\n",
      " |  \n",
      " |  __rand__ lambda self, other\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rfloordiv__ lambda self, other\n",
      " |  \n",
      " |  __rmod__ lambda self, other\n",
      " |  \n",
      " |  __rmul__ lambda self, other\n",
      " |  \n",
      " |  __ror__ lambda self, other\n",
      " |  \n",
      " |  __rpow__ lambda self, other\n",
      " |  \n",
      " |  __rsub__ lambda self, other\n",
      " |  \n",
      " |  __rtruediv__ lambda self, other\n",
      " |  \n",
      " |  __rxor__ lambda self, other\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  __sub__ lambda self, other\n",
      " |  \n",
      " |  __truediv__ lambda self, other\n",
      " |  \n",
      " |  __xor__ lambda self, other\n",
      " |  \n",
      " |  abs(self)\n",
      " |      Return a Series/DataFrame with absolute numeric value of each element.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.abs.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      This function only applies to elements that are all numeric.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      abs\n",
      " |          Series/DataFrame containing the absolute value of each element.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.absolute : Calculate the absolute value element-wise.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For ``complex`` inputs, ``1.2 + 1j``, the absolute value is\n",
      " |      :math:`\\sqrt{ a^2 + b^2 }`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Absolute numeric values in a Series.\n",
      " |      \n",
      " |      >>> s = pd.Series([-1.10, 2, -3.33, 4])  # doctest: +SKIP\n",
      " |      >>> s.abs()  # doctest: +SKIP\n",
      " |      0    1.10\n",
      " |      1    2.00\n",
      " |      2    3.33\n",
      " |      3    4.00\n",
      " |      dtype: float64\n",
      " |      \n",
      " |      Absolute numeric values in a Series with complex numbers.\n",
      " |      \n",
      " |      >>> s = pd.Series([1.2 + 1j])  # doctest: +SKIP\n",
      " |      >>> s.abs()  # doctest: +SKIP\n",
      " |      0    1.56205\n",
      " |      dtype: float64\n",
      " |      \n",
      " |      Absolute numeric values in a Series with a Timedelta element.\n",
      " |      \n",
      " |      >>> s = pd.Series([pd.Timedelta('1 days')])  # doctest: +SKIP\n",
      " |      >>> s.abs()  # doctest: +SKIP\n",
      " |      0   1 days\n",
      " |      dtype: timedelta64[ns]\n",
      " |      \n",
      " |      Select rows with data closest to certain value using argsort (from\n",
      " |      `StackOverflow <https://stackoverflow.com/a/17758115>`__).\n",
      " |      \n",
      " |      >>> df = pd.DataFrame({  # doctest: +SKIP\n",
      " |      ...     'a': [4, 5, 6, 7],\n",
      " |      ...     'b': [10, 20, 30, 40],\n",
      " |      ...     'c': [100, 50, -30, -50]\n",
      " |      ... })\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |           a    b    c\n",
      " |      0    4   10  100\n",
      " |      1    5   20   50\n",
      " |      2    6   30  -30\n",
      " |      3    7   40  -50\n",
      " |      >>> df.loc[(df.c - 43).abs().argsort()]  # doctest: +SKIP\n",
      " |           a    b    c\n",
      " |      1    5   20   50\n",
      " |      0    4   10  100\n",
      " |      2    6   30  -30\n",
      " |      3    7   40  -50\n",
      " |  \n",
      " |  add_prefix(self, prefix)\n",
      " |      Prefix labels with string `prefix`.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.add_prefix.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      For Series, the row labels are prefixed.\n",
      " |      For DataFrame, the column labels are prefixed.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      prefix : str\n",
      " |          The string to add before each label.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame\n",
      " |          New Series or DataFrame with updated labels.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.add_suffix: Suffix row labels with string `suffix`.\n",
      " |      DataFrame.add_suffix: Suffix column labels with string `suffix`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> s = pd.Series([1, 2, 3, 4])  # doctest: +SKIP\n",
      " |      >>> s  # doctest: +SKIP\n",
      " |      0    1\n",
      " |      1    2\n",
      " |      2    3\n",
      " |      3    4\n",
      " |      dtype: int64\n",
      " |      \n",
      " |      >>> s.add_prefix('item_')  # doctest: +SKIP\n",
      " |      item_0    1\n",
      " |      item_1    2\n",
      " |      item_2    3\n",
      " |      item_3    4\n",
      " |      dtype: int64\n",
      " |      \n",
      " |      >>> df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [3, 4, 5, 6]})  # doctest: +SKIP\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |         A  B\n",
      " |      0  1  3\n",
      " |      1  2  4\n",
      " |      2  3  5\n",
      " |      3  4  6\n",
      " |      \n",
      " |      >>> df.add_prefix('col_')  # doctest: +SKIP\n",
      " |           col_A  col_B\n",
      " |      0       1       3\n",
      " |      1       2       4\n",
      " |      2       3       5\n",
      " |      3       4       6\n",
      " |  \n",
      " |  add_suffix(self, suffix)\n",
      " |      Suffix labels with string `suffix`.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.add_suffix.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      For Series, the row labels are suffixed.\n",
      " |      For DataFrame, the column labels are suffixed.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      suffix : str\n",
      " |          The string to add after each label.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame\n",
      " |          New Series or DataFrame with updated labels.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.add_prefix: Prefix row labels with string `prefix`.\n",
      " |      DataFrame.add_prefix: Prefix column labels with string `prefix`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> s = pd.Series([1, 2, 3, 4])  # doctest: +SKIP\n",
      " |      >>> s  # doctest: +SKIP\n",
      " |      0    1\n",
      " |      1    2\n",
      " |      2    3\n",
      " |      3    4\n",
      " |      dtype: int64\n",
      " |      \n",
      " |      >>> s.add_suffix('_item')  # doctest: +SKIP\n",
      " |      0_item    1\n",
      " |      1_item    2\n",
      " |      2_item    3\n",
      " |      3_item    4\n",
      " |      dtype: int64\n",
      " |      \n",
      " |      >>> df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [3, 4, 5, 6]})  # doctest: +SKIP\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |         A  B\n",
      " |      0  1  3\n",
      " |      1  2  4\n",
      " |      2  3  5\n",
      " |      3  4  6\n",
      " |      \n",
      " |      >>> df.add_suffix('_col')  # doctest: +SKIP\n",
      " |           A_col  B_col\n",
      " |      0       1       3\n",
      " |      1       2       4\n",
      " |      2       3       5\n",
      " |      3       4       6\n",
      " |  \n",
      " |  align(self, other, join='outer', axis=None, fill_value=None)\n",
      " |      Align two objects on their axes with the specified join method.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.align.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Join method is specified for each axis Index.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : DataFrame or Series\n",
      " |      join : {'outer', 'inner', 'left', 'right'}, default 'outer'\n",
      " |      axis : allowed axis of the other object, default None\n",
      " |          Align on index (0), columns (1), or both (None).\n",
      " |      level : int or level name, default None  (Not supported in Dask)\n",
      " |          Broadcast across a level, matching Index values on the\n",
      " |          passed MultiIndex level.\n",
      " |      copy : bool, default True  (Not supported in Dask)\n",
      " |          Always returns new objects. If copy=False and no reindexing is\n",
      " |          required then original objects are returned.\n",
      " |      fill_value : scalar, default np.NaN\n",
      " |          Value to use for missing values. Defaults to NaN, but can be any\n",
      " |          \"compatible\" value.\n",
      " |      method : {'backfill', 'bfill', 'pad', 'ffill', None}, default None  (Not supported in Dask)\n",
      " |          Method to use for filling holes in reindexed Series:\n",
      " |      \n",
      " |          - pad / ffill: propagate last valid observation forward to next valid.\n",
      " |          - backfill / bfill: use NEXT valid observation to fill gap.\n",
      " |      \n",
      " |      limit : int, default None  (Not supported in Dask)\n",
      " |          If method is specified, this is the maximum number of consecutive\n",
      " |          NaN values to forward/backward fill. In other words, if there is\n",
      " |          a gap with more than this number of consecutive NaNs, it will only\n",
      " |          be partially filled. If method is not specified, this is the\n",
      " |          maximum number of entries along the entire axis where NaNs will be\n",
      " |          filled. Must be greater than 0 if not None.\n",
      " |      fill_axis : {0 or 'index', 1 or 'columns'}, default 0  (Not supported in Dask)\n",
      " |          Filling axis, method and limit.\n",
      " |      broadcast_axis : {0 or 'index', 1 or 'columns'}, default None  (Not supported in Dask)\n",
      " |          Broadcast values along this axis, if aligning two objects of\n",
      " |          different dimensions.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      (left, right) : (DataFrame, type of other)\n",
      " |          Aligned objects.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame(  # doctest: +SKIP\n",
      " |      ...     [[1, 2, 3, 4], [6, 7, 8, 9]], columns=[\"D\", \"B\", \"E\", \"A\"], index=[1, 2]\n",
      " |      ... )\n",
      " |      >>> other = pd.DataFrame(  # doctest: +SKIP\n",
      " |      ...     [[10, 20, 30, 40], [60, 70, 80, 90], [600, 700, 800, 900]],\n",
      " |      ...     columns=[\"A\", \"B\", \"C\", \"D\"],\n",
      " |      ...     index=[2, 3, 4],\n",
      " |      ... )\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |         D  B  E  A\n",
      " |      1  1  2  3  4\n",
      " |      2  6  7  8  9\n",
      " |      >>> other  # doctest: +SKIP\n",
      " |          A    B    C    D\n",
      " |      2   10   20   30   40\n",
      " |      3   60   70   80   90\n",
      " |      4  600  700  800  900\n",
      " |      \n",
      " |      Align on columns:\n",
      " |      \n",
      " |      >>> left, right = df.align(other, join=\"outer\", axis=1)  # doctest: +SKIP\n",
      " |      >>> left  # doctest: +SKIP\n",
      " |         A  B   C  D  E\n",
      " |      1  4  2 NaN  1  3\n",
      " |      2  9  7 NaN  6  8\n",
      " |      >>> right  # doctest: +SKIP\n",
      " |          A    B    C    D   E\n",
      " |      2   10   20   30   40 NaN\n",
      " |      3   60   70   80   90 NaN\n",
      " |      4  600  700  800  900 NaN\n",
      " |      \n",
      " |      We can also align on the index:\n",
      " |      \n",
      " |      >>> left, right = df.align(other, join=\"outer\", axis=0)  # doctest: +SKIP\n",
      " |      >>> left  # doctest: +SKIP\n",
      " |          D    B    E    A\n",
      " |      1  1.0  2.0  3.0  4.0\n",
      " |      2  6.0  7.0  8.0  9.0\n",
      " |      3  NaN  NaN  NaN  NaN\n",
      " |      4  NaN  NaN  NaN  NaN\n",
      " |      >>> right  # doctest: +SKIP\n",
      " |          A      B      C      D\n",
      " |      1    NaN    NaN    NaN    NaN\n",
      " |      2   10.0   20.0   30.0   40.0\n",
      " |      3   60.0   70.0   80.0   90.0\n",
      " |      4  600.0  700.0  800.0  900.0\n",
      " |      \n",
      " |      Finally, the default `axis=None` will align on both index and columns:\n",
      " |      \n",
      " |      >>> left, right = df.align(other, join=\"outer\", axis=None)  # doctest: +SKIP\n",
      " |      >>> left  # doctest: +SKIP\n",
      " |           A    B   C    D    E\n",
      " |      1  4.0  2.0 NaN  1.0  3.0\n",
      " |      2  9.0  7.0 NaN  6.0  8.0\n",
      " |      3  NaN  NaN NaN  NaN  NaN\n",
      " |      4  NaN  NaN NaN  NaN  NaN\n",
      " |      >>> right  # doctest: +SKIP\n",
      " |             A      B      C      D   E\n",
      " |      1    NaN    NaN    NaN    NaN NaN\n",
      " |      2   10.0   20.0   30.0   40.0 NaN\n",
      " |      3   60.0   70.0   80.0   90.0 NaN\n",
      " |      4  600.0  700.0  800.0  900.0 NaN\n",
      " |  \n",
      " |  all(self, axis=None, skipna=True, split_every=False, out=None)\n",
      " |      Return whether all elements are True, potentially over an axis.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.all.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Returns True unless there at least one element within a series or\n",
      " |      along a Dataframe axis that is False or equivalent (e.g. zero or\n",
      " |      empty).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {0 or 'index', 1 or 'columns', None}, default 0\n",
      " |          Indicate which axis or axes should be reduced.\n",
      " |      \n",
      " |          * 0 / 'index' : reduce the index, return a Series whose index is the\n",
      " |            original column labels.\n",
      " |          * 1 / 'columns' : reduce the columns, return a Series whose index is the\n",
      " |            original index.\n",
      " |          * None : reduce all axes, return a scalar.\n",
      " |      \n",
      " |      bool_only : bool, default None  (Not supported in Dask)\n",
      " |          Include only boolean columns. If None, will attempt to use everything,\n",
      " |          then use only boolean data. Not implemented for Series.\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values. If the entire row/column is NA and skipna is\n",
      " |          True, then the result will be True, as for an empty row/column.\n",
      " |          If skipna is False, then NA are treated as True, because these are not\n",
      " |          equal to zero.\n",
      " |      level : int or level name, default None  (Not supported in Dask)\n",
      " |          If the axis is a MultiIndex (hierarchical), count along a\n",
      " |          particular level, collapsing into a Series.\n",
      " |      **kwargs : any, default None\n",
      " |          Additional keywords have no effect but might be accepted for\n",
      " |          compatibility with NumPy.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame\n",
      " |          If level is specified, then, DataFrame is returned; otherwise, Series\n",
      " |          is returned.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.all : Return True if all elements are True.\n",
      " |      DataFrame.any : Return True if one (or more) elements are True.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      **Series**\n",
      " |      \n",
      " |      >>> pd.Series([True, True]).all()  # doctest: +SKIP\n",
      " |      True\n",
      " |      >>> pd.Series([True, False]).all()  # doctest: +SKIP\n",
      " |      False\n",
      " |      >>> pd.Series([], dtype=\"float64\").all()  # doctest: +SKIP\n",
      " |      True\n",
      " |      >>> pd.Series([np.nan]).all()  # doctest: +SKIP\n",
      " |      True\n",
      " |      >>> pd.Series([np.nan]).all(skipna=False)  # doctest: +SKIP\n",
      " |      True\n",
      " |      \n",
      " |      **DataFrames**\n",
      " |      \n",
      " |      Create a dataframe from a dictionary.\n",
      " |      \n",
      " |      >>> df = pd.DataFrame({'col1': [True, True], 'col2': [True, False]})  # doctest: +SKIP\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |         col1   col2\n",
      " |      0  True   True\n",
      " |      1  True  False\n",
      " |      \n",
      " |      Default behaviour checks if column-wise values all return True.\n",
      " |      \n",
      " |      >>> df.all()  # doctest: +SKIP\n",
      " |      col1     True\n",
      " |      col2    False\n",
      " |      dtype: bool\n",
      " |      \n",
      " |      Specify ``axis='columns'`` to check if row-wise values all return True.\n",
      " |      \n",
      " |      >>> df.all(axis='columns')  # doctest: +SKIP\n",
      " |      0     True\n",
      " |      1    False\n",
      " |      dtype: bool\n",
      " |      \n",
      " |      Or ``axis=None`` for whether every value is True.\n",
      " |      \n",
      " |      >>> df.all(axis=None)  # doctest: +SKIP\n",
      " |      False\n",
      " |  \n",
      " |  any(self, axis=None, skipna=True, split_every=False, out=None)\n",
      " |      Return whether any element is True, potentially over an axis.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.any.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Returns False unless there is at least one element within a series or\n",
      " |      along a Dataframe axis that is True or equivalent (e.g. non-zero or\n",
      " |      non-empty).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {0 or 'index', 1 or 'columns', None}, default 0\n",
      " |          Indicate which axis or axes should be reduced.\n",
      " |      \n",
      " |          * 0 / 'index' : reduce the index, return a Series whose index is the\n",
      " |            original column labels.\n",
      " |          * 1 / 'columns' : reduce the columns, return a Series whose index is the\n",
      " |            original index.\n",
      " |          * None : reduce all axes, return a scalar.\n",
      " |      \n",
      " |      bool_only : bool, default None  (Not supported in Dask)\n",
      " |          Include only boolean columns. If None, will attempt to use everything,\n",
      " |          then use only boolean data. Not implemented for Series.\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values. If the entire row/column is NA and skipna is\n",
      " |          True, then the result will be False, as for an empty row/column.\n",
      " |          If skipna is False, then NA are treated as True, because these are not\n",
      " |          equal to zero.\n",
      " |      level : int or level name, default None  (Not supported in Dask)\n",
      " |          If the axis is a MultiIndex (hierarchical), count along a\n",
      " |          particular level, collapsing into a Series.\n",
      " |      **kwargs : any, default None\n",
      " |          Additional keywords have no effect but might be accepted for\n",
      " |          compatibility with NumPy.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame\n",
      " |          If level is specified, then, DataFrame is returned; otherwise, Series\n",
      " |          is returned.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.any : Numpy version of this method.\n",
      " |      Series.any : Return whether any element is True.\n",
      " |      Series.all : Return whether all elements are True.\n",
      " |      DataFrame.any : Return whether any element is True over requested axis.\n",
      " |      DataFrame.all : Return whether all elements are True over requested axis.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      **Series**\n",
      " |      \n",
      " |      For Series input, the output is a scalar indicating whether any element\n",
      " |      is True.\n",
      " |      \n",
      " |      >>> pd.Series([False, False]).any()  # doctest: +SKIP\n",
      " |      False\n",
      " |      >>> pd.Series([True, False]).any()  # doctest: +SKIP\n",
      " |      True\n",
      " |      >>> pd.Series([], dtype=\"float64\").any()  # doctest: +SKIP\n",
      " |      False\n",
      " |      >>> pd.Series([np.nan]).any()  # doctest: +SKIP\n",
      " |      False\n",
      " |      >>> pd.Series([np.nan]).any(skipna=False)  # doctest: +SKIP\n",
      " |      True\n",
      " |      \n",
      " |      **DataFrame**\n",
      " |      \n",
      " |      Whether each column contains at least one True element (the default).\n",
      " |      \n",
      " |      >>> df = pd.DataFrame({\"A\": [1, 2], \"B\": [0, 2], \"C\": [0, 0]})  # doctest: +SKIP\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |         A  B  C\n",
      " |      0  1  0  0\n",
      " |      1  2  2  0\n",
      " |      \n",
      " |      >>> df.any()  # doctest: +SKIP\n",
      " |      A     True\n",
      " |      B     True\n",
      " |      C    False\n",
      " |      dtype: bool\n",
      " |      \n",
      " |      Aggregating over the columns.\n",
      " |      \n",
      " |      >>> df = pd.DataFrame({\"A\": [True, False], \"B\": [1, 2]})  # doctest: +SKIP\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |             A  B\n",
      " |      0   True  1\n",
      " |      1  False  2\n",
      " |      \n",
      " |      >>> df.any(axis='columns')  # doctest: +SKIP\n",
      " |      0    True\n",
      " |      1    True\n",
      " |      dtype: bool\n",
      " |      \n",
      " |      >>> df = pd.DataFrame({\"A\": [True, False], \"B\": [1, 0]})  # doctest: +SKIP\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |             A  B\n",
      " |      0   True  1\n",
      " |      1  False  0\n",
      " |      \n",
      " |      >>> df.any(axis='columns')  # doctest: +SKIP\n",
      " |      0    True\n",
      " |      1    False\n",
      " |      dtype: bool\n",
      " |      \n",
      " |      Aggregating over the entire DataFrame with ``axis=None``.\n",
      " |      \n",
      " |      >>> df.any(axis=None)  # doctest: +SKIP\n",
      " |      True\n",
      " |      \n",
      " |      `any` for an empty DataFrame is an empty Series.\n",
      " |      \n",
      " |      >>> pd.DataFrame([]).any()  # doctest: +SKIP\n",
      " |      Series([], dtype: bool)\n",
      " |  \n",
      " |  astype(self, dtype)\n",
      " |      Cast a pandas object to a specified dtype ``dtype``.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.astype.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dtype : data type, or dict of column name -> data type\n",
      " |          Use a numpy.dtype or Python type to cast entire pandas object to\n",
      " |          the same type. Alternatively, use {col: dtype, ...}, where col is a\n",
      " |          column label and dtype is a numpy.dtype or Python type to cast one\n",
      " |          or more of the DataFrame's columns to column-specific types.\n",
      " |      copy : bool, default True  (Not supported in Dask)\n",
      " |          Return a copy when ``copy=True`` (be very careful setting\n",
      " |          ``copy=False`` as changes to values then may propagate to other\n",
      " |          pandas objects).\n",
      " |      errors : {'raise', 'ignore'}, default 'raise'  (Not supported in Dask)\n",
      " |          Control raising of exceptions on invalid data for provided dtype.\n",
      " |      \n",
      " |          - ``raise`` : allow exceptions to be raised\n",
      " |          - ``ignore`` : suppress exceptions. On error return original object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      casted : same type as caller\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      to_datetime : Convert argument to datetime.\n",
      " |      to_timedelta : Convert argument to timedelta.\n",
      " |      to_numeric : Convert argument to a numeric type.\n",
      " |      numpy.ndarray.astype : Cast a numpy array to a specified type.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: 1.3.0\n",
      " |      \n",
      " |          Using ``astype`` to convert from timezone-naive dtype to\n",
      " |          timezone-aware dtype is deprecated and will raise in a\n",
      " |          future version.  Use :meth:`Series.dt.tz_localize` instead.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Create a DataFrame:\n",
      " |      \n",
      " |      >>> d = {'col1': [1, 2], 'col2': [3, 4]}  # doctest: +SKIP\n",
      " |      >>> df = pd.DataFrame(data=d)  # doctest: +SKIP\n",
      " |      >>> df.dtypes  # doctest: +SKIP\n",
      " |      col1    int64\n",
      " |      col2    int64\n",
      " |      dtype: object\n",
      " |      \n",
      " |      Cast all columns to int32:\n",
      " |      \n",
      " |      >>> df.astype('int32').dtypes  # doctest: +SKIP\n",
      " |      col1    int32\n",
      " |      col2    int32\n",
      " |      dtype: object\n",
      " |      \n",
      " |      Cast col1 to int32 using a dictionary:\n",
      " |      \n",
      " |      >>> df.astype({'col1': 'int32'}).dtypes  # doctest: +SKIP\n",
      " |      col1    int32\n",
      " |      col2    int64\n",
      " |      dtype: object\n",
      " |      \n",
      " |      Create a series:\n",
      " |      \n",
      " |      >>> ser = pd.Series([1, 2], dtype='int32')  # doctest: +SKIP\n",
      " |      >>> ser  # doctest: +SKIP\n",
      " |      0    1\n",
      " |      1    2\n",
      " |      dtype: int32\n",
      " |      >>> ser.astype('int64')  # doctest: +SKIP\n",
      " |      0    1\n",
      " |      1    2\n",
      " |      dtype: int64\n",
      " |      \n",
      " |      Convert to categorical type:\n",
      " |      \n",
      " |      >>> ser.astype('category')  # doctest: +SKIP\n",
      " |      0    1\n",
      " |      1    2\n",
      " |      dtype: category\n",
      " |      Categories (2, int64): [1, 2]\n",
      " |      \n",
      " |      Convert to ordered categorical type with custom ordering:\n",
      " |      \n",
      " |      >>> from pandas.api.types import CategoricalDtype  # doctest: +SKIP\n",
      " |      >>> cat_dtype = CategoricalDtype(  # doctest: +SKIP\n",
      " |      ...     categories=[2, 1], ordered=True)\n",
      " |      >>> ser.astype(cat_dtype)  # doctest: +SKIP\n",
      " |      0    1\n",
      " |      1    2\n",
      " |      dtype: category\n",
      " |      Categories (2, int64): [2 < 1]\n",
      " |      \n",
      " |      Note that using ``copy=False`` and changing data on a new\n",
      " |      pandas object may propagate changes:\n",
      " |      \n",
      " |      >>> s1 = pd.Series([1, 2])  # doctest: +SKIP\n",
      " |      >>> s2 = s1.astype('int64', copy=False)  # doctest: +SKIP\n",
      " |      >>> s2[0] = 10  # doctest: +SKIP\n",
      " |      >>> s1  # note that s1[0] has changed too  # doctest: +SKIP\n",
      " |      0    10\n",
      " |      1     2\n",
      " |      dtype: int64\n",
      " |      \n",
      " |      Create a series of dates:\n",
      " |      \n",
      " |      >>> ser_date = pd.Series(pd.date_range('20200101', periods=3))  # doctest: +SKIP\n",
      " |      >>> ser_date  # doctest: +SKIP\n",
      " |      0   2020-01-01\n",
      " |      1   2020-01-02\n",
      " |      2   2020-01-03\n",
      " |      dtype: datetime64[ns]\n",
      " |  \n",
      " |  bfill(self, axis=None, limit=None)\n",
      " |      Synonym for :meth:`DataFrame.fillna` with ``method='bfill'``.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.bfill.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series/DataFrame or None\n",
      " |          Object with missing values filled or None if ``inplace=True``.\n",
      " |  \n",
      " |  clear_divisions(self)\n",
      " |      Forget division information\n",
      " |  \n",
      " |  combine(self, other, func, fill_value=None, overwrite=True)\n",
      " |      Perform column-wise combine with another DataFrame.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.combine.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Combines a DataFrame with `other` DataFrame using `func`\n",
      " |      to element-wise combine columns. The row and column indexes of the\n",
      " |      resulting DataFrame will be the union of the two.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : DataFrame\n",
      " |          The DataFrame to merge column-wise.\n",
      " |      func : function\n",
      " |          Function that takes two series as inputs and return a Series or a\n",
      " |          scalar. Used to merge the two dataframes column by columns.\n",
      " |      fill_value : scalar value, default None\n",
      " |          The value to fill NaNs with prior to passing any column to the\n",
      " |          merge func.\n",
      " |      overwrite : bool, default True\n",
      " |          If True, columns in `self` that do not exist in `other` will be\n",
      " |          overwritten with NaNs.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Combination of the provided DataFrames.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.combine_first : Combine two DataFrame objects and default to\n",
      " |          non-null values in frame calling the method.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Combine using a simple function that chooses the smaller column.\n",
      " |      \n",
      " |      >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [4, 4]})  # doctest: +SKIP\n",
      " |      >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})  # doctest: +SKIP\n",
      " |      >>> take_smaller = lambda s1, s2: s1 if s1.sum() < s2.sum() else s2  # doctest: +SKIP\n",
      " |      >>> df1.combine(df2, take_smaller)  # doctest: +SKIP\n",
      " |         A  B\n",
      " |      0  0  3\n",
      " |      1  0  3\n",
      " |      \n",
      " |      Example using a true element-wise combine function.\n",
      " |      \n",
      " |      >>> df1 = pd.DataFrame({'A': [5, 0], 'B': [2, 4]})  # doctest: +SKIP\n",
      " |      >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})  # doctest: +SKIP\n",
      " |      >>> df1.combine(df2, np.minimum)  # doctest: +SKIP\n",
      " |         A  B\n",
      " |      0  1  2\n",
      " |      1  0  3\n",
      " |      \n",
      " |      Using `fill_value` fills Nones prior to passing the column to the\n",
      " |      merge function.\n",
      " |      \n",
      " |      >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [None, 4]})  # doctest: +SKIP\n",
      " |      >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})  # doctest: +SKIP\n",
      " |      >>> df1.combine(df2, take_smaller, fill_value=-5)  # doctest: +SKIP\n",
      " |         A    B\n",
      " |      0  0 -5.0\n",
      " |      1  0  4.0\n",
      " |      \n",
      " |      However, if the same element in both dataframes is None, that None\n",
      " |      is preserved\n",
      " |      \n",
      " |      >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [None, 4]})  # doctest: +SKIP\n",
      " |      >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [None, 3]})  # doctest: +SKIP\n",
      " |      >>> df1.combine(df2, take_smaller, fill_value=-5)  # doctest: +SKIP\n",
      " |          A    B\n",
      " |      0  0 -5.0\n",
      " |      1  0  3.0\n",
      " |      \n",
      " |      Example that demonstrates the use of `overwrite` and behavior when\n",
      " |      the axis differ between the dataframes.\n",
      " |      \n",
      " |      >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [4, 4]})  # doctest: +SKIP\n",
      " |      >>> df2 = pd.DataFrame({'B': [3, 3], 'C': [-10, 1], }, index=[1, 2])  # doctest: +SKIP\n",
      " |      >>> df1.combine(df2, take_smaller)  # doctest: +SKIP\n",
      " |           A    B     C\n",
      " |      0  NaN  NaN   NaN\n",
      " |      1  NaN  3.0 -10.0\n",
      " |      2  NaN  3.0   1.0\n",
      " |      \n",
      " |      >>> df1.combine(df2, take_smaller, overwrite=False)  # doctest: +SKIP\n",
      " |           A    B     C\n",
      " |      0  0.0  NaN   NaN\n",
      " |      1  0.0  3.0 -10.0\n",
      " |      2  NaN  3.0   1.0\n",
      " |      \n",
      " |      Demonstrating the preference of the passed in dataframe.\n",
      " |      \n",
      " |      >>> df2 = pd.DataFrame({'B': [3, 3], 'C': [1, 1], }, index=[1, 2])  # doctest: +SKIP\n",
      " |      >>> df2.combine(df1, take_smaller)  # doctest: +SKIP\n",
      " |         A    B   C\n",
      " |      0  0.0  NaN NaN\n",
      " |      1  0.0  3.0 NaN\n",
      " |      2  NaN  3.0 NaN\n",
      " |      \n",
      " |      >>> df2.combine(df1, take_smaller, overwrite=False)  # doctest: +SKIP\n",
      " |           A    B   C\n",
      " |      0  0.0  NaN NaN\n",
      " |      1  0.0  3.0 1.0\n",
      " |      2  NaN  3.0 1.0\n",
      " |  \n",
      " |  combine_first(self, other)\n",
      " |      Update null elements with value in the same location in `other`.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.combine_first.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Combine two DataFrame objects by filling null values in one DataFrame\n",
      " |      with non-null values from other DataFrame. The row and column indexes\n",
      " |      of the resulting DataFrame will be the union of the two.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : DataFrame\n",
      " |          Provided DataFrame to use to fill null values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          The result of combining the provided DataFrame with the other object.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.combine : Perform series-wise operation on two DataFrames\n",
      " |          using a given function.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df1 = pd.DataFrame({'A': [None, 0], 'B': [None, 4]})  # doctest: +SKIP\n",
      " |      >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})  # doctest: +SKIP\n",
      " |      >>> df1.combine_first(df2)  # doctest: +SKIP\n",
      " |           A    B\n",
      " |      0  1.0  3.0\n",
      " |      1  0.0  4.0\n",
      " |      \n",
      " |      Null values still persist if the location of that null value\n",
      " |      does not exist in `other`\n",
      " |      \n",
      " |      >>> df1 = pd.DataFrame({'A': [None, 0], 'B': [4, None]})  # doctest: +SKIP\n",
      " |      >>> df2 = pd.DataFrame({'B': [3, 3], 'C': [1, 1]}, index=[1, 2])  # doctest: +SKIP\n",
      " |      >>> df1.combine_first(df2)  # doctest: +SKIP\n",
      " |           A    B    C\n",
      " |      0  NaN  4.0  NaN\n",
      " |      1  0.0  3.0  1.0\n",
      " |      2  NaN  3.0  1.0\n",
      " |  \n",
      " |  compute_current_divisions(self, col=None)\n",
      " |      Compute the current divisions of the DataFrame.\n",
      " |      \n",
      " |      This method triggers immediate computation. If you find yourself running this command\n",
      " |      repeatedly for the same dataframe, we recommend storing the result\n",
      " |      so you don't have to rerun it.\n",
      " |      \n",
      " |      If the column or index values overlap between partitions, raises ``ValueError``.\n",
      " |      To prevent this, make sure the data are sorted by the column or index.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      col : string, optional\n",
      " |          Calculate the divisions for a non-index column by passing in the name of the column.\n",
      " |          If col is not specified, the index will be used to calculate divisions.\n",
      " |          In this case, if the divisions are already known, they will be returned\n",
      " |          immediately without computing.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import dask\n",
      " |      >>> ddf = dask.datasets.timeseries(start=\"2021-01-01\", end=\"2021-01-07\", freq=\"1H\").clear_divisions()\n",
      " |      >>> divisions = ddf.compute_current_divisions()\n",
      " |      >>> print(divisions)  # doctest: +NORMALIZE_WHITESPACE\n",
      " |      (Timestamp('2021-01-01 00:00:00'),\n",
      " |       Timestamp('2021-01-02 00:00:00'),\n",
      " |       Timestamp('2021-01-03 00:00:00'),\n",
      " |       Timestamp('2021-01-04 00:00:00'),\n",
      " |       Timestamp('2021-01-05 00:00:00'),\n",
      " |       Timestamp('2021-01-06 00:00:00'),\n",
      " |       Timestamp('2021-01-06 23:00:00'))\n",
      " |      \n",
      " |      >>> ddf.divisions = divisions\n",
      " |      >>> ddf.known_divisions\n",
      " |      True\n",
      " |      \n",
      " |      >>> ddf = ddf.reset_index().clear_divisions()\n",
      " |      >>> divisions = ddf.compute_current_divisions(\"timestamp\")\n",
      " |      >>> print(divisions)  # doctest: +NORMALIZE_WHITESPACE\n",
      " |      (Timestamp('2021-01-01 00:00:00'),\n",
      " |       Timestamp('2021-01-02 00:00:00'),\n",
      " |       Timestamp('2021-01-03 00:00:00'),\n",
      " |       Timestamp('2021-01-04 00:00:00'),\n",
      " |       Timestamp('2021-01-05 00:00:00'),\n",
      " |       Timestamp('2021-01-06 00:00:00'),\n",
      " |       Timestamp('2021-01-06 23:00:00'))\n",
      " |      \n",
      " |      >>> ddf = ddf.set_index(\"timestamp\", divisions=divisions, sorted=True)\n",
      " |  \n",
      " |  copy(self, deep=False)\n",
      " |      Make a copy of the dataframe\n",
      " |      \n",
      " |      This is strictly a shallow copy of the underlying computational graph.\n",
      " |      It does not affect the underlying data\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, default False\n",
      " |          The deep value must be `False` and it is declared as a parameter just for\n",
      " |          compatibility with third-party libraries like cuDF\n",
      " |  \n",
      " |  count(self, axis=None, split_every=False, numeric_only=None)\n",
      " |      Count non-NA cells for each column or row.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.count.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      The values `None`, `NaN`, `NaT`, and optionally `numpy.inf` (depending\n",
      " |      on `pandas.options.mode.use_inf_as_na`) are considered NA.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          If 0 or 'index' counts are generated for each column.\n",
      " |          If 1 or 'columns' counts are generated for each row.\n",
      " |      level : int or str, optional  (Not supported in Dask)\n",
      " |          If the axis is a `MultiIndex` (hierarchical), count along a\n",
      " |          particular `level`, collapsing into a `DataFrame`.\n",
      " |          A `str` specifies the level name.\n",
      " |      numeric_only : bool, default False\n",
      " |          Include only `float`, `int` or `boolean` data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame\n",
      " |          For each column/row the number of non-NA/null entries.\n",
      " |          If `level` is specified returns a `DataFrame`.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.count: Number of non-NA elements in a Series.\n",
      " |      DataFrame.value_counts: Count unique combinations of columns.\n",
      " |      DataFrame.shape: Number of DataFrame rows and columns (including NA\n",
      " |          elements).\n",
      " |      DataFrame.isna: Boolean same-sized DataFrame showing places of NA\n",
      " |          elements.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Constructing DataFrame from a dictionary:\n",
      " |      \n",
      " |      >>> df = pd.DataFrame({\"Person\":  # doctest: +SKIP\n",
      " |      ...                    [\"John\", \"Myla\", \"Lewis\", \"John\", \"Myla\"],\n",
      " |      ...                    \"Age\": [24., np.nan, 21., 33, 26],\n",
      " |      ...                    \"Single\": [False, True, True, True, False]})\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |         Person   Age  Single\n",
      " |      0    John  24.0   False\n",
      " |      1    Myla   NaN    True\n",
      " |      2   Lewis  21.0    True\n",
      " |      3    John  33.0    True\n",
      " |      4    Myla  26.0   False\n",
      " |      \n",
      " |      Notice the uncounted NA values:\n",
      " |      \n",
      " |      >>> df.count()  # doctest: +SKIP\n",
      " |      Person    5\n",
      " |      Age       4\n",
      " |      Single    5\n",
      " |      dtype: int64\n",
      " |      \n",
      " |      Counts for each **row**:\n",
      " |      \n",
      " |      >>> df.count(axis='columns')  # doctest: +SKIP\n",
      " |      0    3\n",
      " |      1    2\n",
      " |      2    3\n",
      " |      3    3\n",
      " |      4    3\n",
      " |      dtype: int64\n",
      " |  \n",
      " |  cummax(self, axis=None, skipna=True, out=None)\n",
      " |      Return cumulative maximum over a DataFrame or Series axis.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.cummax.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Returns a DataFrame or Series of the same size containing the cumulative\n",
      " |      maximum.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          The index or the name of the axis. 0 is equivalent to None or 'index'.\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values. If an entire row/column is NA, the result\n",
      " |          will be NA.\n",
      " |      *args, **kwargs\n",
      " |          Additional keywords have no effect but might be accepted for\n",
      " |          compatibility with NumPy.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame\n",
      " |          Return cumulative maximum of Series or DataFrame.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      core.window.Expanding.max : Similar functionality\n",
      " |          but ignores ``NaN`` values.\n",
      " |      DataFrame.max : Return the maximum over\n",
      " |          DataFrame axis.\n",
      " |      DataFrame.cummax : Return cumulative maximum over DataFrame axis.\n",
      " |      DataFrame.cummin : Return cumulative minimum over DataFrame axis.\n",
      " |      DataFrame.cumsum : Return cumulative sum over DataFrame axis.\n",
      " |      DataFrame.cumprod : Return cumulative product over DataFrame axis.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      **Series**\n",
      " |      \n",
      " |      >>> s = pd.Series([2, np.nan, 5, -1, 0])  # doctest: +SKIP\n",
      " |      >>> s  # doctest: +SKIP\n",
      " |      0    2.0\n",
      " |      1    NaN\n",
      " |      2    5.0\n",
      " |      3   -1.0\n",
      " |      4    0.0\n",
      " |      dtype: float64\n",
      " |      \n",
      " |      By default, NA values are ignored.\n",
      " |      \n",
      " |      >>> s.cummax()  # doctest: +SKIP\n",
      " |      0    2.0\n",
      " |      1    NaN\n",
      " |      2    5.0\n",
      " |      3    5.0\n",
      " |      4    5.0\n",
      " |      dtype: float64\n",
      " |      \n",
      " |      To include NA values in the operation, use ``skipna=False``\n",
      " |      \n",
      " |      >>> s.cummax(skipna=False)  # doctest: +SKIP\n",
      " |      0    2.0\n",
      " |      1    NaN\n",
      " |      2    NaN\n",
      " |      3    NaN\n",
      " |      4    NaN\n",
      " |      dtype: float64\n",
      " |      \n",
      " |      **DataFrame**\n",
      " |      \n",
      " |      >>> df = pd.DataFrame([[2.0, 1.0],  # doctest: +SKIP\n",
      " |      ...                    [3.0, np.nan],\n",
      " |      ...                    [1.0, 0.0]],\n",
      " |      ...                    columns=list('AB'))\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |           A    B\n",
      " |      0  2.0  1.0\n",
      " |      1  3.0  NaN\n",
      " |      2  1.0  0.0\n",
      " |      \n",
      " |      By default, iterates over rows and finds the maximum\n",
      " |      in each column. This is equivalent to ``axis=None`` or ``axis='index'``.\n",
      " |      \n",
      " |      >>> df.cummax()  # doctest: +SKIP\n",
      " |           A    B\n",
      " |      0  2.0  1.0\n",
      " |      1  3.0  NaN\n",
      " |      2  3.0  1.0\n",
      " |      \n",
      " |      To iterate over columns and find the maximum in each row,\n",
      " |      use ``axis=1``\n",
      " |      \n",
      " |      >>> df.cummax(axis=1)  # doctest: +SKIP\n",
      " |           A    B\n",
      " |      0  2.0  2.0\n",
      " |      1  3.0  NaN\n",
      " |      2  1.0  1.0\n",
      " |  \n",
      " |  cummin(self, axis=None, skipna=True, out=None)\n",
      " |      Return cumulative minimum over a DataFrame or Series axis.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.cummin.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Returns a DataFrame or Series of the same size containing the cumulative\n",
      " |      minimum.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          The index or the name of the axis. 0 is equivalent to None or 'index'.\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values. If an entire row/column is NA, the result\n",
      " |          will be NA.\n",
      " |      *args, **kwargs\n",
      " |          Additional keywords have no effect but might be accepted for\n",
      " |          compatibility with NumPy.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame\n",
      " |          Return cumulative minimum of Series or DataFrame.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      core.window.Expanding.min : Similar functionality\n",
      " |          but ignores ``NaN`` values.\n",
      " |      DataFrame.min : Return the minimum over\n",
      " |          DataFrame axis.\n",
      " |      DataFrame.cummax : Return cumulative maximum over DataFrame axis.\n",
      " |      DataFrame.cummin : Return cumulative minimum over DataFrame axis.\n",
      " |      DataFrame.cumsum : Return cumulative sum over DataFrame axis.\n",
      " |      DataFrame.cumprod : Return cumulative product over DataFrame axis.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      **Series**\n",
      " |      \n",
      " |      >>> s = pd.Series([2, np.nan, 5, -1, 0])  # doctest: +SKIP\n",
      " |      >>> s  # doctest: +SKIP\n",
      " |      0    2.0\n",
      " |      1    NaN\n",
      " |      2    5.0\n",
      " |      3   -1.0\n",
      " |      4    0.0\n",
      " |      dtype: float64\n",
      " |      \n",
      " |      By default, NA values are ignored.\n",
      " |      \n",
      " |      >>> s.cummin()  # doctest: +SKIP\n",
      " |      0    2.0\n",
      " |      1    NaN\n",
      " |      2    2.0\n",
      " |      3   -1.0\n",
      " |      4   -1.0\n",
      " |      dtype: float64\n",
      " |      \n",
      " |      To include NA values in the operation, use ``skipna=False``\n",
      " |      \n",
      " |      >>> s.cummin(skipna=False)  # doctest: +SKIP\n",
      " |      0    2.0\n",
      " |      1    NaN\n",
      " |      2    NaN\n",
      " |      3    NaN\n",
      " |      4    NaN\n",
      " |      dtype: float64\n",
      " |      \n",
      " |      **DataFrame**\n",
      " |      \n",
      " |      >>> df = pd.DataFrame([[2.0, 1.0],  # doctest: +SKIP\n",
      " |      ...                    [3.0, np.nan],\n",
      " |      ...                    [1.0, 0.0]],\n",
      " |      ...                    columns=list('AB'))\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |           A    B\n",
      " |      0  2.0  1.0\n",
      " |      1  3.0  NaN\n",
      " |      2  1.0  0.0\n",
      " |      \n",
      " |      By default, iterates over rows and finds the minimum\n",
      " |      in each column. This is equivalent to ``axis=None`` or ``axis='index'``.\n",
      " |      \n",
      " |      >>> df.cummin()  # doctest: +SKIP\n",
      " |           A    B\n",
      " |      0  2.0  1.0\n",
      " |      1  2.0  NaN\n",
      " |      2  1.0  0.0\n",
      " |      \n",
      " |      To iterate over columns and find the minimum in each row,\n",
      " |      use ``axis=1``\n",
      " |      \n",
      " |      >>> df.cummin(axis=1)  # doctest: +SKIP\n",
      " |           A    B\n",
      " |      0  2.0  1.0\n",
      " |      1  3.0  NaN\n",
      " |      2  1.0  0.0\n",
      " |  \n",
      " |  cumprod(self, axis=None, skipna=True, dtype=None, out=None)\n",
      " |      Return cumulative product over a DataFrame or Series axis.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.cumprod.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Returns a DataFrame or Series of the same size containing the cumulative\n",
      " |      product.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          The index or the name of the axis. 0 is equivalent to None or 'index'.\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values. If an entire row/column is NA, the result\n",
      " |          will be NA.\n",
      " |      *args, **kwargs\n",
      " |          Additional keywords have no effect but might be accepted for\n",
      " |          compatibility with NumPy.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame\n",
      " |          Return cumulative product of Series or DataFrame.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      core.window.Expanding.prod : Similar functionality\n",
      " |          but ignores ``NaN`` values.\n",
      " |      DataFrame.prod : Return the product over\n",
      " |          DataFrame axis.\n",
      " |      DataFrame.cummax : Return cumulative maximum over DataFrame axis.\n",
      " |      DataFrame.cummin : Return cumulative minimum over DataFrame axis.\n",
      " |      DataFrame.cumsum : Return cumulative sum over DataFrame axis.\n",
      " |      DataFrame.cumprod : Return cumulative product over DataFrame axis.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      **Series**\n",
      " |      \n",
      " |      >>> s = pd.Series([2, np.nan, 5, -1, 0])  # doctest: +SKIP\n",
      " |      >>> s  # doctest: +SKIP\n",
      " |      0    2.0\n",
      " |      1    NaN\n",
      " |      2    5.0\n",
      " |      3   -1.0\n",
      " |      4    0.0\n",
      " |      dtype: float64\n",
      " |      \n",
      " |      By default, NA values are ignored.\n",
      " |      \n",
      " |      >>> s.cumprod()  # doctest: +SKIP\n",
      " |      0     2.0\n",
      " |      1     NaN\n",
      " |      2    10.0\n",
      " |      3   -10.0\n",
      " |      4    -0.0\n",
      " |      dtype: float64\n",
      " |      \n",
      " |      To include NA values in the operation, use ``skipna=False``\n",
      " |      \n",
      " |      >>> s.cumprod(skipna=False)  # doctest: +SKIP\n",
      " |      0    2.0\n",
      " |      1    NaN\n",
      " |      2    NaN\n",
      " |      3    NaN\n",
      " |      4    NaN\n",
      " |      dtype: float64\n",
      " |      \n",
      " |      **DataFrame**\n",
      " |      \n",
      " |      >>> df = pd.DataFrame([[2.0, 1.0],  # doctest: +SKIP\n",
      " |      ...                    [3.0, np.nan],\n",
      " |      ...                    [1.0, 0.0]],\n",
      " |      ...                    columns=list('AB'))\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |           A    B\n",
      " |      0  2.0  1.0\n",
      " |      1  3.0  NaN\n",
      " |      2  1.0  0.0\n",
      " |      \n",
      " |      By default, iterates over rows and finds the product\n",
      " |      in each column. This is equivalent to ``axis=None`` or ``axis='index'``.\n",
      " |      \n",
      " |      >>> df.cumprod()  # doctest: +SKIP\n",
      " |           A    B\n",
      " |      0  2.0  1.0\n",
      " |      1  6.0  NaN\n",
      " |      2  6.0  0.0\n",
      " |      \n",
      " |      To iterate over columns and find the product in each row,\n",
      " |      use ``axis=1``\n",
      " |      \n",
      " |      >>> df.cumprod(axis=1)  # doctest: +SKIP\n",
      " |           A    B\n",
      " |      0  2.0  2.0\n",
      " |      1  3.0  NaN\n",
      " |      2  1.0  0.0\n",
      " |  \n",
      " |  cumsum(self, axis=None, skipna=True, dtype=None, out=None)\n",
      " |      Return cumulative sum over a DataFrame or Series axis.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.cumsum.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Returns a DataFrame or Series of the same size containing the cumulative\n",
      " |      sum.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          The index or the name of the axis. 0 is equivalent to None or 'index'.\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values. If an entire row/column is NA, the result\n",
      " |          will be NA.\n",
      " |      *args, **kwargs\n",
      " |          Additional keywords have no effect but might be accepted for\n",
      " |          compatibility with NumPy.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame\n",
      " |          Return cumulative sum of Series or DataFrame.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      core.window.Expanding.sum : Similar functionality\n",
      " |          but ignores ``NaN`` values.\n",
      " |      DataFrame.sum : Return the sum over\n",
      " |          DataFrame axis.\n",
      " |      DataFrame.cummax : Return cumulative maximum over DataFrame axis.\n",
      " |      DataFrame.cummin : Return cumulative minimum over DataFrame axis.\n",
      " |      DataFrame.cumsum : Return cumulative sum over DataFrame axis.\n",
      " |      DataFrame.cumprod : Return cumulative product over DataFrame axis.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      **Series**\n",
      " |      \n",
      " |      >>> s = pd.Series([2, np.nan, 5, -1, 0])  # doctest: +SKIP\n",
      " |      >>> s  # doctest: +SKIP\n",
      " |      0    2.0\n",
      " |      1    NaN\n",
      " |      2    5.0\n",
      " |      3   -1.0\n",
      " |      4    0.0\n",
      " |      dtype: float64\n",
      " |      \n",
      " |      By default, NA values are ignored.\n",
      " |      \n",
      " |      >>> s.cumsum()  # doctest: +SKIP\n",
      " |      0    2.0\n",
      " |      1    NaN\n",
      " |      2    7.0\n",
      " |      3    6.0\n",
      " |      4    6.0\n",
      " |      dtype: float64\n",
      " |      \n",
      " |      To include NA values in the operation, use ``skipna=False``\n",
      " |      \n",
      " |      >>> s.cumsum(skipna=False)  # doctest: +SKIP\n",
      " |      0    2.0\n",
      " |      1    NaN\n",
      " |      2    NaN\n",
      " |      3    NaN\n",
      " |      4    NaN\n",
      " |      dtype: float64\n",
      " |      \n",
      " |      **DataFrame**\n",
      " |      \n",
      " |      >>> df = pd.DataFrame([[2.0, 1.0],  # doctest: +SKIP\n",
      " |      ...                    [3.0, np.nan],\n",
      " |      ...                    [1.0, 0.0]],\n",
      " |      ...                    columns=list('AB'))\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |           A    B\n",
      " |      0  2.0  1.0\n",
      " |      1  3.0  NaN\n",
      " |      2  1.0  0.0\n",
      " |      \n",
      " |      By default, iterates over rows and finds the sum\n",
      " |      in each column. This is equivalent to ``axis=None`` or ``axis='index'``.\n",
      " |      \n",
      " |      >>> df.cumsum()  # doctest: +SKIP\n",
      " |           A    B\n",
      " |      0  2.0  1.0\n",
      " |      1  5.0  NaN\n",
      " |      2  6.0  1.0\n",
      " |      \n",
      " |      To iterate over columns and find the sum in each row,\n",
      " |      use ``axis=1``\n",
      " |      \n",
      " |      >>> df.cumsum(axis=1)  # doctest: +SKIP\n",
      " |           A    B\n",
      " |      0  2.0  3.0\n",
      " |      1  3.0  NaN\n",
      " |      2  1.0  1.0\n",
      " |  \n",
      " |  describe(self, split_every=False, percentiles=None, percentiles_method='default', include=None, exclude=None, datetime_is_numeric=False)\n",
      " |      Generate descriptive statistics.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.describe.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Descriptive statistics include those that summarize the central\n",
      " |      tendency, dispersion and shape of a\n",
      " |      dataset's distribution, excluding ``NaN`` values.\n",
      " |      \n",
      " |      Analyzes both numeric and object series, as well\n",
      " |      as ``DataFrame`` column sets of mixed data types. The output\n",
      " |      will vary depending on what is provided. Refer to the notes\n",
      " |      below for more detail.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      percentiles : list-like of numbers, optional\n",
      " |          The percentiles to include in the output. All should\n",
      " |          fall between 0 and 1. The default is\n",
      " |          ``[.25, .5, .75]``, which returns the 25th, 50th, and\n",
      " |          75th percentiles.\n",
      " |      include : 'all', list-like of dtypes or None (default), optional\n",
      " |          A white list of data types to include in the result. Ignored\n",
      " |          for ``Series``. Here are the options:\n",
      " |      \n",
      " |          - 'all' : All columns of the input will be included in the output.\n",
      " |          - A list-like of dtypes : Limits the results to the\n",
      " |            provided data types.\n",
      " |            To limit the result to numeric types submit\n",
      " |            ``numpy.number``. To limit it instead to object columns submit\n",
      " |            the ``numpy.object`` data type. Strings\n",
      " |            can also be used in the style of\n",
      " |            ``select_dtypes`` (e.g. ``df.describe(include=['O'])``). To\n",
      " |            select pandas categorical columns, use ``'category'``\n",
      " |          - None (default) : The result will include all numeric columns.\n",
      " |      exclude : list-like of dtypes or None (default), optional,\n",
      " |          A black list of data types to omit from the result. Ignored\n",
      " |          for ``Series``. Here are the options:\n",
      " |      \n",
      " |          - A list-like of dtypes : Excludes the provided data types\n",
      " |            from the result. To exclude numeric types submit\n",
      " |            ``numpy.number``. To exclude object columns submit the data\n",
      " |            type ``numpy.object``. Strings can also be used in the style of\n",
      " |            ``select_dtypes`` (e.g. ``df.describe(exclude=['O'])``). To\n",
      " |            exclude pandas categorical columns, use ``'category'``\n",
      " |          - None (default) : The result will exclude nothing.\n",
      " |      datetime_is_numeric : bool, default False\n",
      " |          Whether to treat datetime dtypes as numeric. This affects statistics\n",
      " |          calculated for the column. For DataFrame input, this also\n",
      " |          controls whether datetime columns are included by default.\n",
      " |      \n",
      " |          .. versionadded:: 1.1.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame\n",
      " |          Summary statistics of the Series or Dataframe provided.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.count: Count number of non-NA/null observations.\n",
      " |      DataFrame.max: Maximum of the values in the object.\n",
      " |      DataFrame.min: Minimum of the values in the object.\n",
      " |      DataFrame.mean: Mean of the values.\n",
      " |      DataFrame.std: Standard deviation of the observations.\n",
      " |      DataFrame.select_dtypes: Subset of a DataFrame including/excluding\n",
      " |          columns based on their dtype.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For numeric data, the result's index will include ``count``,\n",
      " |      ``mean``, ``std``, ``min``, ``max`` as well as lower, ``50`` and\n",
      " |      upper percentiles. By default the lower percentile is ``25`` and the\n",
      " |      upper percentile is ``75``. The ``50`` percentile is the\n",
      " |      same as the median.\n",
      " |      \n",
      " |      For object data (e.g. strings or timestamps), the result's index\n",
      " |      will include ``count``, ``unique``, ``top``, and ``freq``. The ``top``\n",
      " |      is the most common value. The ``freq`` is the most common value's\n",
      " |      frequency. Timestamps also include the ``first`` and ``last`` items.\n",
      " |      \n",
      " |      If multiple object values have the highest count, then the\n",
      " |      ``count`` and ``top`` results will be arbitrarily chosen from\n",
      " |      among those with the highest count.\n",
      " |      \n",
      " |      For mixed data types provided via a ``DataFrame``, the default is to\n",
      " |      return only an analysis of numeric columns. If the dataframe consists\n",
      " |      only of object and categorical data without any numeric columns, the\n",
      " |      default is to return an analysis of both the object and categorical\n",
      " |      columns. If ``include='all'`` is provided as an option, the result\n",
      " |      will include a union of attributes of each type.\n",
      " |      \n",
      " |      The `include` and `exclude` parameters can be used to limit\n",
      " |      which columns in a ``DataFrame`` are analyzed for the output.\n",
      " |      The parameters are ignored when analyzing a ``Series``.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Describing a numeric ``Series``.\n",
      " |      \n",
      " |      >>> s = pd.Series([1, 2, 3])  # doctest: +SKIP\n",
      " |      >>> s.describe()  # doctest: +SKIP\n",
      " |      count    3.0\n",
      " |      mean     2.0\n",
      " |      std      1.0\n",
      " |      min      1.0\n",
      " |      25%      1.5\n",
      " |      50%      2.0\n",
      " |      75%      2.5\n",
      " |      max      3.0\n",
      " |      dtype: float64\n",
      " |      \n",
      " |      Describing a categorical ``Series``.\n",
      " |      \n",
      " |      >>> s = pd.Series(['a', 'a', 'b', 'c'])  # doctest: +SKIP\n",
      " |      >>> s.describe()  # doctest: +SKIP\n",
      " |      count     4\n",
      " |      unique    3\n",
      " |      top       a\n",
      " |      freq      2\n",
      " |      dtype: object\n",
      " |      \n",
      " |      Describing a timestamp ``Series``.\n",
      " |      \n",
      " |      >>> s = pd.Series([  # doctest: +SKIP\n",
      " |      ...   np.datetime64(\"2000-01-01\"),\n",
      " |      ...   np.datetime64(\"2010-01-01\"),\n",
      " |      ...   np.datetime64(\"2010-01-01\")\n",
      " |      ... ])\n",
      " |      >>> s.describe(datetime_is_numeric=True)  # doctest: +SKIP\n",
      " |      count                      3\n",
      " |      mean     2006-09-01 08:00:00\n",
      " |      min      2000-01-01 00:00:00\n",
      " |      25%      2004-12-31 12:00:00\n",
      " |      50%      2010-01-01 00:00:00\n",
      " |      75%      2010-01-01 00:00:00\n",
      " |      max      2010-01-01 00:00:00\n",
      " |      dtype: object\n",
      " |      \n",
      " |      Describing a ``DataFrame``. By default only numeric fields\n",
      " |      are returned.\n",
      " |      \n",
      " |      >>> df = pd.DataFrame({'categorical': pd.Categorical(['d','e','f']),  # doctest: +SKIP\n",
      " |      ...                    'numeric': [1, 2, 3],\n",
      " |      ...                    'object': ['a', 'b', 'c']\n",
      " |      ...                   })\n",
      " |      >>> df.describe()  # doctest: +SKIP\n",
      " |             numeric\n",
      " |      count      3.0\n",
      " |      mean       2.0\n",
      " |      std        1.0\n",
      " |      min        1.0\n",
      " |      25%        1.5\n",
      " |      50%        2.0\n",
      " |      75%        2.5\n",
      " |      max        3.0\n",
      " |      \n",
      " |      Describing all columns of a ``DataFrame`` regardless of data type.\n",
      " |      \n",
      " |      >>> df.describe(include='all')  # doctest: +SKIP\n",
      " |             categorical  numeric object\n",
      " |      count            3      3.0      3\n",
      " |      unique           3      NaN      3\n",
      " |      top              f      NaN      a\n",
      " |      freq             1      NaN      1\n",
      " |      mean           NaN      2.0    NaN\n",
      " |      std            NaN      1.0    NaN\n",
      " |      min            NaN      1.0    NaN\n",
      " |      25%            NaN      1.5    NaN\n",
      " |      50%            NaN      2.0    NaN\n",
      " |      75%            NaN      2.5    NaN\n",
      " |      max            NaN      3.0    NaN\n",
      " |      \n",
      " |      Describing a column from a ``DataFrame`` by accessing it as\n",
      " |      an attribute.\n",
      " |      \n",
      " |      >>> df.numeric.describe()  # doctest: +SKIP\n",
      " |      count    3.0\n",
      " |      mean     2.0\n",
      " |      std      1.0\n",
      " |      min      1.0\n",
      " |      25%      1.5\n",
      " |      50%      2.0\n",
      " |      75%      2.5\n",
      " |      max      3.0\n",
      " |      Name: numeric, dtype: float64\n",
      " |      \n",
      " |      Including only numeric columns in a ``DataFrame`` description.\n",
      " |      \n",
      " |      >>> df.describe(include=[np.number])  # doctest: +SKIP\n",
      " |             numeric\n",
      " |      count      3.0\n",
      " |      mean       2.0\n",
      " |      std        1.0\n",
      " |      min        1.0\n",
      " |      25%        1.5\n",
      " |      50%        2.0\n",
      " |      75%        2.5\n",
      " |      max        3.0\n",
      " |      \n",
      " |      Including only string columns in a ``DataFrame`` description.\n",
      " |      \n",
      " |      >>> df.describe(include=[object])  # doctest: +SKIP\n",
      " |             object\n",
      " |      count       3\n",
      " |      unique      3\n",
      " |      top         a\n",
      " |      freq        1\n",
      " |      \n",
      " |      Including only categorical columns from a ``DataFrame`` description.\n",
      " |      \n",
      " |      >>> df.describe(include=['category'])  # doctest: +SKIP\n",
      " |             categorical\n",
      " |      count            3\n",
      " |      unique           3\n",
      " |      top              d\n",
      " |      freq             1\n",
      " |      \n",
      " |      Excluding numeric columns from a ``DataFrame`` description.\n",
      " |      \n",
      " |      >>> df.describe(exclude=[np.number])  # doctest: +SKIP\n",
      " |             categorical object\n",
      " |      count            3      3\n",
      " |      unique           3      3\n",
      " |      top              f      a\n",
      " |      freq             1      1\n",
      " |      \n",
      " |      Excluding object columns from a ``DataFrame`` description.\n",
      " |      \n",
      " |      >>> df.describe(exclude=[object])  # doctest: +SKIP\n",
      " |             categorical  numeric\n",
      " |      count            3      3.0\n",
      " |      unique           3      NaN\n",
      " |      top              f      NaN\n",
      " |      freq             1      NaN\n",
      " |      mean           NaN      2.0\n",
      " |      std            NaN      1.0\n",
      " |      min            NaN      1.0\n",
      " |      25%            NaN      1.5\n",
      " |      50%            NaN      2.0\n",
      " |      75%            NaN      2.5\n",
      " |      max            NaN      3.0\n",
      " |  \n",
      " |  diff(self, periods=1, axis=0)\n",
      " |      First discrete difference of element.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.diff.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      \n",
      " |              .. note::\n",
      " |      \n",
      " |                 Pandas currently uses an ``object``-dtype column to represent\n",
      " |                 boolean data with missing values. This can cause issues for\n",
      " |                 boolean-specific operations, like ``|``. To enable boolean-\n",
      " |                 specific operations, at the cost of metadata that doesn't match\n",
      " |                 pandas, use ``.astype(bool)`` after the ``shift``.\n",
      " |              \n",
      " |      \n",
      " |      Calculates the difference of a Dataframe element compared with another\n",
      " |      element in the Dataframe (default is element in previous row).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      periods : int, default 1\n",
      " |          Periods to shift for calculating difference, accepts negative\n",
      " |          values.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          Take difference over rows (0) or columns (1).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Dataframe\n",
      " |          First differences of the Series.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataframe.pct_change: Percent change over given number of periods.\n",
      " |      Dataframe.shift: Shift index by desired number of periods with an\n",
      " |          optional time freq.\n",
      " |      Series.diff: First discrete difference of object.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For boolean dtypes, this uses :meth:`operator.xor` rather than\n",
      " |      :meth:`operator.sub`.\n",
      " |      The result is calculated according to current dtype in Dataframe,\n",
      " |      however dtype of the result is always float64.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      Difference with previous row\n",
      " |      \n",
      " |      >>> df = pd.DataFrame({'a': [1, 2, 3, 4, 5, 6],  # doctest: +SKIP\n",
      " |      ...                    'b': [1, 1, 2, 3, 5, 8],\n",
      " |      ...                    'c': [1, 4, 9, 16, 25, 36]})\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |         a  b   c\n",
      " |      0  1  1   1\n",
      " |      1  2  1   4\n",
      " |      2  3  2   9\n",
      " |      3  4  3  16\n",
      " |      4  5  5  25\n",
      " |      5  6  8  36\n",
      " |      \n",
      " |      >>> df.diff()  # doctest: +SKIP\n",
      " |           a    b     c\n",
      " |      0  NaN  NaN   NaN\n",
      " |      1  1.0  0.0   3.0\n",
      " |      2  1.0  1.0   5.0\n",
      " |      3  1.0  1.0   7.0\n",
      " |      4  1.0  2.0   9.0\n",
      " |      5  1.0  3.0  11.0\n",
      " |      \n",
      " |      Difference with previous column\n",
      " |      \n",
      " |      >>> df.diff(axis=1)  # doctest: +SKIP\n",
      " |          a  b   c\n",
      " |      0 NaN  0   0\n",
      " |      1 NaN -1   3\n",
      " |      2 NaN -1   7\n",
      " |      3 NaN -1  13\n",
      " |      4 NaN  0  20\n",
      " |      5 NaN  2  28\n",
      " |      \n",
      " |      Difference with 3rd previous row\n",
      " |      \n",
      " |      >>> df.diff(periods=3)  # doctest: +SKIP\n",
      " |           a    b     c\n",
      " |      0  NaN  NaN   NaN\n",
      " |      1  NaN  NaN   NaN\n",
      " |      2  NaN  NaN   NaN\n",
      " |      3  3.0  2.0  15.0\n",
      " |      4  3.0  4.0  21.0\n",
      " |      5  3.0  6.0  27.0\n",
      " |      \n",
      " |      Difference with following row\n",
      " |      \n",
      " |      >>> df.diff(periods=-1)  # doctest: +SKIP\n",
      " |           a    b     c\n",
      " |      0 -1.0  0.0  -3.0\n",
      " |      1 -1.0 -1.0  -5.0\n",
      " |      2 -1.0 -1.0  -7.0\n",
      " |      3 -1.0 -2.0  -9.0\n",
      " |      4 -1.0 -3.0 -11.0\n",
      " |      5  NaN  NaN   NaN\n",
      " |      \n",
      " |      Overflow in input dtype\n",
      " |      \n",
      " |      >>> df = pd.DataFrame({'a': [1, 0]}, dtype=np.uint8)  # doctest: +SKIP\n",
      " |      >>> df.diff()  # doctest: +SKIP\n",
      " |             a\n",
      " |      0    NaN\n",
      " |      1  255.0\n",
      " |  \n",
      " |  dot(self, other, meta='__no_default__')\n",
      " |      Compute the dot product between the Series and the columns of other.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.series.Series.dot.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      This method computes the dot product between the Series and another\n",
      " |      one, or the Series and each columns of a DataFrame, or the Series and\n",
      " |      each columns of an array.\n",
      " |      \n",
      " |      It can also be called using `self @ other` in Python >= 3.5.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : Series, DataFrame or array-like\n",
      " |          The other object to compute the dot product with its columns.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      scalar, Series or numpy.ndarray\n",
      " |          Return the dot product of the Series and other if other is a\n",
      " |          Series, the Series of the dot product of Series and each rows of\n",
      " |          other if other is a DataFrame or a numpy.ndarray between the Series\n",
      " |          and each columns of the numpy array.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.dot: Compute the matrix product with the DataFrame.\n",
      " |      Series.mul: Multiplication of series and other, element-wise.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The Series and other has to share the same index if other is a Series\n",
      " |      or a DataFrame.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> s = pd.Series([0, 1, 2, 3])  # doctest: +SKIP\n",
      " |      >>> other = pd.Series([-1, 2, -3, 4])  # doctest: +SKIP\n",
      " |      >>> s.dot(other)  # doctest: +SKIP\n",
      " |      8\n",
      " |      >>> s @ other  # doctest: +SKIP\n",
      " |      8\n",
      " |      >>> df = pd.DataFrame([[0, 1], [-2, 3], [4, -5], [6, 7]])  # doctest: +SKIP\n",
      " |      >>> s.dot(df)  # doctest: +SKIP\n",
      " |      0    24\n",
      " |      1    14\n",
      " |      dtype: int64\n",
      " |      >>> arr = np.array([[0, 1], [-2, 3], [4, -5], [6, 7]])  # doctest: +SKIP\n",
      " |      >>> s.dot(arr)  # doctest: +SKIP\n",
      " |      array([24, 14])\n",
      " |  \n",
      " |  drop_duplicates(self, subset=None, split_every=None, split_out=1, ignore_index=False, **kwargs)\n",
      " |      Return DataFrame with duplicate rows removed.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.drop_duplicates.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Considering certain columns is optional. Indexes, including time indexes\n",
      " |      are ignored.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      subset : column label or sequence of labels, optional\n",
      " |          Only consider certain columns for identifying duplicates, by\n",
      " |          default use all of the columns.\n",
      " |      keep : {'first', 'last', False}, default 'first'  (Not supported in Dask)\n",
      " |          Determines which duplicates (if any) to keep.\n",
      " |          - ``first`` : Drop duplicates except for the first occurrence.\n",
      " |          - ``last`` : Drop duplicates except for the last occurrence.\n",
      " |          - False : Drop all duplicates.\n",
      " |      inplace : bool, default False  (Not supported in Dask)\n",
      " |          Whether to drop duplicates in place or to return a copy.\n",
      " |      ignore_index : bool, default False\n",
      " |          If True, the resulting axis will be labeled 0, 1, …, n - 1.\n",
      " |      \n",
      " |          .. versionadded:: 1.0.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame or None\n",
      " |          DataFrame with duplicates removed or None if ``inplace=True``.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.value_counts: Count unique combinations of columns.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Consider dataset containing ramen rating.\n",
      " |      \n",
      " |      >>> df = pd.DataFrame({  # doctest: +SKIP\n",
      " |      ...     'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],\n",
      " |      ...     'style': ['cup', 'cup', 'cup', 'pack', 'pack'],\n",
      " |      ...     'rating': [4, 4, 3.5, 15, 5]\n",
      " |      ... })\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |          brand style  rating\n",
      " |      0  Yum Yum   cup     4.0\n",
      " |      1  Yum Yum   cup     4.0\n",
      " |      2  Indomie   cup     3.5\n",
      " |      3  Indomie  pack    15.0\n",
      " |      4  Indomie  pack     5.0\n",
      " |      \n",
      " |      By default, it removes duplicate rows based on all columns.\n",
      " |      \n",
      " |      >>> df.drop_duplicates()  # doctest: +SKIP\n",
      " |          brand style  rating\n",
      " |      0  Yum Yum   cup     4.0\n",
      " |      2  Indomie   cup     3.5\n",
      " |      3  Indomie  pack    15.0\n",
      " |      4  Indomie  pack     5.0\n",
      " |      \n",
      " |      To remove duplicates on specific column(s), use ``subset``.\n",
      " |      \n",
      " |      >>> df.drop_duplicates(subset=['brand'])  # doctest: +SKIP\n",
      " |          brand style  rating\n",
      " |      0  Yum Yum   cup     4.0\n",
      " |      2  Indomie   cup     3.5\n",
      " |      \n",
      " |      To remove duplicates and keep last occurrences, use ``keep``.\n",
      " |      \n",
      " |      >>> df.drop_duplicates(subset=['brand', 'style'], keep='last')  # doctest: +SKIP\n",
      " |          brand style  rating\n",
      " |      1  Yum Yum   cup     4.0\n",
      " |      2  Indomie   cup     3.5\n",
      " |      4  Indomie  pack     5.0\n",
      " |  \n",
      " |  ffill(self, axis=None, limit=None)\n",
      " |      Synonym for :meth:`DataFrame.fillna` with ``method='ffill'``.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.ffill.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series/DataFrame or None\n",
      " |          Object with missing values filled or None if ``inplace=True``.\n",
      " |  \n",
      " |  fillna(self, value=None, method=None, limit=None, axis=None)\n",
      " |      Fill NA/NaN values using the specified method.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.fillna.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      value : scalar, dict, Series, or DataFrame\n",
      " |          Value to use to fill holes (e.g. 0), alternately a\n",
      " |          dict/Series/DataFrame of values specifying which value to use for\n",
      " |          each index (for a Series) or column (for a DataFrame).  Values not\n",
      " |          in the dict/Series/DataFrame will not be filled. This value cannot\n",
      " |          be a list.\n",
      " |      method : {'backfill', 'bfill', 'pad', 'ffill', None}, default None\n",
      " |          Method to use for filling holes in reindexed Series\n",
      " |          pad / ffill: propagate last valid observation forward to next valid\n",
      " |          backfill / bfill: use next valid observation to fill gap.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}\n",
      " |          Axis along which to fill missing values.\n",
      " |      inplace : bool, default False  (Not supported in Dask)\n",
      " |          If True, fill in-place. Note: this will modify any\n",
      " |          other views on this object (e.g., a no-copy slice for a column in a\n",
      " |          DataFrame).\n",
      " |      limit : int, default None\n",
      " |          If method is specified, this is the maximum number of consecutive\n",
      " |          NaN values to forward/backward fill. In other words, if there is\n",
      " |          a gap with more than this number of consecutive NaNs, it will only\n",
      " |          be partially filled. If method is not specified, this is the\n",
      " |          maximum number of entries along the entire axis where NaNs will be\n",
      " |          filled. Must be greater than 0 if not None.\n",
      " |      downcast : dict, default is None  (Not supported in Dask)\n",
      " |          A dict of item->dtype of what to downcast if possible,\n",
      " |          or the string 'infer' which will try to downcast to an appropriate\n",
      " |          equal type (e.g. float64 to int64 if possible).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame or None\n",
      " |          Object with missing values filled or None if ``inplace=True``.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      interpolate : Fill NaN values using interpolation.\n",
      " |      reindex : Conform object to new index.\n",
      " |      asfreq : Convert TimeSeries to specified frequency.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame([[np.nan, 2, np.nan, 0],  # doctest: +SKIP\n",
      " |      ...                    [3, 4, np.nan, 1],\n",
      " |      ...                    [np.nan, np.nan, np.nan, np.nan],\n",
      " |      ...                    [np.nan, 3, np.nan, 4]],\n",
      " |      ...                   columns=list(\"ABCD\"))\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |           A    B   C    D\n",
      " |      0  NaN  2.0 NaN  0.0\n",
      " |      1  3.0  4.0 NaN  1.0\n",
      " |      2  NaN  NaN NaN  NaN\n",
      " |      3  NaN  3.0 NaN  4.0\n",
      " |      \n",
      " |      Replace all NaN elements with 0s.\n",
      " |      \n",
      " |      >>> df.fillna(0)  # doctest: +SKIP\n",
      " |           A    B    C    D\n",
      " |      0  0.0  2.0  0.0  0.0\n",
      " |      1  3.0  4.0  0.0  1.0\n",
      " |      2  0.0  0.0  0.0  0.0\n",
      " |      3  0.0  3.0  0.0  4.0\n",
      " |      \n",
      " |      We can also propagate non-null values forward or backward.\n",
      " |      \n",
      " |      >>> df.fillna(method=\"ffill\")  # doctest: +SKIP\n",
      " |           A    B   C    D\n",
      " |      0  NaN  2.0 NaN  0.0\n",
      " |      1  3.0  4.0 NaN  1.0\n",
      " |      2  3.0  4.0 NaN  1.0\n",
      " |      3  3.0  3.0 NaN  4.0\n",
      " |      \n",
      " |      Replace all NaN elements in column 'A', 'B', 'C', and 'D', with 0, 1,\n",
      " |      2, and 3 respectively.\n",
      " |      \n",
      " |      >>> values = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}  # doctest: +SKIP\n",
      " |      >>> df.fillna(value=values)  # doctest: +SKIP\n",
      " |           A    B    C    D\n",
      " |      0  0.0  2.0  2.0  0.0\n",
      " |      1  3.0  4.0  2.0  1.0\n",
      " |      2  0.0  1.0  2.0  3.0\n",
      " |      3  0.0  3.0  2.0  4.0\n",
      " |      \n",
      " |      Only replace the first NaN element.\n",
      " |      \n",
      " |      >>> df.fillna(value=values, limit=1)  # doctest: +SKIP\n",
      " |           A    B    C    D\n",
      " |      0  0.0  2.0  2.0  0.0\n",
      " |      1  3.0  4.0  NaN  1.0\n",
      " |      2  NaN  1.0  NaN  3.0\n",
      " |      3  NaN  3.0  NaN  4.0\n",
      " |      \n",
      " |      When filling using a DataFrame, replacement happens along\n",
      " |      the same column names and same indices\n",
      " |      \n",
      " |      >>> df2 = pd.DataFrame(np.zeros((4, 4)), columns=list(\"ABCE\"))  # doctest: +SKIP\n",
      " |      >>> df.fillna(df2)  # doctest: +SKIP\n",
      " |           A    B    C    D\n",
      " |      0  0.0  2.0  0.0  0.0\n",
      " |      1  3.0  4.0  0.0  1.0\n",
      " |      2  0.0  0.0  0.0  NaN\n",
      " |      3  0.0  3.0  0.0  4.0\n",
      " |      \n",
      " |      Note that column D is not affected since it is not present in df2.\n",
      " |  \n",
      " |  first(self, offset)\n",
      " |      Select initial periods of time series data based on a date offset.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.first.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      When having a DataFrame with dates as index, this function can\n",
      " |      select the first few rows based on a date offset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      offset : str, DateOffset or dateutil.relativedelta\n",
      " |          The offset length of the data that will be selected. For instance,\n",
      " |          '1M' will display all the rows having their index within the first month.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame\n",
      " |          A subset of the caller.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      TypeError\n",
      " |          If the index is not  a :class:`DatetimeIndex`\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      last : Select final periods of time series based on a date offset.\n",
      " |      at_time : Select values at a particular time of the day.\n",
      " |      between_time : Select values between particular times of the day.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> i = pd.date_range('2018-04-09', periods=4, freq='2D')  # doctest: +SKIP\n",
      " |      >>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)  # doctest: +SKIP\n",
      " |      >>> ts  # doctest: +SKIP\n",
      " |                  A\n",
      " |      2018-04-09  1\n",
      " |      2018-04-11  2\n",
      " |      2018-04-13  3\n",
      " |      2018-04-15  4\n",
      " |      \n",
      " |      Get the rows for the first 3 days:\n",
      " |      \n",
      " |      >>> ts.first('3D')  # doctest: +SKIP\n",
      " |                  A\n",
      " |      2018-04-09  1\n",
      " |      2018-04-11  2\n",
      " |      \n",
      " |      Notice the data for 3 first calendar days were returned, not the first\n",
      " |      3 days observed in the dataset, and therefore data for 2018-04-13 was\n",
      " |      not returned.\n",
      " |  \n",
      " |  get_partition(self, n)\n",
      " |      Get a dask DataFrame/Series representing the `nth` partition.\n",
      " |  \n",
      " |  head(self, n=5, npartitions=1, compute=True)\n",
      " |      First n rows of the dataset\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : int, optional\n",
      " |          The number of rows to return. Default is 5.\n",
      " |      npartitions : int, optional\n",
      " |          Elements are only taken from the first ``npartitions``, with a\n",
      " |          default of 1. If there are fewer than ``n`` rows in the first\n",
      " |          ``npartitions`` a warning will be raised and any found rows\n",
      " |          returned. Pass -1 to use all partitions.\n",
      " |      compute : bool, optional\n",
      " |          Whether to compute the result, default is True.\n",
      " |  \n",
      " |  idxmax(self, axis=None, skipna=True, split_every=False)\n",
      " |      Return index of first occurrence of maximum over requested axis.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.idxmax.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      NA/null values are excluded.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise.\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values. If an entire row/column is NA, the result\n",
      " |          will be NA.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series\n",
      " |          Indexes of maxima along the specified axis.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ValueError\n",
      " |          * If the row/column is empty\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.idxmax : Return index of the maximum element.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method is the DataFrame version of ``ndarray.argmax``.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Consider a dataset containing food consumption in Argentina.\n",
      " |      \n",
      " |      >>> df = pd.DataFrame({'consumption': [10.51, 103.11, 55.48],  # doctest: +SKIP\n",
      " |      ...                    'co2_emissions': [37.2, 19.66, 1712]},\n",
      " |      ...                    index=['Pork', 'Wheat Products', 'Beef'])\n",
      " |      \n",
      " |      >>> df  # doctest: +SKIP\n",
      " |                      consumption  co2_emissions\n",
      " |      Pork                  10.51         37.20\n",
      " |      Wheat Products       103.11         19.66\n",
      " |      Beef                  55.48       1712.00\n",
      " |      \n",
      " |      By default, it returns the index for the maximum value in each column.\n",
      " |      \n",
      " |      >>> df.idxmax()  # doctest: +SKIP\n",
      " |      consumption     Wheat Products\n",
      " |      co2_emissions             Beef\n",
      " |      dtype: object\n",
      " |      \n",
      " |      To return the index for the maximum value in each row, use ``axis=\"columns\"``.\n",
      " |      \n",
      " |      >>> df.idxmax(axis=\"columns\")  # doctest: +SKIP\n",
      " |      Pork              co2_emissions\n",
      " |      Wheat Products     consumption\n",
      " |      Beef              co2_emissions\n",
      " |      dtype: object\n",
      " |  \n",
      " |  idxmin(self, axis=None, skipna=True, split_every=False)\n",
      " |      Return index of first occurrence of minimum over requested axis.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.idxmin.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      NA/null values are excluded.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      " |          The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise.\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values. If an entire row/column is NA, the result\n",
      " |          will be NA.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series\n",
      " |          Indexes of minima along the specified axis.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ValueError\n",
      " |          * If the row/column is empty\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.idxmin : Return index of the minimum element.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method is the DataFrame version of ``ndarray.argmin``.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Consider a dataset containing food consumption in Argentina.\n",
      " |      \n",
      " |      >>> df = pd.DataFrame({'consumption': [10.51, 103.11, 55.48],  # doctest: +SKIP\n",
      " |      ...                    'co2_emissions': [37.2, 19.66, 1712]},\n",
      " |      ...                    index=['Pork', 'Wheat Products', 'Beef'])\n",
      " |      \n",
      " |      >>> df  # doctest: +SKIP\n",
      " |                      consumption  co2_emissions\n",
      " |      Pork                  10.51         37.20\n",
      " |      Wheat Products       103.11         19.66\n",
      " |      Beef                  55.48       1712.00\n",
      " |      \n",
      " |      By default, it returns the index for the minimum value in each column.\n",
      " |      \n",
      " |      >>> df.idxmin()  # doctest: +SKIP\n",
      " |      consumption                Pork\n",
      " |      co2_emissions    Wheat Products\n",
      " |      dtype: object\n",
      " |      \n",
      " |      To return the index for the minimum value in each row, use ``axis=\"columns\"``.\n",
      " |      \n",
      " |      >>> df.idxmin(axis=\"columns\")  # doctest: +SKIP\n",
      " |      Pork                consumption\n",
      " |      Wheat Products    co2_emissions\n",
      " |      Beef                consumption\n",
      " |      dtype: object\n",
      " |  \n",
      " |  isin(self, values)\n",
      " |      Whether each element in the DataFrame is contained in values.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.isin.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      values : iterable, Series, DataFrame or dict\n",
      " |          The result will only be true at a location if all the\n",
      " |          labels match. If `values` is a Series, that's the index. If\n",
      " |          `values` is a dict, the keys must be the column names,\n",
      " |          which must match. If `values` is a DataFrame,\n",
      " |          then both the index and column labels must match.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          DataFrame of booleans showing whether each element in the DataFrame\n",
      " |          is contained in values.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.eq: Equality test for DataFrame.\n",
      " |      Series.isin: Equivalent method on Series.\n",
      " |      Series.str.contains: Test if pattern or regex is contained within a\n",
      " |          string of a Series or Index.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'num_legs': [2, 4], 'num_wings': [2, 0]},  # doctest: +SKIP\n",
      " |      ...                   index=['falcon', 'dog'])\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |              num_legs  num_wings\n",
      " |      falcon         2          2\n",
      " |      dog            4          0\n",
      " |      \n",
      " |      When ``values`` is a list check whether every value in the DataFrame\n",
      " |      is present in the list (which animals have 0 or 2 legs or wings)\n",
      " |      \n",
      " |      >>> df.isin([0, 2])  # doctest: +SKIP\n",
      " |              num_legs  num_wings\n",
      " |      falcon      True       True\n",
      " |      dog        False       True\n",
      " |      \n",
      " |      To check if ``values`` is *not* in the DataFrame, use the ``~`` operator:\n",
      " |      \n",
      " |      >>> ~df.isin([0, 2])  # doctest: +SKIP\n",
      " |              num_legs  num_wings\n",
      " |      falcon     False      False\n",
      " |      dog         True      False\n",
      " |      \n",
      " |      When ``values`` is a dict, we can pass values to check for each\n",
      " |      column separately:\n",
      " |      \n",
      " |      >>> df.isin({'num_wings': [0, 3]})  # doctest: +SKIP\n",
      " |              num_legs  num_wings\n",
      " |      falcon     False      False\n",
      " |      dog        False       True\n",
      " |      \n",
      " |      When ``values`` is a Series or DataFrame the index and column must\n",
      " |      match. Note that 'falcon' does not match based on the number of legs\n",
      " |      in other.\n",
      " |      \n",
      " |      >>> other = pd.DataFrame({'num_legs': [8, 3], 'num_wings': [0, 2]},  # doctest: +SKIP\n",
      " |      ...                      index=['spider', 'falcon'])\n",
      " |      >>> df.isin(other)  # doctest: +SKIP\n",
      " |              num_legs  num_wings\n",
      " |      falcon     False       True\n",
      " |      dog        False      False\n",
      " |  \n",
      " |  isna(self)\n",
      " |      Detect missing values.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.isna.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Return a boolean same-sized object indicating if the values are NA.\n",
      " |      NA values, such as None or :attr:`numpy.NaN`, gets mapped to True\n",
      " |      values.\n",
      " |      Everything else gets mapped to False values. Characters such as empty\n",
      " |      strings ``''`` or :attr:`numpy.inf` are not considered NA values\n",
      " |      (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Mask of bool values for each element in DataFrame that\n",
      " |          indicates whether an element is an NA value.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.isnull : Alias of isna.\n",
      " |      DataFrame.notna : Boolean inverse of isna.\n",
      " |      DataFrame.dropna : Omit axes labels with missing values.\n",
      " |      isna : Top-level isna.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Show which entries in a DataFrame are NA.\n",
      " |      \n",
      " |      >>> df = pd.DataFrame(dict(age=[5, 6, np.NaN],  # doctest: +SKIP\n",
      " |      ...                    born=[pd.NaT, pd.Timestamp('1939-05-27'),\n",
      " |      ...                          pd.Timestamp('1940-04-25')],\n",
      " |      ...                    name=['Alfred', 'Batman', ''],\n",
      " |      ...                    toy=[None, 'Batmobile', 'Joker']))\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |         age       born    name        toy\n",
      " |      0  5.0        NaT  Alfred       None\n",
      " |      1  6.0 1939-05-27  Batman  Batmobile\n",
      " |      2  NaN 1940-04-25              Joker\n",
      " |      \n",
      " |      >>> df.isna()  # doctest: +SKIP\n",
      " |           age   born   name    toy\n",
      " |      0  False   True  False   True\n",
      " |      1  False  False  False  False\n",
      " |      2   True  False  False  False\n",
      " |      \n",
      " |      Show which entries in a Series are NA.\n",
      " |      \n",
      " |      >>> ser = pd.Series([5, 6, np.NaN])  # doctest: +SKIP\n",
      " |      >>> ser  # doctest: +SKIP\n",
      " |      0    5.0\n",
      " |      1    6.0\n",
      " |      2    NaN\n",
      " |      dtype: float64\n",
      " |      \n",
      " |      >>> ser.isna()  # doctest: +SKIP\n",
      " |      0    False\n",
      " |      1    False\n",
      " |      2     True\n",
      " |      dtype: bool\n",
      " |  \n",
      " |  isnull(self)\n",
      " |      DataFrame.isnull is an alias for DataFrame.isna.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.isnull.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Detect missing values.\n",
      " |      \n",
      " |      Return a boolean same-sized object indicating if the values are NA.\n",
      " |      NA values, such as None or :attr:`numpy.NaN`, gets mapped to True\n",
      " |      values.\n",
      " |      Everything else gets mapped to False values. Characters such as empty\n",
      " |      strings ``''`` or :attr:`numpy.inf` are not considered NA values\n",
      " |      (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Mask of bool values for each element in DataFrame that\n",
      " |          indicates whether an element is an NA value.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.isnull : Alias of isna.\n",
      " |      DataFrame.notna : Boolean inverse of isna.\n",
      " |      DataFrame.dropna : Omit axes labels with missing values.\n",
      " |      isna : Top-level isna.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Show which entries in a DataFrame are NA.\n",
      " |      \n",
      " |      >>> df = pd.DataFrame(dict(age=[5, 6, np.NaN],  # doctest: +SKIP\n",
      " |      ...                    born=[pd.NaT, pd.Timestamp('1939-05-27'),\n",
      " |      ...                          pd.Timestamp('1940-04-25')],\n",
      " |      ...                    name=['Alfred', 'Batman', ''],\n",
      " |      ...                    toy=[None, 'Batmobile', 'Joker']))\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |         age       born    name        toy\n",
      " |      0  5.0        NaT  Alfred       None\n",
      " |      1  6.0 1939-05-27  Batman  Batmobile\n",
      " |      2  NaN 1940-04-25              Joker\n",
      " |      \n",
      " |      >>> df.isna()  # doctest: +SKIP\n",
      " |           age   born   name    toy\n",
      " |      0  False   True  False   True\n",
      " |      1  False  False  False  False\n",
      " |      2   True  False  False  False\n",
      " |      \n",
      " |      Show which entries in a Series are NA.\n",
      " |      \n",
      " |      >>> ser = pd.Series([5, 6, np.NaN])  # doctest: +SKIP\n",
      " |      >>> ser  # doctest: +SKIP\n",
      " |      0    5.0\n",
      " |      1    6.0\n",
      " |      2    NaN\n",
      " |      dtype: float64\n",
      " |      \n",
      " |      >>> ser.isna()  # doctest: +SKIP\n",
      " |      0    False\n",
      " |      1    False\n",
      " |      2     True\n",
      " |      dtype: bool\n",
      " |  \n",
      " |  kurtosis(self, axis=None, fisher=True, bias=True, nan_policy='propagate', out=None, numeric_only=None)\n",
      " |      Return unbiased kurtosis over requested axis.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.kurtosis.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      \n",
      " |              .. note::\n",
      " |      \n",
      " |                 This implementation follows the dask.array.stats implementation\n",
      " |                 of kurtosis and calculates kurtosis without taking into account\n",
      " |                 a bias term for finite sample size, which corresponds to the\n",
      " |                 default settings of the scipy.stats kurtosis calculation. This differs\n",
      " |                 from pandas.\n",
      " |      \n",
      " |                 Further, this method currently does not support filtering out NaN\n",
      " |                 values, which is again a difference to Pandas.\n",
      " |              \n",
      " |      \n",
      " |      Kurtosis obtained using Fisher's definition of\n",
      " |      kurtosis (kurtosis of normal == 0.0). Normalized by N-1.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {index (0), columns (1)}\n",
      " |          Axis for the function to be applied on.\n",
      " |      skipna : bool, default True  (Not supported in Dask)\n",
      " |          Exclude NA/null values when computing the result.\n",
      " |      level : int or level name, default None  (Not supported in Dask)\n",
      " |          If the axis is a MultiIndex (hierarchical), count along a\n",
      " |          particular level, collapsing into a Series.\n",
      " |      numeric_only : bool, default None\n",
      " |          Include only float, int, boolean columns. If None, will attempt to use\n",
      " |          everything, then use only numeric data. Not implemented for Series.\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments to be passed to the function.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame (if level specified)\n",
      " |  \n",
      " |  last(self, offset)\n",
      " |      Select final periods of time series data based on a date offset.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.last.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      For a DataFrame with a sorted DatetimeIndex, this function\n",
      " |      selects the last few rows based on a date offset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      offset : str, DateOffset, dateutil.relativedelta\n",
      " |          The offset length of the data that will be selected. For instance,\n",
      " |          '3D' will display all the rows having their index within the last 3 days.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame\n",
      " |          A subset of the caller.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      TypeError\n",
      " |          If the index is not  a :class:`DatetimeIndex`\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      first : Select initial periods of time series based on a date offset.\n",
      " |      at_time : Select values at a particular time of the day.\n",
      " |      between_time : Select values between particular times of the day.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> i = pd.date_range('2018-04-09', periods=4, freq='2D')  # doctest: +SKIP\n",
      " |      >>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)  # doctest: +SKIP\n",
      " |      >>> ts  # doctest: +SKIP\n",
      " |                  A\n",
      " |      2018-04-09  1\n",
      " |      2018-04-11  2\n",
      " |      2018-04-13  3\n",
      " |      2018-04-15  4\n",
      " |      \n",
      " |      Get the rows for the last 3 days:\n",
      " |      \n",
      " |      >>> ts.last('3D')  # doctest: +SKIP\n",
      " |                  A\n",
      " |      2018-04-13  3\n",
      " |      2018-04-15  4\n",
      " |      \n",
      " |      Notice the data for 3 last calendar days were returned, not the last\n",
      " |      3 observed days in the dataset, and therefore data for 2018-04-11 was\n",
      " |      not returned.\n",
      " |  \n",
      " |  map_overlap(self, func, before, after, *args, **kwargs)\n",
      " |      Apply a function to each partition, sharing rows with adjacent partitions.\n",
      " |      \n",
      " |      This can be useful for implementing windowing functions such as\n",
      " |      ``df.rolling(...).mean()`` or ``df.diff()``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : function\n",
      " |          Function applied to each partition.\n",
      " |      before : int\n",
      " |          The number of rows to prepend to partition ``i`` from the end of\n",
      " |          partition ``i - 1``.\n",
      " |      after : int\n",
      " |          The number of rows to append to partition ``i`` from the beginning\n",
      " |          of partition ``i + 1``.\n",
      " |      args, kwargs :\n",
      " |          Arguments and keywords to pass to the function. The partition will\n",
      " |          be the first argument, and these will be passed *after*.\n",
      " |      meta : pd.DataFrame, pd.Series, dict, iterable, tuple, optional\n",
      " |          An empty ``pd.DataFrame`` or ``pd.Series`` that matches the dtypes\n",
      " |          and column names of the output. This metadata is necessary for\n",
      " |          many algorithms in dask dataframe to work.  For ease of use, some\n",
      " |          alternative inputs are also available. Instead of a ``DataFrame``,\n",
      " |          a ``dict`` of ``{name: dtype}`` or iterable of ``(name, dtype)``\n",
      " |          can be provided (note that the order of the names should match the\n",
      " |          order of the columns). Instead of a series, a tuple of ``(name,\n",
      " |          dtype)`` can be used. If not provided, dask will try to infer the\n",
      " |          metadata. This may lead to unexpected results, so providing\n",
      " |          ``meta`` is recommended. For more information, see\n",
      " |          ``dask.dataframe.utils.make_meta``.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Given positive integers ``before`` and ``after``, and a function\n",
      " |      ``func``, ``map_overlap`` does the following:\n",
      " |      \n",
      " |      1. Prepend ``before`` rows to each partition ``i`` from the end of\n",
      " |         partition ``i - 1``. The first partition has no rows prepended.\n",
      " |      \n",
      " |      2. Append ``after`` rows to each partition ``i`` from the beginning of\n",
      " |         partition ``i + 1``. The last partition has no rows appended.\n",
      " |      \n",
      " |      3. Apply ``func`` to each partition, passing in any extra ``args`` and\n",
      " |         ``kwargs`` if provided.\n",
      " |      \n",
      " |      4. Trim ``before`` rows from the beginning of all but the first\n",
      " |         partition.\n",
      " |      \n",
      " |      5. Trim ``after`` rows from the end of all but the last partition.\n",
      " |      \n",
      " |      Note that the index and divisions are assumed to remain unchanged.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Given a DataFrame, Series, or Index, such as:\n",
      " |      \n",
      " |      >>> import pandas as pd\n",
      " |      >>> import dask.dataframe as dd\n",
      " |      >>> df = pd.DataFrame({'x': [1, 2, 4, 7, 11],\n",
      " |      ...                    'y': [1., 2., 3., 4., 5.]})\n",
      " |      >>> ddf = dd.from_pandas(df, npartitions=2)\n",
      " |      \n",
      " |      A rolling sum with a trailing moving window of size 2 can be computed by\n",
      " |      overlapping 2 rows before each partition, and then mapping calls to\n",
      " |      ``df.rolling(2).sum()``:\n",
      " |      \n",
      " |      >>> ddf.compute()\n",
      " |          x    y\n",
      " |      0   1  1.0\n",
      " |      1   2  2.0\n",
      " |      2   4  3.0\n",
      " |      3   7  4.0\n",
      " |      4  11  5.0\n",
      " |      >>> ddf.map_overlap(lambda df: df.rolling(2).sum(), 2, 0).compute()\n",
      " |            x    y\n",
      " |      0   NaN  NaN\n",
      " |      1   3.0  3.0\n",
      " |      2   6.0  5.0\n",
      " |      3  11.0  7.0\n",
      " |      4  18.0  9.0\n",
      " |      \n",
      " |      The pandas ``diff`` method computes a discrete difference shifted by a\n",
      " |      number of periods (can be positive or negative). This can be\n",
      " |      implemented by mapping calls to ``df.diff`` to each partition after\n",
      " |      prepending/appending that many rows, depending on sign:\n",
      " |      \n",
      " |      >>> def diff(df, periods=1):\n",
      " |      ...     before, after = (periods, 0) if periods > 0 else (0, -periods)\n",
      " |      ...     return df.map_overlap(lambda df, periods=1: df.diff(periods),\n",
      " |      ...                           periods, 0, periods=periods)\n",
      " |      >>> diff(ddf, 1).compute()\n",
      " |           x    y\n",
      " |      0  NaN  NaN\n",
      " |      1  1.0  1.0\n",
      " |      2  2.0  1.0\n",
      " |      3  3.0  1.0\n",
      " |      4  4.0  1.0\n",
      " |      \n",
      " |      If you have a ``DatetimeIndex``, you can use a ``pd.Timedelta`` for time-\n",
      " |      based windows.\n",
      " |      \n",
      " |      >>> ts = pd.Series(range(10), index=pd.date_range('2017', periods=10))\n",
      " |      >>> dts = dd.from_pandas(ts, npartitions=2)\n",
      " |      >>> dts.map_overlap(lambda df: df.rolling('2D').sum(),\n",
      " |      ...                 pd.Timedelta('2D'), 0).compute()\n",
      " |      2017-01-01     0.0\n",
      " |      2017-01-02     1.0\n",
      " |      2017-01-03     3.0\n",
      " |      2017-01-04     5.0\n",
      " |      2017-01-05     7.0\n",
      " |      2017-01-06     9.0\n",
      " |      2017-01-07    11.0\n",
      " |      2017-01-08    13.0\n",
      " |      2017-01-09    15.0\n",
      " |      2017-01-10    17.0\n",
      " |      Freq: D, dtype: float64\n",
      " |  \n",
      " |  map_partitions(self, func, *args, **kwargs)\n",
      " |      Apply Python function on each DataFrame partition.\n",
      " |      \n",
      " |      Note that the index and divisions are assumed to remain unchanged.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : function\n",
      " |          The function applied to each partition. If this function accepts\n",
      " |          the special ``partition_info`` keyword argument, it will receive\n",
      " |          information on the partition's relative location within the\n",
      " |          dataframe.\n",
      " |      args, kwargs :\n",
      " |          Positional and keyword arguments to pass to the function.\n",
      " |          Positional arguments are computed on a per-partition basis, while\n",
      " |          keyword arguments are shared across all partitions. The partition\n",
      " |          itself will be the first positional argument, with all other\n",
      " |          arguments passed *after*. Arguments can be ``Scalar``, ``Delayed``,\n",
      " |          or regular Python objects. DataFrame-like args (both dask and\n",
      " |          pandas) will be repartitioned to align (if necessary) before\n",
      " |          applying the function; see ``align_dataframes`` to control this\n",
      " |          behavior.\n",
      " |      enforce_metadata : bool, default True\n",
      " |          Whether to enforce at runtime that the structure of the DataFrame\n",
      " |          produced by ``func`` actually matches the structure of ``meta``.\n",
      " |          This will rename and reorder columns for each partition,\n",
      " |          and will raise an error if this doesn't work or types don't match.\n",
      " |      transform_divisions : bool, default True\n",
      " |          Whether to apply the function onto the divisions and apply those\n",
      " |          transformed divisions to the output.\n",
      " |      align_dataframes : bool, default True\n",
      " |          Whether to repartition DataFrame- or Series-like args\n",
      " |          (both dask and pandas) so their divisions align before applying\n",
      " |          the function. This requires all inputs to have known divisions.\n",
      " |          Single-partition inputs will be split into multiple partitions.\n",
      " |      \n",
      " |          If False, all inputs must have either the same number of partitions\n",
      " |          or a single partition. Single-partition inputs will be broadcast to\n",
      " |          every partition of multi-partition inputs.\n",
      " |      meta : pd.DataFrame, pd.Series, dict, iterable, tuple, optional\n",
      " |          An empty ``pd.DataFrame`` or ``pd.Series`` that matches the dtypes\n",
      " |          and column names of the output. This metadata is necessary for\n",
      " |          many algorithms in dask dataframe to work.  For ease of use, some\n",
      " |          alternative inputs are also available. Instead of a ``DataFrame``,\n",
      " |          a ``dict`` of ``{name: dtype}`` or iterable of ``(name, dtype)``\n",
      " |          can be provided (note that the order of the names should match the\n",
      " |          order of the columns). Instead of a series, a tuple of ``(name,\n",
      " |          dtype)`` can be used. If not provided, dask will try to infer the\n",
      " |          metadata. This may lead to unexpected results, so providing\n",
      " |          ``meta`` is recommended. For more information, see\n",
      " |          ``dask.dataframe.utils.make_meta``.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Given a DataFrame, Series, or Index, such as:\n",
      " |      \n",
      " |      >>> import pandas as pd\n",
      " |      >>> import dask.dataframe as dd\n",
      " |      >>> df = pd.DataFrame({'x': [1, 2, 3, 4, 5],\n",
      " |      ...                    'y': [1., 2., 3., 4., 5.]})\n",
      " |      >>> ddf = dd.from_pandas(df, npartitions=2)\n",
      " |      \n",
      " |      One can use ``map_partitions`` to apply a function on each partition.\n",
      " |      Extra arguments and keywords can optionally be provided, and will be\n",
      " |      passed to the function after the partition.\n",
      " |      \n",
      " |      Here we apply a function with arguments and keywords to a DataFrame,\n",
      " |      resulting in a Series:\n",
      " |      \n",
      " |      >>> def myadd(df, a, b=1):\n",
      " |      ...     return df.x + df.y + a + b\n",
      " |      >>> res = ddf.map_partitions(myadd, 1, b=2)\n",
      " |      >>> res.dtype\n",
      " |      dtype('float64')\n",
      " |      \n",
      " |      Here we apply a function to a Series resulting in a Series:\n",
      " |      \n",
      " |      >>> res = ddf.x.map_partitions(lambda x: len(x)) # ddf.x is a Dask Series Structure\n",
      " |      >>> res.dtype\n",
      " |      dtype('int64')\n",
      " |      \n",
      " |      By default, dask tries to infer the output metadata by running your\n",
      " |      provided function on some fake data. This works well in many cases, but\n",
      " |      can sometimes be expensive, or even fail. To avoid this, you can\n",
      " |      manually specify the output metadata with the ``meta`` keyword. This\n",
      " |      can be specified in many forms, for more information see\n",
      " |      ``dask.dataframe.utils.make_meta``.\n",
      " |      \n",
      " |      Here we specify the output is a Series with no name, and dtype\n",
      " |      ``float64``:\n",
      " |      \n",
      " |      >>> res = ddf.map_partitions(myadd, 1, b=2, meta=(None, 'f8'))\n",
      " |      \n",
      " |      Here we map a function that takes in a DataFrame, and returns a\n",
      " |      DataFrame with a new column:\n",
      " |      \n",
      " |      >>> res = ddf.map_partitions(lambda df: df.assign(z=df.x * df.y))\n",
      " |      >>> res.dtypes\n",
      " |      x      int64\n",
      " |      y    float64\n",
      " |      z    float64\n",
      " |      dtype: object\n",
      " |      \n",
      " |      As before, the output metadata can also be specified manually. This\n",
      " |      time we pass in a ``dict``, as the output is a DataFrame:\n",
      " |      \n",
      " |      >>> res = ddf.map_partitions(lambda df: df.assign(z=df.x * df.y),\n",
      " |      ...                          meta={'x': 'i8', 'y': 'f8', 'z': 'f8'})\n",
      " |      \n",
      " |      In the case where the metadata doesn't change, you can also pass in\n",
      " |      the object itself directly:\n",
      " |      \n",
      " |      >>> res = ddf.map_partitions(lambda df: df.head(), meta=ddf)\n",
      " |      \n",
      " |      Also note that the index and divisions are assumed to remain unchanged.\n",
      " |      If the function you're mapping changes the index/divisions, you'll need\n",
      " |      to clear them afterwards:\n",
      " |      \n",
      " |      >>> ddf.map_partitions(func).clear_divisions()  # doctest: +SKIP\n",
      " |      \n",
      " |      Your map function gets information about where it is in the dataframe by\n",
      " |      accepting a special ``partition_info`` keyword argument.\n",
      " |      \n",
      " |      >>> def func(partition, partition_info=None):\n",
      " |      ...     pass\n",
      " |      \n",
      " |      This will receive the following information:\n",
      " |      \n",
      " |      >>> partition_info  # doctest: +SKIP\n",
      " |      {'number': 1, 'division': 3}\n",
      " |      \n",
      " |      For each argument and keyword arguments that are dask dataframes you will\n",
      " |      receive the number (n) which represents the nth partition of the dataframe\n",
      " |      and the division (the first index value in the partition). If divisions\n",
      " |      are not known (for instance if the index is not sorted) then you will get\n",
      " |      None as the division.\n",
      " |  \n",
      " |  mask(self, cond, other=nan)\n",
      " |      Replace values where the condition is True.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.mask.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cond : bool Series/DataFrame, array-like, or callable\n",
      " |          Where `cond` is False, keep the original value. Where\n",
      " |          True, replace with corresponding value from `other`.\n",
      " |          If `cond` is callable, it is computed on the Series/DataFrame and\n",
      " |          should return boolean Series/DataFrame or array. The callable must\n",
      " |          not change input Series/DataFrame (though pandas doesn't check it).\n",
      " |      other : scalar, Series/DataFrame, or callable\n",
      " |          Entries where `cond` is True are replaced with\n",
      " |          corresponding value from `other`.\n",
      " |          If other is callable, it is computed on the Series/DataFrame and\n",
      " |          should return scalar or Series/DataFrame. The callable must not\n",
      " |          change input Series/DataFrame (though pandas doesn't check it).\n",
      " |      inplace : bool, default False  (Not supported in Dask)\n",
      " |          Whether to perform the operation in place on the data.\n",
      " |      axis : int, default None  (Not supported in Dask)\n",
      " |          Alignment axis if needed.\n",
      " |      level : int, default None  (Not supported in Dask)\n",
      " |          Alignment level if needed.\n",
      " |      errors : str, {'raise', 'ignore'}, default 'raise'  (Not supported in Dask)\n",
      " |          Note that currently this parameter won't affect\n",
      " |          the results and will always coerce to a suitable dtype.\n",
      " |      \n",
      " |          - 'raise' : allow exceptions to be raised.\n",
      " |          - 'ignore' : suppress exceptions. On error return original object.\n",
      " |      \n",
      " |      try_cast : bool, default None  (Not supported in Dask)\n",
      " |          Try to cast the result back to the input type (if possible).\n",
      " |      \n",
      " |          .. deprecated:: 1.3.0\n",
      " |              Manually cast back if necessary.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Same type as caller or None if ``inplace=True``.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :func:`DataFrame.where` : Return an object of same shape as\n",
      " |          self.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The mask method is an application of the if-then idiom. For each\n",
      " |      element in the calling DataFrame, if ``cond`` is ``False`` the\n",
      " |      element is used; otherwise the corresponding element from the DataFrame\n",
      " |      ``other`` is used.\n",
      " |      \n",
      " |      The signature for :func:`DataFrame.where` differs from\n",
      " |      :func:`numpy.where`. Roughly ``df1.where(m, df2)`` is equivalent to\n",
      " |      ``np.where(m, df1, df2)``.\n",
      " |      \n",
      " |      For further details and examples see the ``mask`` documentation in\n",
      " |      :ref:`indexing <indexing.where_mask>`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> s = pd.Series(range(5))  # doctest: +SKIP\n",
      " |      >>> s.where(s > 0)  # doctest: +SKIP\n",
      " |      0    NaN\n",
      " |      1    1.0\n",
      " |      2    2.0\n",
      " |      3    3.0\n",
      " |      4    4.0\n",
      " |      dtype: float64\n",
      " |      >>> s.mask(s > 0)  # doctest: +SKIP\n",
      " |      0    0.0\n",
      " |      1    NaN\n",
      " |      2    NaN\n",
      " |      3    NaN\n",
      " |      4    NaN\n",
      " |      dtype: float64\n",
      " |      \n",
      " |      >>> s.where(s > 1, 10)  # doctest: +SKIP\n",
      " |      0    10\n",
      " |      1    10\n",
      " |      2    2\n",
      " |      3    3\n",
      " |      4    4\n",
      " |      dtype: int64\n",
      " |      >>> s.mask(s > 1, 10)  # doctest: +SKIP\n",
      " |      0     0\n",
      " |      1     1\n",
      " |      2    10\n",
      " |      3    10\n",
      " |      4    10\n",
      " |      dtype: int64\n",
      " |      \n",
      " |      >>> df = pd.DataFrame(np.arange(10).reshape(-1, 2), columns=['A', 'B'])  # doctest: +SKIP\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |         A  B\n",
      " |      0  0  1\n",
      " |      1  2  3\n",
      " |      2  4  5\n",
      " |      3  6  7\n",
      " |      4  8  9\n",
      " |      >>> m = df % 3 == 0  # doctest: +SKIP\n",
      " |      >>> df.where(m, -df)  # doctest: +SKIP\n",
      " |         A  B\n",
      " |      0  0 -1\n",
      " |      1 -2  3\n",
      " |      2 -4 -5\n",
      " |      3  6 -7\n",
      " |      4 -8  9\n",
      " |      >>> df.where(m, -df) == np.where(m, df, -df)  # doctest: +SKIP\n",
      " |            A     B\n",
      " |      0  True  True\n",
      " |      1  True  True\n",
      " |      2  True  True\n",
      " |      3  True  True\n",
      " |      4  True  True\n",
      " |      >>> df.where(m, -df) == df.mask(~m, -df)  # doctest: +SKIP\n",
      " |            A     B\n",
      " |      0  True  True\n",
      " |      1  True  True\n",
      " |      2  True  True\n",
      " |      3  True  True\n",
      " |      4  True  True\n",
      " |  \n",
      " |  max(self, axis=None, skipna=True, split_every=False, out=None, numeric_only=None)\n",
      " |      Return the maximum of the values over the requested axis.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.max.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      If you want the *index* of the maximum, use ``idxmax``. This is the equivalent of the ``numpy.ndarray`` method ``argmax``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {index (0), columns (1)}\n",
      " |          Axis for the function to be applied on.\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values when computing the result.\n",
      " |      level : int or level name, default None  (Not supported in Dask)\n",
      " |          If the axis is a MultiIndex (hierarchical), count along a\n",
      " |          particular level, collapsing into a Series.\n",
      " |      numeric_only : bool, default None\n",
      " |          Include only float, int, boolean columns. If None, will attempt to use\n",
      " |          everything, then use only numeric data. Not implemented for Series.\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments to be passed to the function.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame (if level specified)\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.sum : Return the sum.\n",
      " |      Series.min : Return the minimum.\n",
      " |      Series.max : Return the maximum.\n",
      " |      Series.idxmin : Return the index of the minimum.\n",
      " |      Series.idxmax : Return the index of the maximum.\n",
      " |      DataFrame.sum : Return the sum over the requested axis.\n",
      " |      DataFrame.min : Return the minimum over the requested axis.\n",
      " |      DataFrame.max : Return the maximum over the requested axis.\n",
      " |      DataFrame.idxmin : Return the index of the minimum over the requested axis.\n",
      " |      DataFrame.idxmax : Return the index of the maximum over the requested axis.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> idx = pd.MultiIndex.from_arrays([  # doctest: +SKIP\n",
      " |      ...     ['warm', 'warm', 'cold', 'cold'],\n",
      " |      ...     ['dog', 'falcon', 'fish', 'spider']],\n",
      " |      ...     names=['blooded', 'animal'])\n",
      " |      >>> s = pd.Series([4, 2, 0, 8], name='legs', index=idx)  # doctest: +SKIP\n",
      " |      >>> s  # doctest: +SKIP\n",
      " |      blooded  animal\n",
      " |      warm     dog       4\n",
      " |               falcon    2\n",
      " |      cold     fish      0\n",
      " |               spider    8\n",
      " |      Name: legs, dtype: int64\n",
      " |      \n",
      " |      >>> s.max()  # doctest: +SKIP\n",
      " |      8\n",
      " |  \n",
      " |  mean(self, axis=None, skipna=True, split_every=False, dtype=None, out=None, numeric_only=None)\n",
      " |      Return the mean of the values over the requested axis.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.mean.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {index (0), columns (1)}\n",
      " |          Axis for the function to be applied on.\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values when computing the result.\n",
      " |      level : int or level name, default None  (Not supported in Dask)\n",
      " |          If the axis is a MultiIndex (hierarchical), count along a\n",
      " |          particular level, collapsing into a Series.\n",
      " |      numeric_only : bool, default None\n",
      " |          Include only float, int, boolean columns. If None, will attempt to use\n",
      " |          everything, then use only numeric data. Not implemented for Series.\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments to be passed to the function.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame (if level specified)\n",
      " |  \n",
      " |  memory_usage_per_partition(self, index=True, deep=False)\n",
      " |      Return the memory usage of each partition\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      index : bool, default True\n",
      " |          Specifies whether to include the memory usage of the index in\n",
      " |          returned Series.\n",
      " |      deep : bool, default False\n",
      " |          If True, introspect the data deeply by interrogating\n",
      " |          ``object`` dtypes for system-level memory consumption, and include\n",
      " |          it in the returned values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series\n",
      " |          A Series whose index is the partition number and whose values\n",
      " |          are the memory usage of each partition in bytes.\n",
      " |  \n",
      " |  min(self, axis=None, skipna=True, split_every=False, out=None, numeric_only=None)\n",
      " |      Return the minimum of the values over the requested axis.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.min.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      If you want the *index* of the minimum, use ``idxmin``. This is the equivalent of the ``numpy.ndarray`` method ``argmin``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {index (0), columns (1)}\n",
      " |          Axis for the function to be applied on.\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values when computing the result.\n",
      " |      level : int or level name, default None  (Not supported in Dask)\n",
      " |          If the axis is a MultiIndex (hierarchical), count along a\n",
      " |          particular level, collapsing into a Series.\n",
      " |      numeric_only : bool, default None\n",
      " |          Include only float, int, boolean columns. If None, will attempt to use\n",
      " |          everything, then use only numeric data. Not implemented for Series.\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments to be passed to the function.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame (if level specified)\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.sum : Return the sum.\n",
      " |      Series.min : Return the minimum.\n",
      " |      Series.max : Return the maximum.\n",
      " |      Series.idxmin : Return the index of the minimum.\n",
      " |      Series.idxmax : Return the index of the maximum.\n",
      " |      DataFrame.sum : Return the sum over the requested axis.\n",
      " |      DataFrame.min : Return the minimum over the requested axis.\n",
      " |      DataFrame.max : Return the maximum over the requested axis.\n",
      " |      DataFrame.idxmin : Return the index of the minimum over the requested axis.\n",
      " |      DataFrame.idxmax : Return the index of the maximum over the requested axis.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> idx = pd.MultiIndex.from_arrays([  # doctest: +SKIP\n",
      " |      ...     ['warm', 'warm', 'cold', 'cold'],\n",
      " |      ...     ['dog', 'falcon', 'fish', 'spider']],\n",
      " |      ...     names=['blooded', 'animal'])\n",
      " |      >>> s = pd.Series([4, 2, 0, 8], name='legs', index=idx)  # doctest: +SKIP\n",
      " |      >>> s  # doctest: +SKIP\n",
      " |      blooded  animal\n",
      " |      warm     dog       4\n",
      " |               falcon    2\n",
      " |      cold     fish      0\n",
      " |               spider    8\n",
      " |      Name: legs, dtype: int64\n",
      " |      \n",
      " |      >>> s.min()  # doctest: +SKIP\n",
      " |      0\n",
      " |  \n",
      " |  notnull(self)\n",
      " |      DataFrame.notnull is an alias for DataFrame.notna.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.notnull.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Detect existing (non-missing) values.\n",
      " |      \n",
      " |      Return a boolean same-sized object indicating if the values are not NA.\n",
      " |      Non-missing values get mapped to True. Characters such as empty\n",
      " |      strings ``''`` or :attr:`numpy.inf` are not considered NA values\n",
      " |      (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n",
      " |      NA values, such as None or :attr:`numpy.NaN`, get mapped to False\n",
      " |      values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Mask of bool values for each element in DataFrame that\n",
      " |          indicates whether an element is not an NA value.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.notnull : Alias of notna.\n",
      " |      DataFrame.isna : Boolean inverse of notna.\n",
      " |      DataFrame.dropna : Omit axes labels with missing values.\n",
      " |      notna : Top-level notna.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Show which entries in a DataFrame are not NA.\n",
      " |      \n",
      " |      >>> df = pd.DataFrame(dict(age=[5, 6, np.NaN],  # doctest: +SKIP\n",
      " |      ...                    born=[pd.NaT, pd.Timestamp('1939-05-27'),\n",
      " |      ...                          pd.Timestamp('1940-04-25')],\n",
      " |      ...                    name=['Alfred', 'Batman', ''],\n",
      " |      ...                    toy=[None, 'Batmobile', 'Joker']))\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |         age       born    name        toy\n",
      " |      0  5.0        NaT  Alfred       None\n",
      " |      1  6.0 1939-05-27  Batman  Batmobile\n",
      " |      2  NaN 1940-04-25              Joker\n",
      " |      \n",
      " |      >>> df.notna()  # doctest: +SKIP\n",
      " |           age   born  name    toy\n",
      " |      0   True  False  True  False\n",
      " |      1   True   True  True   True\n",
      " |      2  False   True  True   True\n",
      " |      \n",
      " |      Show which entries in a Series are not NA.\n",
      " |      \n",
      " |      >>> ser = pd.Series([5, 6, np.NaN])  # doctest: +SKIP\n",
      " |      >>> ser  # doctest: +SKIP\n",
      " |      0    5.0\n",
      " |      1    6.0\n",
      " |      2    NaN\n",
      " |      dtype: float64\n",
      " |      \n",
      " |      >>> ser.notna()  # doctest: +SKIP\n",
      " |      0     True\n",
      " |      1     True\n",
      " |      2    False\n",
      " |      dtype: bool\n",
      " |  \n",
      " |  nunique_approx(self, split_every=None)\n",
      " |      Approximate number of unique rows.\n",
      " |      \n",
      " |      This method uses the HyperLogLog algorithm for cardinality\n",
      " |      estimation to compute the approximate number of unique rows.\n",
      " |      The approximate error is 0.406%.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      split_every : int, optional\n",
      " |          Group partitions into groups of this size while performing a\n",
      " |          tree-reduction. If set to False, no tree-reduction will be used.\n",
      " |          Default is 8.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      a float representing the approximate number of elements\n",
      " |  \n",
      " |  pipe(self, func, *args, **kwargs)\n",
      " |      Apply chainable functions that expect Series or DataFrames.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.pipe.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : function\n",
      " |          Function to apply to the Series/DataFrame.\n",
      " |          ``args``, and ``kwargs`` are passed into ``func``.\n",
      " |          Alternatively a ``(callable, data_keyword)`` tuple where\n",
      " |          ``data_keyword`` is a string indicating the keyword of\n",
      " |          ``callable`` that expects the Series/DataFrame.\n",
      " |      args : iterable, optional\n",
      " |          Positional arguments passed into ``func``.\n",
      " |      kwargs : mapping, optional\n",
      " |          A dictionary of keyword arguments passed into ``func``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      object : the return type of ``func``.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.apply : Apply a function along input axis of DataFrame.\n",
      " |      DataFrame.applymap : Apply a function elementwise on a whole DataFrame.\n",
      " |      Series.map : Apply a mapping correspondence on a\n",
      " |          :class:`~pandas.Series`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Use ``.pipe`` when chaining together functions that expect\n",
      " |      Series, DataFrames or GroupBy objects. Instead of writing\n",
      " |      \n",
      " |      >>> func(g(h(df), arg1=a), arg2=b, arg3=c)  # doctest: +SKIP\n",
      " |      \n",
      " |      You can write\n",
      " |      \n",
      " |      >>> (df.pipe(h)  # doctest: +SKIP\n",
      " |      ...    .pipe(g, arg1=a)\n",
      " |      ...    .pipe(func, arg2=b, arg3=c)\n",
      " |      ... )  # doctest: +SKIP\n",
      " |      \n",
      " |      If you have a function that takes the data as (say) the second\n",
      " |      argument, pass a tuple indicating which keyword expects the\n",
      " |      data. For example, suppose ``f`` takes its data as ``arg2``:\n",
      " |      \n",
      " |      >>> (df.pipe(h)  # doctest: +SKIP\n",
      " |      ...    .pipe(g, arg1=a)\n",
      " |      ...    .pipe((func, 'arg2'), arg1=a, arg3=c)\n",
      " |      ...  )  # doctest: +SKIP\n",
      " |  \n",
      " |  prod(self, axis=None, skipna=True, split_every=False, dtype=None, out=None, min_count=None, numeric_only=None)\n",
      " |      Return the product of the values over the requested axis.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.prod.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {index (0), columns (1)}\n",
      " |          Axis for the function to be applied on.\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values when computing the result.\n",
      " |      level : int or level name, default None  (Not supported in Dask)\n",
      " |          If the axis is a MultiIndex (hierarchical), count along a\n",
      " |          particular level, collapsing into a Series.\n",
      " |      numeric_only : bool, default None\n",
      " |          Include only float, int, boolean columns. If None, will attempt to use\n",
      " |          everything, then use only numeric data. Not implemented for Series.\n",
      " |      min_count : int, default 0\n",
      " |          The required number of valid values to perform the operation. If fewer than\n",
      " |          ``min_count`` non-NA values are present the result will be NA.\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments to be passed to the function.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame (if level specified)\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.sum : Return the sum.\n",
      " |      Series.min : Return the minimum.\n",
      " |      Series.max : Return the maximum.\n",
      " |      Series.idxmin : Return the index of the minimum.\n",
      " |      Series.idxmax : Return the index of the maximum.\n",
      " |      DataFrame.sum : Return the sum over the requested axis.\n",
      " |      DataFrame.min : Return the minimum over the requested axis.\n",
      " |      DataFrame.max : Return the maximum over the requested axis.\n",
      " |      DataFrame.idxmin : Return the index of the minimum over the requested axis.\n",
      " |      DataFrame.idxmax : Return the index of the maximum over the requested axis.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      By default, the product of an empty or all-NA Series is ``1``\n",
      " |      \n",
      " |      >>> pd.Series([], dtype=\"float64\").prod()  # doctest: +SKIP\n",
      " |      1.0\n",
      " |      \n",
      " |      This can be controlled with the ``min_count`` parameter\n",
      " |      \n",
      " |      >>> pd.Series([], dtype=\"float64\").prod(min_count=1)  # doctest: +SKIP\n",
      " |      nan\n",
      " |      \n",
      " |      Thanks to the ``skipna`` parameter, ``min_count`` handles all-NA and\n",
      " |      empty series identically.\n",
      " |      \n",
      " |      >>> pd.Series([np.nan]).prod()  # doctest: +SKIP\n",
      " |      1.0\n",
      " |      \n",
      " |      >>> pd.Series([np.nan]).prod(min_count=1)  # doctest: +SKIP\n",
      " |      nan\n",
      " |  \n",
      " |  product = prod(self, axis=None, skipna=True, split_every=False, dtype=None, out=None, min_count=None, numeric_only=None)\n",
      " |  \n",
      " |  quantile(self, q=0.5, axis=0, method='default')\n",
      " |      Approximate row-wise and precise column-wise quantiles of DataFrame\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      q : list/array of floats, default 0.5 (50%)\n",
      " |          Iterable of numbers ranging from 0 to 1 for the desired quantiles\n",
      " |      axis : {0, 1, 'index', 'columns'} (default 0)\n",
      " |          0 or 'index' for row-wise, 1 or 'columns' for column-wise\n",
      " |      method : {'default', 'tdigest', 'dask'}, optional\n",
      " |          What method to use. By default will use dask's internal custom\n",
      " |          algorithm (``'dask'``).  If set to ``'tdigest'`` will use tdigest\n",
      " |          for floats and ints and fallback to the ``'dask'`` otherwise.\n",
      " |  \n",
      " |  random_split(self, frac, random_state=None, shuffle=False)\n",
      " |      Pseudorandomly split dataframe into different pieces row-wise\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      frac : list\n",
      " |          List of floats that should sum to one.\n",
      " |      random_state : int or np.random.RandomState\n",
      " |          If int create a new RandomState with this as the seed.\n",
      " |          Otherwise draw from the passed RandomState.\n",
      " |      shuffle : bool, default False\n",
      " |          If set to True, the dataframe is shuffled (within partition)\n",
      " |          before the split.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      50/50 split\n",
      " |      \n",
      " |      >>> a, b = df.random_split([0.5, 0.5])  # doctest: +SKIP\n",
      " |      \n",
      " |      80/10/10 split, consistent random_state\n",
      " |      \n",
      " |      >>> a, b, c = df.random_split([0.8, 0.1, 0.1], random_state=123)  # doctest: +SKIP\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      dask.DataFrame.sample\n",
      " |  \n",
      " |  reduction(self, chunk, aggregate=None, combine=None, meta='__no_default__', token=None, split_every=None, chunk_kwargs=None, aggregate_kwargs=None, combine_kwargs=None, **kwargs)\n",
      " |      Generic row-wise reductions.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      chunk : callable\n",
      " |          Function to operate on each partition. Should return a\n",
      " |          ``pandas.DataFrame``, ``pandas.Series``, or a scalar.\n",
      " |      aggregate : callable, optional\n",
      " |          Function to operate on the concatenated result of ``chunk``. If not\n",
      " |          specified, defaults to ``chunk``. Used to do the final aggregation\n",
      " |          in a tree reduction.\n",
      " |      \n",
      " |          The input to ``aggregate`` depends on the output of ``chunk``.\n",
      " |          If the output of ``chunk`` is a:\n",
      " |      \n",
      " |          - scalar: Input is a Series, with one row per partition.\n",
      " |          - Series: Input is a DataFrame, with one row per partition. Columns\n",
      " |            are the rows in the output series.\n",
      " |          - DataFrame: Input is a DataFrame, with one row per partition.\n",
      " |            Columns are the columns in the output dataframes.\n",
      " |      \n",
      " |          Should return a ``pandas.DataFrame``, ``pandas.Series``, or a\n",
      " |          scalar.\n",
      " |      combine : callable, optional\n",
      " |          Function to operate on intermediate concatenated results of\n",
      " |          ``chunk`` in a tree-reduction. If not provided, defaults to\n",
      " |          ``aggregate``. The input/output requirements should match that of\n",
      " |          ``aggregate`` described above.\n",
      " |      meta : pd.DataFrame, pd.Series, dict, iterable, tuple, optional\n",
      " |          An empty ``pd.DataFrame`` or ``pd.Series`` that matches the dtypes\n",
      " |          and column names of the output. This metadata is necessary for\n",
      " |          many algorithms in dask dataframe to work.  For ease of use, some\n",
      " |          alternative inputs are also available. Instead of a ``DataFrame``,\n",
      " |          a ``dict`` of ``{name: dtype}`` or iterable of ``(name, dtype)``\n",
      " |          can be provided (note that the order of the names should match the\n",
      " |          order of the columns). Instead of a series, a tuple of ``(name,\n",
      " |          dtype)`` can be used. If not provided, dask will try to infer the\n",
      " |          metadata. This may lead to unexpected results, so providing\n",
      " |          ``meta`` is recommended. For more information, see\n",
      " |          ``dask.dataframe.utils.make_meta``.\n",
      " |      token : str, optional\n",
      " |          The name to use for the output keys.\n",
      " |      split_every : int, optional\n",
      " |          Group partitions into groups of this size while performing a\n",
      " |          tree-reduction. If set to False, no tree-reduction will be used,\n",
      " |          and all intermediates will be concatenated and passed to\n",
      " |          ``aggregate``. Default is 8.\n",
      " |      chunk_kwargs : dict, optional\n",
      " |          Keyword arguments to pass on to ``chunk`` only.\n",
      " |      aggregate_kwargs : dict, optional\n",
      " |          Keyword arguments to pass on to ``aggregate`` only.\n",
      " |      combine_kwargs : dict, optional\n",
      " |          Keyword arguments to pass on to ``combine`` only.\n",
      " |      kwargs :\n",
      " |          All remaining keywords will be passed to ``chunk``, ``combine``,\n",
      " |          and ``aggregate``.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import pandas as pd\n",
      " |      >>> import dask.dataframe as dd\n",
      " |      >>> df = pd.DataFrame({'x': range(50), 'y': range(50, 100)})\n",
      " |      >>> ddf = dd.from_pandas(df, npartitions=4)\n",
      " |      \n",
      " |      Count the number of rows in a DataFrame. To do this, count the number\n",
      " |      of rows in each partition, then sum the results:\n",
      " |      \n",
      " |      >>> res = ddf.reduction(lambda x: x.count(),\n",
      " |      ...                     aggregate=lambda x: x.sum())\n",
      " |      >>> res.compute()\n",
      " |      x    50\n",
      " |      y    50\n",
      " |      dtype: int64\n",
      " |      \n",
      " |      Count the number of rows in a Series with elements greater than or\n",
      " |      equal to a value (provided via a keyword).\n",
      " |      \n",
      " |      >>> def count_greater(x, value=0):\n",
      " |      ...     return (x >= value).sum()\n",
      " |      >>> res = ddf.x.reduction(count_greater, aggregate=lambda x: x.sum(),\n",
      " |      ...                       chunk_kwargs={'value': 25})\n",
      " |      >>> res.compute()\n",
      " |      25\n",
      " |      \n",
      " |      Aggregate both the sum and count of a Series at the same time:\n",
      " |      \n",
      " |      >>> def sum_and_count(x):\n",
      " |      ...     return pd.Series({'count': x.count(), 'sum': x.sum()},\n",
      " |      ...                      index=['count', 'sum'])\n",
      " |      >>> res = ddf.x.reduction(sum_and_count, aggregate=lambda x: x.sum())\n",
      " |      >>> res.compute()\n",
      " |      count      50\n",
      " |      sum      1225\n",
      " |      dtype: int64\n",
      " |      \n",
      " |      Doing the same, but for a DataFrame. Here ``chunk`` returns a\n",
      " |      DataFrame, meaning the input to ``aggregate`` is a DataFrame with an\n",
      " |      index with non-unique entries for both 'x' and 'y'. We groupby the\n",
      " |      index, and sum each group to get the final result.\n",
      " |      \n",
      " |      >>> def sum_and_count(x):\n",
      " |      ...     return pd.DataFrame({'count': x.count(), 'sum': x.sum()},\n",
      " |      ...                         columns=['count', 'sum'])\n",
      " |      >>> res = ddf.reduction(sum_and_count,\n",
      " |      ...                     aggregate=lambda x: x.groupby(level=0).sum())\n",
      " |      >>> res.compute()\n",
      " |         count   sum\n",
      " |      x     50  1225\n",
      " |      y     50  3725\n",
      " |  \n",
      " |  repartition(self, divisions=None, npartitions=None, partition_size=None, freq=None, force=False)\n",
      " |      Repartition dataframe along new divisions\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      divisions : list, optional\n",
      " |          The \"dividing lines\" used to split the dataframe into partitions.\n",
      " |          For ``divisions=[0, 10, 50, 100]``, there would be three output partitions,\n",
      " |          where the new index contained [0, 10), [10, 50), and [50, 100), respectively.\n",
      " |          See https://docs.dask.org/en/latest/dataframe-design.html#partitions.\n",
      " |          Only used if npartitions and partition_size isn't specified.\n",
      " |          For convenience if given an integer this will defer to npartitions\n",
      " |          and if given a string it will defer to partition_size (see below)\n",
      " |      npartitions : int, optional\n",
      " |          Approximate number of partitions of output. Only used if partition_size\n",
      " |          isn't specified. The number of partitions used may be slightly\n",
      " |          lower than npartitions depending on data distribution, but will never be\n",
      " |          higher.\n",
      " |      partition_size: int or string, optional\n",
      " |          Max number of bytes of memory for each partition. Use numbers or\n",
      " |          strings like 5MB. If specified npartitions and divisions will be\n",
      " |          ignored.\n",
      " |      \n",
      " |          .. warning::\n",
      " |      \n",
      " |             This keyword argument triggers computation to determine\n",
      " |             the memory size of each partition, which may be expensive.\n",
      " |      \n",
      " |      freq : str, pd.Timedelta\n",
      " |          A period on which to partition timeseries data like ``'7D'`` or\n",
      " |          ``'12h'`` or ``pd.Timedelta(hours=12)``.  Assumes a datetime index.\n",
      " |      force : bool, default False\n",
      " |          Allows the expansion of the existing divisions.\n",
      " |          If False then the new divisions' lower and upper bounds must be\n",
      " |          the same as the old divisions'.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Exactly one of `divisions`, `npartitions`, `partition_size`, or `freq`\n",
      " |      should be specified. A ``ValueError`` will be raised when that is\n",
      " |      not the case.\n",
      " |      \n",
      " |      Also note that ``len(divisons)`` is equal to ``npartitions + 1``. This is because ``divisions``\n",
      " |      represents the upper and lower bounds of each partition. The first item is the\n",
      " |      lower bound of the first partition, the second item is the lower bound of the\n",
      " |      second partition and the upper bound of the first partition, and so on.\n",
      " |      The second-to-last item is the lower bound of the last partition, and the last\n",
      " |      (extra) item is the upper bound of the last partition.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = df.repartition(npartitions=10)  # doctest: +SKIP\n",
      " |      >>> df = df.repartition(divisions=[0, 5, 10, 20])  # doctest: +SKIP\n",
      " |      >>> df = df.repartition(freq='7d')  # doctest: +SKIP\n",
      " |  \n",
      " |  replace(self, to_replace=None, value=None, regex=False)\n",
      " |      Replace values given in `to_replace` with `value`.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.replace.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Values of the DataFrame are replaced with other values dynamically.\n",
      " |      \n",
      " |      This differs from updating with ``.loc`` or ``.iloc``, which require\n",
      " |      you to specify a location to update with some value.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      to_replace : str, regex, list, dict, Series, int, float, or None\n",
      " |          How to find the values that will be replaced.\n",
      " |      \n",
      " |          * numeric, str or regex:\n",
      " |      \n",
      " |              - numeric: numeric values equal to `to_replace` will be\n",
      " |                replaced with `value`\n",
      " |              - str: string exactly matching `to_replace` will be replaced\n",
      " |                with `value`\n",
      " |              - regex: regexs matching `to_replace` will be replaced with\n",
      " |                `value`\n",
      " |      \n",
      " |          * list of str, regex, or numeric:\n",
      " |      \n",
      " |              - First, if `to_replace` and `value` are both lists, they\n",
      " |                **must** be the same length.\n",
      " |              - Second, if ``regex=True`` then all of the strings in **both**\n",
      " |                lists will be interpreted as regexs otherwise they will match\n",
      " |                directly. This doesn't matter much for `value` since there\n",
      " |                are only a few possible substitution regexes you can use.\n",
      " |              - str, regex and numeric rules apply as above.\n",
      " |      \n",
      " |          * dict:\n",
      " |      \n",
      " |              - Dicts can be used to specify different replacement values\n",
      " |                for different existing values. For example,\n",
      " |                ``{'a': 'b', 'y': 'z'}`` replaces the value 'a' with 'b' and\n",
      " |                'y' with 'z'. To use a dict in this way the `value`\n",
      " |                parameter should be `None`.\n",
      " |              - For a DataFrame a dict can specify that different values\n",
      " |                should be replaced in different columns. For example,\n",
      " |                ``{'a': 1, 'b': 'z'}`` looks for the value 1 in column 'a'\n",
      " |                and the value 'z' in column 'b' and replaces these values\n",
      " |                with whatever is specified in `value`. The `value` parameter\n",
      " |                should not be ``None`` in this case. You can treat this as a\n",
      " |                special case of passing two lists except that you are\n",
      " |                specifying the column to search in.\n",
      " |              - For a DataFrame nested dictionaries, e.g.,\n",
      " |                ``{'a': {'b': np.nan}}``, are read as follows: look in column\n",
      " |                'a' for the value 'b' and replace it with NaN. The `value`\n",
      " |                parameter should be ``None`` to use a nested dict in this\n",
      " |                way. You can nest regular expressions as well. Note that\n",
      " |                column names (the top-level dictionary keys in a nested\n",
      " |                dictionary) **cannot** be regular expressions.\n",
      " |      \n",
      " |          * None:\n",
      " |      \n",
      " |              - This means that the `regex` argument must be a string,\n",
      " |                compiled regular expression, or list, dict, ndarray or\n",
      " |                Series of such elements. If `value` is also ``None`` then\n",
      " |                this **must** be a nested dictionary or Series.\n",
      " |      \n",
      " |          See the examples section for examples of each of these.\n",
      " |      value : scalar, dict, list, str, regex, default None\n",
      " |          Value to replace any values matching `to_replace` with.\n",
      " |          For a DataFrame a dict of values can be used to specify which\n",
      " |          value to use for each column (columns not in the dict will not be\n",
      " |          filled). Regular expressions, strings and lists or dicts of such\n",
      " |          objects are also allowed.\n",
      " |      \n",
      " |      inplace : bool, default False  (Not supported in Dask)\n",
      " |          If True, performs operation inplace and returns None.\n",
      " |      limit : int, default None  (Not supported in Dask)\n",
      " |          Maximum size gap to forward or backward fill.\n",
      " |      regex : bool or same types as `to_replace`, default False\n",
      " |          Whether to interpret `to_replace` and/or `value` as regular\n",
      " |          expressions. If this is ``True`` then `to_replace` *must* be a\n",
      " |          string. Alternatively, this could be a regular expression or a\n",
      " |          list, dict, or array of regular expressions in which case\n",
      " |          `to_replace` must be ``None``.\n",
      " |      method : {'pad', 'ffill', 'bfill', `None`}  (Not supported in Dask)\n",
      " |          The method to use when for replacement, when `to_replace` is a\n",
      " |          scalar, list or tuple and `value` is ``None``.\n",
      " |      \n",
      " |          .. versionchanged:: 0.23.0\n",
      " |              Added to DataFrame.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Object after replacement.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      AssertionError\n",
      " |          * If `regex` is not a ``bool`` and `to_replace` is not\n",
      " |            ``None``.\n",
      " |      \n",
      " |      TypeError\n",
      " |          * If `to_replace` is not a scalar, array-like, ``dict``, or ``None``\n",
      " |          * If `to_replace` is a ``dict`` and `value` is not a ``list``,\n",
      " |            ``dict``, ``ndarray``, or ``Series``\n",
      " |          * If `to_replace` is ``None`` and `regex` is not compilable\n",
      " |            into a regular expression or is a list, dict, ndarray, or\n",
      " |            Series.\n",
      " |          * When replacing multiple ``bool`` or ``datetime64`` objects and\n",
      " |            the arguments to `to_replace` does not match the type of the\n",
      " |            value being replaced\n",
      " |      \n",
      " |      ValueError\n",
      " |          * If a ``list`` or an ``ndarray`` is passed to `to_replace` and\n",
      " |            `value` but they are not the same length.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.fillna : Fill NA values.\n",
      " |      DataFrame.where : Replace values based on boolean condition.\n",
      " |      Series.str.replace : Simple string replacement.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      * Regex substitution is performed under the hood with ``re.sub``. The\n",
      " |        rules for substitution for ``re.sub`` are the same.\n",
      " |      * Regular expressions will only substitute on strings, meaning you\n",
      " |        cannot provide, for example, a regular expression matching floating\n",
      " |        point numbers and expect the columns in your frame that have a\n",
      " |        numeric dtype to be matched. However, if those floating point\n",
      " |        numbers *are* strings, then you can do this.\n",
      " |      * This method has *a lot* of options. You are encouraged to experiment\n",
      " |        and play with this method to gain intuition about how it works.\n",
      " |      * When dict is used as the `to_replace` value, it is like\n",
      " |        key(s) in the dict are the to_replace part and\n",
      " |        value(s) in the dict are the value parameter.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      **Scalar `to_replace` and `value`**\n",
      " |      \n",
      " |      >>> s = pd.Series([1, 2, 3, 4, 5])  # doctest: +SKIP\n",
      " |      >>> s.replace(1, 5)  # doctest: +SKIP\n",
      " |      0    5\n",
      " |      1    2\n",
      " |      2    3\n",
      " |      3    4\n",
      " |      4    5\n",
      " |      dtype: int64\n",
      " |      \n",
      " |      >>> df = pd.DataFrame({'A': [0, 1, 2, 3, 4],  # doctest: +SKIP\n",
      " |      ...                    'B': [5, 6, 7, 8, 9],\n",
      " |      ...                    'C': ['a', 'b', 'c', 'd', 'e']})\n",
      " |      >>> df.replace(0, 5)  # doctest: +SKIP\n",
      " |          A  B  C\n",
      " |      0  5  5  a\n",
      " |      1  1  6  b\n",
      " |      2  2  7  c\n",
      " |      3  3  8  d\n",
      " |      4  4  9  e\n",
      " |      \n",
      " |      **List-like `to_replace`**\n",
      " |      \n",
      " |      >>> df.replace([0, 1, 2, 3], 4)  # doctest: +SKIP\n",
      " |          A  B  C\n",
      " |      0  4  5  a\n",
      " |      1  4  6  b\n",
      " |      2  4  7  c\n",
      " |      3  4  8  d\n",
      " |      4  4  9  e\n",
      " |      \n",
      " |      >>> df.replace([0, 1, 2, 3], [4, 3, 2, 1])  # doctest: +SKIP\n",
      " |          A  B  C\n",
      " |      0  4  5  a\n",
      " |      1  3  6  b\n",
      " |      2  2  7  c\n",
      " |      3  1  8  d\n",
      " |      4  4  9  e\n",
      " |      \n",
      " |      >>> s.replace([1, 2], method='bfill')  # doctest: +SKIP\n",
      " |      0    3\n",
      " |      1    3\n",
      " |      2    3\n",
      " |      3    4\n",
      " |      4    5\n",
      " |      dtype: int64\n",
      " |      \n",
      " |      **dict-like `to_replace`**\n",
      " |      \n",
      " |      >>> df.replace({0: 10, 1: 100})  # doctest: +SKIP\n",
      " |              A  B  C\n",
      " |      0   10  5  a\n",
      " |      1  100  6  b\n",
      " |      2    2  7  c\n",
      " |      3    3  8  d\n",
      " |      4    4  9  e\n",
      " |      \n",
      " |      >>> df.replace({'A': 0, 'B': 5}, 100)  # doctest: +SKIP\n",
      " |              A    B  C\n",
      " |      0  100  100  a\n",
      " |      1    1    6  b\n",
      " |      2    2    7  c\n",
      " |      3    3    8  d\n",
      " |      4    4    9  e\n",
      " |      \n",
      " |      >>> df.replace({'A': {0: 100, 4: 400}})  # doctest: +SKIP\n",
      " |              A  B  C\n",
      " |      0  100  5  a\n",
      " |      1    1  6  b\n",
      " |      2    2  7  c\n",
      " |      3    3  8  d\n",
      " |      4  400  9  e\n",
      " |      \n",
      " |      **Regular expression `to_replace`**\n",
      " |      \n",
      " |      >>> df = pd.DataFrame({'A': ['bat', 'foo', 'bait'],  # doctest: +SKIP\n",
      " |      ...                    'B': ['abc', 'bar', 'xyz']})\n",
      " |      >>> df.replace(to_replace=r'^ba.$', value='new', regex=True)  # doctest: +SKIP\n",
      " |              A    B\n",
      " |      0   new  abc\n",
      " |      1   foo  new\n",
      " |      2  bait  xyz\n",
      " |      \n",
      " |      >>> df.replace({'A': r'^ba.$'}, {'A': 'new'}, regex=True)  # doctest: +SKIP\n",
      " |              A    B\n",
      " |      0   new  abc\n",
      " |      1   foo  bar\n",
      " |      2  bait  xyz\n",
      " |      \n",
      " |      >>> df.replace(regex=r'^ba.$', value='new')  # doctest: +SKIP\n",
      " |              A    B\n",
      " |      0   new  abc\n",
      " |      1   foo  new\n",
      " |      2  bait  xyz\n",
      " |      \n",
      " |      >>> df.replace(regex={r'^ba.$': 'new', 'foo': 'xyz'})  # doctest: +SKIP\n",
      " |              A    B\n",
      " |      0   new  abc\n",
      " |      1   xyz  new\n",
      " |      2  bait  xyz\n",
      " |      \n",
      " |      >>> df.replace(regex=[r'^ba.$', 'foo'], value='new')  # doctest: +SKIP\n",
      " |              A    B\n",
      " |      0   new  abc\n",
      " |      1   new  new\n",
      " |      2  bait  xyz\n",
      " |      \n",
      " |      Compare the behavior of ``s.replace({'a': None})`` and\n",
      " |      ``s.replace('a', None)`` to understand the peculiarities\n",
      " |      of the `to_replace` parameter:\n",
      " |      \n",
      " |      >>> s = pd.Series([10, 'a', 'a', 'b', 'a'])  # doctest: +SKIP\n",
      " |      \n",
      " |      When one uses a dict as the `to_replace` value, it is like the\n",
      " |      value(s) in the dict are equal to the `value` parameter.\n",
      " |      ``s.replace({'a': None})`` is equivalent to\n",
      " |      ``s.replace(to_replace={'a': None}, value=None, method=None)``:\n",
      " |      \n",
      " |      >>> s.replace({'a': None})  # doctest: +SKIP\n",
      " |      0      10\n",
      " |      1    None\n",
      " |      2    None\n",
      " |      3       b\n",
      " |      4    None\n",
      " |      dtype: object\n",
      " |      \n",
      " |      When ``value`` is not explicitly passed and `to_replace` is a scalar, list\n",
      " |      or tuple, `replace` uses the method parameter (default 'pad') to do the\n",
      " |      replacement. So this is why the 'a' values are being replaced by 10\n",
      " |      in rows 1 and 2 and 'b' in row 4 in this case.\n",
      " |      \n",
      " |      >>> s.replace('a')  # doctest: +SKIP\n",
      " |      0    10\n",
      " |      1    10\n",
      " |      2    10\n",
      " |      3     b\n",
      " |      4     b\n",
      " |      dtype: object\n",
      " |      \n",
      " |      On the other hand, if ``None`` is explicitly passed for ``value``, it will\n",
      " |      be respected:\n",
      " |      \n",
      " |      >>> s.replace('a', None)  # doctest: +SKIP\n",
      " |      0      10\n",
      " |      1    None\n",
      " |      2    None\n",
      " |      3       b\n",
      " |      4    None\n",
      " |      dtype: object\n",
      " |      \n",
      " |          .. versionchanged:: 1.4.0\n",
      " |              Previously the explicit ``None`` was silently ignored.\n",
      " |  \n",
      " |  resample(self, rule, closed=None, label=None)\n",
      " |      Resample time-series data.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.resample.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Convenience method for frequency conversion and resampling of time series.\n",
      " |      The object must have a datetime-like index (`DatetimeIndex`, `PeriodIndex`,\n",
      " |      or `TimedeltaIndex`), or the caller must pass the label of a datetime-like\n",
      " |      series/index to the ``on``/``level`` keyword parameter.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      rule : DateOffset, Timedelta or str\n",
      " |          The offset string or object representing target conversion.\n",
      " |      axis : {0 or 'index', 1 or 'columns'}, default 0  (Not supported in Dask)\n",
      " |          Which axis to use for up- or down-sampling. For `Series` this\n",
      " |          will default to 0, i.e. along the rows. Must be\n",
      " |          `DatetimeIndex`, `TimedeltaIndex` or `PeriodIndex`.\n",
      " |      closed : {'right', 'left'}, default None\n",
      " |          Which side of bin interval is closed. The default is 'left'\n",
      " |          for all frequency offsets except for 'M', 'A', 'Q', 'BM',\n",
      " |          'BA', 'BQ', and 'W' which all have a default of 'right'.\n",
      " |      label : {'right', 'left'}, default None\n",
      " |          Which bin edge label to label bucket with. The default is 'left'\n",
      " |          for all frequency offsets except for 'M', 'A', 'Q', 'BM',\n",
      " |          'BA', 'BQ', and 'W' which all have a default of 'right'.\n",
      " |      convention : {'start', 'end', 's', 'e'}, default 'start'  (Not supported in Dask)\n",
      " |          For `PeriodIndex` only, controls whether to use the start or\n",
      " |          end of `rule`.\n",
      " |      kind : {'timestamp', 'period'}, optional, default None  (Not supported in Dask)\n",
      " |          Pass 'timestamp' to convert the resulting index to a\n",
      " |          `DateTimeIndex` or 'period' to convert it to a `PeriodIndex`.\n",
      " |          By default the input representation is retained.\n",
      " |      loffset : timedelta, default None  (Not supported in Dask)\n",
      " |          Adjust the resampled time labels.\n",
      " |      \n",
      " |          .. deprecated:: 1.1.0\n",
      " |              You should add the loffset to the `df.index` after the resample.\n",
      " |              See below.\n",
      " |      \n",
      " |      base : int, default 0  (Not supported in Dask)\n",
      " |          For frequencies that evenly subdivide 1 day, the \"origin\" of the\n",
      " |          aggregated intervals. For example, for '5min' frequency, base could\n",
      " |          range from 0 through 4. Defaults to 0.\n",
      " |      \n",
      " |          .. deprecated:: 1.1.0\n",
      " |              The new arguments that you should use are 'offset' or 'origin'.\n",
      " |      \n",
      " |      on : str, optional  (Not supported in Dask)\n",
      " |          For a DataFrame, column to use instead of index for resampling.\n",
      " |          Column must be datetime-like.\n",
      " |      level : str or int, optional  (Not supported in Dask)\n",
      " |          For a MultiIndex, level (name or number) to use for\n",
      " |          resampling. `level` must be datetime-like.\n",
      " |      origin : Timestamp or str, default 'start_day'  (Not supported in Dask)\n",
      " |          The timestamp on which to adjust the grouping. The timezone of origin\n",
      " |          must match the timezone of the index.\n",
      " |          If string, must be one of the following:\n",
      " |      \n",
      " |          - 'epoch': `origin` is 1970-01-01\n",
      " |          - 'start': `origin` is the first value of the timeseries\n",
      " |          - 'start_day': `origin` is the first day at midnight of the timeseries\n",
      " |      \n",
      " |          .. versionadded:: 1.1.0\n",
      " |      \n",
      " |          - 'end': `origin` is the last value of the timeseries\n",
      " |          - 'end_day': `origin` is the ceiling midnight of the last day\n",
      " |      \n",
      " |          .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      offset : Timedelta or str, default is None  (Not supported in Dask)\n",
      " |          An offset timedelta added to the origin.\n",
      " |      \n",
      " |          .. versionadded:: 1.1.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      pandas.core.Resampler\n",
      " |          :class:`~pandas.core.Resampler` object.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.resample : Resample a Series.\n",
      " |      DataFrame.resample : Resample a DataFrame.\n",
      " |      groupby : Group DataFrame by mapping, function, label, or list of labels.\n",
      " |      asfreq : Reindex a DataFrame with the given frequency without grouping.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      See the `user guide\n",
      " |      <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#resampling>`__\n",
      " |      for more.\n",
      " |      \n",
      " |      To learn more about the offset strings, please see `this link\n",
      " |      <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects>`__.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Start by creating a series with 9 one minute timestamps.\n",
      " |      \n",
      " |      >>> index = pd.date_range('1/1/2000', periods=9, freq='T')  # doctest: +SKIP\n",
      " |      >>> series = pd.Series(range(9), index=index)  # doctest: +SKIP\n",
      " |      >>> series  # doctest: +SKIP\n",
      " |      2000-01-01 00:00:00    0\n",
      " |      2000-01-01 00:01:00    1\n",
      " |      2000-01-01 00:02:00    2\n",
      " |      2000-01-01 00:03:00    3\n",
      " |      2000-01-01 00:04:00    4\n",
      " |      2000-01-01 00:05:00    5\n",
      " |      2000-01-01 00:06:00    6\n",
      " |      2000-01-01 00:07:00    7\n",
      " |      2000-01-01 00:08:00    8\n",
      " |      Freq: T, dtype: int64\n",
      " |      \n",
      " |      Downsample the series into 3 minute bins and sum the values\n",
      " |      of the timestamps falling into a bin.\n",
      " |      \n",
      " |      >>> series.resample('3T').sum()  # doctest: +SKIP\n",
      " |      2000-01-01 00:00:00     3\n",
      " |      2000-01-01 00:03:00    12\n",
      " |      2000-01-01 00:06:00    21\n",
      " |      Freq: 3T, dtype: int64\n",
      " |      \n",
      " |      Downsample the series into 3 minute bins as above, but label each\n",
      " |      bin using the right edge instead of the left. Please note that the\n",
      " |      value in the bucket used as the label is not included in the bucket,\n",
      " |      which it labels. For example, in the original series the\n",
      " |      bucket ``2000-01-01 00:03:00`` contains the value 3, but the summed\n",
      " |      value in the resampled bucket with the label ``2000-01-01 00:03:00``\n",
      " |      does not include 3 (if it did, the summed value would be 6, not 3).\n",
      " |      To include this value close the right side of the bin interval as\n",
      " |      illustrated in the example below this one.\n",
      " |      \n",
      " |      >>> series.resample('3T', label='right').sum()  # doctest: +SKIP\n",
      " |      2000-01-01 00:03:00     3\n",
      " |      2000-01-01 00:06:00    12\n",
      " |      2000-01-01 00:09:00    21\n",
      " |      Freq: 3T, dtype: int64\n",
      " |      \n",
      " |      Downsample the series into 3 minute bins as above, but close the right\n",
      " |      side of the bin interval.\n",
      " |      \n",
      " |      >>> series.resample('3T', label='right', closed='right').sum()  # doctest: +SKIP\n",
      " |      2000-01-01 00:00:00     0\n",
      " |      2000-01-01 00:03:00     6\n",
      " |      2000-01-01 00:06:00    15\n",
      " |      2000-01-01 00:09:00    15\n",
      " |      Freq: 3T, dtype: int64\n",
      " |      \n",
      " |      Upsample the series into 30 second bins.\n",
      " |      \n",
      " |      >>> series.resample('30S').asfreq()[0:5]   # Select first 5 rows  # doctest: +SKIP\n",
      " |      2000-01-01 00:00:00   0.0\n",
      " |      2000-01-01 00:00:30   NaN\n",
      " |      2000-01-01 00:01:00   1.0\n",
      " |      2000-01-01 00:01:30   NaN\n",
      " |      2000-01-01 00:02:00   2.0\n",
      " |      Freq: 30S, dtype: float64\n",
      " |      \n",
      " |      Upsample the series into 30 second bins and fill the ``NaN``\n",
      " |      values using the ``pad`` method.\n",
      " |      \n",
      " |      >>> series.resample('30S').pad()[0:5]  # doctest: +SKIP\n",
      " |      2000-01-01 00:00:00    0\n",
      " |      2000-01-01 00:00:30    0\n",
      " |      2000-01-01 00:01:00    1\n",
      " |      2000-01-01 00:01:30    1\n",
      " |      2000-01-01 00:02:00    2\n",
      " |      Freq: 30S, dtype: int64\n",
      " |      \n",
      " |      Upsample the series into 30 second bins and fill the\n",
      " |      ``NaN`` values using the ``bfill`` method.\n",
      " |      \n",
      " |      >>> series.resample('30S').bfill()[0:5]  # doctest: +SKIP\n",
      " |      2000-01-01 00:00:00    0\n",
      " |      2000-01-01 00:00:30    1\n",
      " |      2000-01-01 00:01:00    1\n",
      " |      2000-01-01 00:01:30    2\n",
      " |      2000-01-01 00:02:00    2\n",
      " |      Freq: 30S, dtype: int64\n",
      " |      \n",
      " |      Pass a custom function via ``apply``\n",
      " |      \n",
      " |      >>> def custom_resampler(arraylike):  # doctest: +SKIP\n",
      " |      ...     return np.sum(arraylike) + 5\n",
      " |      ...\n",
      " |      >>> series.resample('3T').apply(custom_resampler)  # doctest: +SKIP\n",
      " |      2000-01-01 00:00:00     8\n",
      " |      2000-01-01 00:03:00    17\n",
      " |      2000-01-01 00:06:00    26\n",
      " |      Freq: 3T, dtype: int64\n",
      " |      \n",
      " |      For a Series with a PeriodIndex, the keyword `convention` can be\n",
      " |      used to control whether to use the start or end of `rule`.\n",
      " |      \n",
      " |      Resample a year by quarter using 'start' `convention`. Values are\n",
      " |      assigned to the first quarter of the period.\n",
      " |      \n",
      " |      >>> s = pd.Series([1, 2], index=pd.period_range('2012-01-01',  # doctest: +SKIP\n",
      " |      ...                                             freq='A',\n",
      " |      ...                                             periods=2))\n",
      " |      >>> s  # doctest: +SKIP\n",
      " |      2012    1\n",
      " |      2013    2\n",
      " |      Freq: A-DEC, dtype: int64\n",
      " |      >>> s.resample('Q', convention='start').asfreq()  # doctest: +SKIP\n",
      " |      2012Q1    1.0\n",
      " |      2012Q2    NaN\n",
      " |      2012Q3    NaN\n",
      " |      2012Q4    NaN\n",
      " |      2013Q1    2.0\n",
      " |      2013Q2    NaN\n",
      " |      2013Q3    NaN\n",
      " |      2013Q4    NaN\n",
      " |      Freq: Q-DEC, dtype: float64\n",
      " |      \n",
      " |      Resample quarters by month using 'end' `convention`. Values are\n",
      " |      assigned to the last month of the period.\n",
      " |      \n",
      " |      >>> q = pd.Series([1, 2, 3, 4], index=pd.period_range('2018-01-01',  # doctest: +SKIP\n",
      " |      ...                                                   freq='Q',\n",
      " |      ...                                                   periods=4))\n",
      " |      >>> q  # doctest: +SKIP\n",
      " |      2018Q1    1\n",
      " |      2018Q2    2\n",
      " |      2018Q3    3\n",
      " |      2018Q4    4\n",
      " |      Freq: Q-DEC, dtype: int64\n",
      " |      >>> q.resample('M', convention='end').asfreq()  # doctest: +SKIP\n",
      " |      2018-03    1.0\n",
      " |      2018-04    NaN\n",
      " |      2018-05    NaN\n",
      " |      2018-06    2.0\n",
      " |      2018-07    NaN\n",
      " |      2018-08    NaN\n",
      " |      2018-09    3.0\n",
      " |      2018-10    NaN\n",
      " |      2018-11    NaN\n",
      " |      2018-12    4.0\n",
      " |      Freq: M, dtype: float64\n",
      " |      \n",
      " |      For DataFrame objects, the keyword `on` can be used to specify the\n",
      " |      column instead of the index for resampling.\n",
      " |      \n",
      " |      >>> d = {'price': [10, 11, 9, 13, 14, 18, 17, 19],  # doctest: +SKIP\n",
      " |      ...      'volume': [50, 60, 40, 100, 50, 100, 40, 50]}\n",
      " |      >>> df = pd.DataFrame(d)  # doctest: +SKIP\n",
      " |      >>> df['week_starting'] = pd.date_range('01/01/2018',  # doctest: +SKIP\n",
      " |      ...                                     periods=8,\n",
      " |      ...                                     freq='W')\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |         price  volume week_starting\n",
      " |      0     10      50    2018-01-07\n",
      " |      1     11      60    2018-01-14\n",
      " |      2      9      40    2018-01-21\n",
      " |      3     13     100    2018-01-28\n",
      " |      4     14      50    2018-02-04\n",
      " |      5     18     100    2018-02-11\n",
      " |      6     17      40    2018-02-18\n",
      " |      7     19      50    2018-02-25\n",
      " |      >>> df.resample('M', on='week_starting').mean()  # doctest: +SKIP\n",
      " |                     price  volume\n",
      " |      week_starting\n",
      " |      2018-01-31     10.75    62.5\n",
      " |      2018-02-28     17.00    60.0\n",
      " |      \n",
      " |      For a DataFrame with MultiIndex, the keyword `level` can be used to\n",
      " |      specify on which level the resampling needs to take place.\n",
      " |      \n",
      " |      >>> days = pd.date_range('1/1/2000', periods=4, freq='D')  # doctest: +SKIP\n",
      " |      >>> d2 = {'price': [10, 11, 9, 13, 14, 18, 17, 19],  # doctest: +SKIP\n",
      " |      ...       'volume': [50, 60, 40, 100, 50, 100, 40, 50]}\n",
      " |      >>> df2 = pd.DataFrame(  # doctest: +SKIP\n",
      " |      ...     d2,\n",
      " |      ...     index=pd.MultiIndex.from_product(\n",
      " |      ...         [days, ['morning', 'afternoon']]\n",
      " |      ...     )\n",
      " |      ... )\n",
      " |      >>> df2  # doctest: +SKIP\n",
      " |                            price  volume\n",
      " |      2000-01-01 morning       10      50\n",
      " |                 afternoon     11      60\n",
      " |      2000-01-02 morning        9      40\n",
      " |                 afternoon     13     100\n",
      " |      2000-01-03 morning       14      50\n",
      " |                 afternoon     18     100\n",
      " |      2000-01-04 morning       17      40\n",
      " |                 afternoon     19      50\n",
      " |      >>> df2.resample('D', level=0).sum()  # doctest: +SKIP\n",
      " |                  price  volume\n",
      " |      2000-01-01     21     110\n",
      " |      2000-01-02     22     140\n",
      " |      2000-01-03     32     150\n",
      " |      2000-01-04     36      90\n",
      " |      \n",
      " |      If you want to adjust the start of the bins based on a fixed timestamp:\n",
      " |      \n",
      " |      >>> start, end = '2000-10-01 23:30:00', '2000-10-02 00:30:00'  # doctest: +SKIP\n",
      " |      >>> rng = pd.date_range(start, end, freq='7min')  # doctest: +SKIP\n",
      " |      >>> ts = pd.Series(np.arange(len(rng)) * 3, index=rng)  # doctest: +SKIP\n",
      " |      >>> ts  # doctest: +SKIP\n",
      " |      2000-10-01 23:30:00     0\n",
      " |      2000-10-01 23:37:00     3\n",
      " |      2000-10-01 23:44:00     6\n",
      " |      2000-10-01 23:51:00     9\n",
      " |      2000-10-01 23:58:00    12\n",
      " |      2000-10-02 00:05:00    15\n",
      " |      2000-10-02 00:12:00    18\n",
      " |      2000-10-02 00:19:00    21\n",
      " |      2000-10-02 00:26:00    24\n",
      " |      Freq: 7T, dtype: int64\n",
      " |      \n",
      " |      >>> ts.resample('17min').sum()  # doctest: +SKIP\n",
      " |      2000-10-01 23:14:00     0\n",
      " |      2000-10-01 23:31:00     9\n",
      " |      2000-10-01 23:48:00    21\n",
      " |      2000-10-02 00:05:00    54\n",
      " |      2000-10-02 00:22:00    24\n",
      " |      Freq: 17T, dtype: int64\n",
      " |      \n",
      " |      >>> ts.resample('17min', origin='epoch').sum()  # doctest: +SKIP\n",
      " |      2000-10-01 23:18:00     0\n",
      " |      2000-10-01 23:35:00    18\n",
      " |      2000-10-01 23:52:00    27\n",
      " |      2000-10-02 00:09:00    39\n",
      " |      2000-10-02 00:26:00    24\n",
      " |      Freq: 17T, dtype: int64\n",
      " |      \n",
      " |      >>> ts.resample('17min', origin='2000-01-01').sum()  # doctest: +SKIP\n",
      " |      2000-10-01 23:24:00     3\n",
      " |      2000-10-01 23:41:00    15\n",
      " |      2000-10-01 23:58:00    45\n",
      " |      2000-10-02 00:15:00    45\n",
      " |      Freq: 17T, dtype: int64\n",
      " |      \n",
      " |      If you want to adjust the start of the bins with an `offset` Timedelta, the two\n",
      " |      following lines are equivalent:\n",
      " |      \n",
      " |      >>> ts.resample('17min', origin='start').sum()  # doctest: +SKIP\n",
      " |      2000-10-01 23:30:00     9\n",
      " |      2000-10-01 23:47:00    21\n",
      " |      2000-10-02 00:04:00    54\n",
      " |      2000-10-02 00:21:00    24\n",
      " |      Freq: 17T, dtype: int64\n",
      " |      \n",
      " |      >>> ts.resample('17min', offset='23h30min').sum()  # doctest: +SKIP\n",
      " |      2000-10-01 23:30:00     9\n",
      " |      2000-10-01 23:47:00    21\n",
      " |      2000-10-02 00:04:00    54\n",
      " |      2000-10-02 00:21:00    24\n",
      " |      Freq: 17T, dtype: int64\n",
      " |      \n",
      " |      If you want to take the largest Timestamp as the end of the bins:\n",
      " |      \n",
      " |      >>> ts.resample('17min', origin='end').sum()  # doctest: +SKIP\n",
      " |      2000-10-01 23:35:00     0\n",
      " |      2000-10-01 23:52:00    18\n",
      " |      2000-10-02 00:09:00    27\n",
      " |      2000-10-02 00:26:00    63\n",
      " |      Freq: 17T, dtype: int64\n",
      " |      \n",
      " |      In contrast with the `start_day`, you can use `end_day` to take the ceiling\n",
      " |      midnight of the largest Timestamp as the end of the bins and drop the bins\n",
      " |      not containing data:\n",
      " |      \n",
      " |      >>> ts.resample('17min', origin='end_day').sum()  # doctest: +SKIP\n",
      " |      2000-10-01 23:38:00     3\n",
      " |      2000-10-01 23:55:00    15\n",
      " |      2000-10-02 00:12:00    45\n",
      " |      2000-10-02 00:29:00    45\n",
      " |      Freq: 17T, dtype: int64\n",
      " |      \n",
      " |      To replace the use of the deprecated `base` argument, you can now use `offset`,\n",
      " |      in this example it is equivalent to have `base=2`:\n",
      " |      \n",
      " |      >>> ts.resample('17min', offset='2min').sum()  # doctest: +SKIP\n",
      " |      2000-10-01 23:16:00     0\n",
      " |      2000-10-01 23:33:00     9\n",
      " |      2000-10-01 23:50:00    36\n",
      " |      2000-10-02 00:07:00    39\n",
      " |      2000-10-02 00:24:00    24\n",
      " |      Freq: 17T, dtype: int64\n",
      " |      \n",
      " |      To replace the use of the deprecated `loffset` argument:\n",
      " |      \n",
      " |      >>> from pandas.tseries.frequencies import to_offset  # doctest: +SKIP\n",
      " |      >>> loffset = '19min'  # doctest: +SKIP\n",
      " |      >>> ts_out = ts.resample('17min').sum()  # doctest: +SKIP\n",
      " |      >>> ts_out.index = ts_out.index + to_offset(loffset)  # doctest: +SKIP\n",
      " |      >>> ts_out  # doctest: +SKIP\n",
      " |      2000-10-01 23:33:00     0\n",
      " |      2000-10-01 23:50:00     9\n",
      " |      2000-10-02 00:07:00    21\n",
      " |      2000-10-02 00:24:00    54\n",
      " |      2000-10-02 00:41:00    24\n",
      " |      Freq: 17T, dtype: int64\n",
      " |  \n",
      " |  reset_index(self, drop=False)\n",
      " |      Reset the index to the default index.\n",
      " |      \n",
      " |      Note that unlike in ``pandas``, the reset ``dask.dataframe`` index will\n",
      " |      not be monotonically increasing from 0. Instead, it will restart at 0\n",
      " |      for each partition (e.g. ``index1 = [0, ..., 10], index2 = [0, ...]``).\n",
      " |      This is due to the inability to statically know the full length of the\n",
      " |      index.\n",
      " |      \n",
      " |      For DataFrame with multi-level index, returns a new DataFrame with\n",
      " |      labeling information in the columns under the index names, defaulting\n",
      " |      to 'level_0', 'level_1', etc. if any are None. For a standard index,\n",
      " |      the index name will be used (if set), otherwise a default 'index' or\n",
      " |      'level_0' (if 'index' is already taken) will be used.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      drop : boolean, default False\n",
      " |          Do not try to insert index into dataframe columns.\n",
      " |  \n",
      " |  rolling(self, window, min_periods=None, center=False, win_type=None, axis=0)\n",
      " |      Provides rolling transformations.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      window : int, str, offset\n",
      " |         Size of the moving window. This is the number of observations used\n",
      " |         for calculating the statistic. When not using a ``DatetimeIndex``,\n",
      " |         the window size must not be so large as to span more than one\n",
      " |         adjacent partition. If using an offset or offset alias like '5D',\n",
      " |         the data must have a ``DatetimeIndex``\n",
      " |      \n",
      " |         .. versionchanged:: 0.15.0\n",
      " |      \n",
      " |            Now accepts offsets and string offset aliases\n",
      " |      \n",
      " |      min_periods : int, default None\n",
      " |          Minimum number of observations in window required to have a value\n",
      " |          (otherwise result is NA).\n",
      " |      center : boolean, default False\n",
      " |          Set the labels at the center of the window.\n",
      " |      win_type : string, default None\n",
      " |          Provide a window type. The recognized window types are identical\n",
      " |          to pandas.\n",
      " |      axis : int, default 0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      a Rolling object on which to call a method to compute a statistic\n",
      " |  \n",
      " |  sample(self, n=None, frac=None, replace=False, random_state=None)\n",
      " |      Random sample of items\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : int, optional\n",
      " |          Number of items to return is not supported by dask. Use frac\n",
      " |          instead.\n",
      " |      frac : float, optional\n",
      " |          Approximate fraction of items to return. This sampling fraction is\n",
      " |          applied to all partitions equally. Note that this is an\n",
      " |          **approximate fraction**. You should not expect exactly ``len(df) * frac``\n",
      " |          items to be returned, as the exact number of elements selected will\n",
      " |          depend on how your data is partitioned (but should be pretty close\n",
      " |          in practice).\n",
      " |      replace : boolean, optional\n",
      " |          Sample with or without replacement. Default = False.\n",
      " |      random_state : int or ``np.random.RandomState``\n",
      " |          If an int, we create a new RandomState with this as the seed;\n",
      " |          Otherwise we draw from the passed RandomState.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.random_split\n",
      " |      pandas.DataFrame.sample\n",
      " |  \n",
      " |  sem(self, axis=None, skipna=True, ddof=1, split_every=False, numeric_only=None)\n",
      " |      Return unbiased standard error of the mean over requested axis.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.sem.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Normalized by N-1 by default. This can be changed using the ddof argument\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {index (0), columns (1)}\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values. If an entire row/column is NA, the result\n",
      " |          will be NA.\n",
      " |      level : int or level name, default None  (Not supported in Dask)\n",
      " |          If the axis is a MultiIndex (hierarchical), count along a\n",
      " |          particular level, collapsing into a Series.\n",
      " |      ddof : int, default 1\n",
      " |          Delta Degrees of Freedom. The divisor used in calculations is N - ddof,\n",
      " |          where N represents the number of elements.\n",
      " |      numeric_only : bool, default None\n",
      " |          Include only float, int, boolean columns. If None, will attempt to use\n",
      " |          everything, then use only numeric data. Not implemented for Series.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame (if level specified)\n",
      " |  \n",
      " |  shift(self, periods=1, freq=None, axis=0)\n",
      " |      Shift index by desired number of periods with an optional time `freq`.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.shift.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      When `freq` is not passed, shift the index without realigning the data.\n",
      " |      If `freq` is passed (in this case, the index must be date or datetime,\n",
      " |      or it will raise a `NotImplementedError`), the index will be\n",
      " |      increased using the periods and the `freq`. `freq` can be inferred\n",
      " |      when specified as \"infer\" as long as either freq or inferred_freq\n",
      " |      attribute is set in the index.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      periods : int\n",
      " |          Number of periods to shift. Can be positive or negative.\n",
      " |      freq : DateOffset, tseries.offsets, timedelta, or str, optional\n",
      " |          Offset to use from the tseries module or time rule (e.g. 'EOM').\n",
      " |          If `freq` is specified then the index values are shifted but the\n",
      " |          data is not realigned. That is, use `freq` if you would like to\n",
      " |          extend the index when shifting and preserve the original data.\n",
      " |          If `freq` is specified as \"infer\" then it will be inferred from\n",
      " |          the freq or inferred_freq attributes of the index. If neither of\n",
      " |          those attributes exist, a ValueError is thrown.\n",
      " |      axis : {0 or 'index', 1 or 'columns', None}, default None\n",
      " |          Shift direction.\n",
      " |      fill_value : object, optional  (Not supported in Dask)\n",
      " |          The scalar value to use for newly introduced missing values.\n",
      " |          the default depends on the dtype of `self`.\n",
      " |          For numeric data, ``np.nan`` is used.\n",
      " |          For datetime, timedelta, or period data, etc. :attr:`NaT` is used.\n",
      " |          For extension dtypes, ``self.dtype.na_value`` is used.\n",
      " |      \n",
      " |          .. versionchanged:: 1.1.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataFrame\n",
      " |          Copy of input object, shifted.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Index.shift : Shift values of Index.\n",
      " |      DatetimeIndex.shift : Shift values of DatetimeIndex.\n",
      " |      PeriodIndex.shift : Shift values of PeriodIndex.\n",
      " |      tshift : Shift the time index, using the index's frequency if\n",
      " |          available.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({\"Col1\": [10, 20, 15, 30, 45],  # doctest: +SKIP\n",
      " |      ...                    \"Col2\": [13, 23, 18, 33, 48],\n",
      " |      ...                    \"Col3\": [17, 27, 22, 37, 52]},\n",
      " |      ...                   index=pd.date_range(\"2020-01-01\", \"2020-01-05\"))\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |                  Col1  Col2  Col3\n",
      " |      2020-01-01    10    13    17\n",
      " |      2020-01-02    20    23    27\n",
      " |      2020-01-03    15    18    22\n",
      " |      2020-01-04    30    33    37\n",
      " |      2020-01-05    45    48    52\n",
      " |      \n",
      " |      >>> df.shift(periods=3)  # doctest: +SKIP\n",
      " |                  Col1  Col2  Col3\n",
      " |      2020-01-01   NaN   NaN   NaN\n",
      " |      2020-01-02   NaN   NaN   NaN\n",
      " |      2020-01-03   NaN   NaN   NaN\n",
      " |      2020-01-04  10.0  13.0  17.0\n",
      " |      2020-01-05  20.0  23.0  27.0\n",
      " |      \n",
      " |      >>> df.shift(periods=1, axis=\"columns\")  # doctest: +SKIP\n",
      " |                  Col1  Col2  Col3\n",
      " |      2020-01-01   NaN    10    13\n",
      " |      2020-01-02   NaN    20    23\n",
      " |      2020-01-03   NaN    15    18\n",
      " |      2020-01-04   NaN    30    33\n",
      " |      2020-01-05   NaN    45    48\n",
      " |      \n",
      " |      >>> df.shift(periods=3, fill_value=0)  # doctest: +SKIP\n",
      " |                  Col1  Col2  Col3\n",
      " |      2020-01-01     0     0     0\n",
      " |      2020-01-02     0     0     0\n",
      " |      2020-01-03     0     0     0\n",
      " |      2020-01-04    10    13    17\n",
      " |      2020-01-05    20    23    27\n",
      " |      \n",
      " |      >>> df.shift(periods=3, freq=\"D\")  # doctest: +SKIP\n",
      " |                  Col1  Col2  Col3\n",
      " |      2020-01-04    10    13    17\n",
      " |      2020-01-05    20    23    27\n",
      " |      2020-01-06    15    18    22\n",
      " |      2020-01-07    30    33    37\n",
      " |      2020-01-08    45    48    52\n",
      " |      \n",
      " |      >>> df.shift(periods=3, freq=\"infer\")  # doctest: +SKIP\n",
      " |                  Col1  Col2  Col3\n",
      " |      2020-01-04    10    13    17\n",
      " |      2020-01-05    20    23    27\n",
      " |      2020-01-06    15    18    22\n",
      " |      2020-01-07    30    33    37\n",
      " |      2020-01-08    45    48    52\n",
      " |  \n",
      " |  shuffle(self, on, npartitions=None, max_branch=None, shuffle=None, ignore_index=False, compute=None)\n",
      " |      Rearrange DataFrame into new partitions\n",
      " |      \n",
      " |      Uses hashing of `on` to map rows to output partitions. After this\n",
      " |      operation, rows with the same value of `on` will be in the same\n",
      " |      partition.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      on : str, list of str, or Series, Index, or DataFrame\n",
      " |          Column(s) or index to be used to map rows to output partitions\n",
      " |      npartitions : int, optional\n",
      " |          Number of partitions of output. Partition count will not be\n",
      " |          changed by default.\n",
      " |      max_branch: int, optional\n",
      " |          The maximum number of splits per input partition. Used within\n",
      " |          the staged shuffling algorithm.\n",
      " |      shuffle: {'disk', 'tasks'}, optional\n",
      " |          Either ``'disk'`` for single-node operation or ``'tasks'`` for\n",
      " |          distributed operation.  Will be inferred by your current scheduler.\n",
      " |      ignore_index: bool, default False\n",
      " |          Ignore index during shuffle.  If ``True``, performance may improve,\n",
      " |          but index values will not be preserved.\n",
      " |      compute: bool\n",
      " |          Whether or not to trigger an immediate computation. Defaults to False.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This does not preserve a meaningful index/partitioning scheme. This\n",
      " |      is not deterministic if done in parallel.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = df.shuffle(df.columns[0])  # doctest: +SKIP\n",
      " |  \n",
      " |  skew(self, axis=None, bias=True, nan_policy='propagate', out=None, numeric_only=None)\n",
      " |      Return unbiased skew over requested axis.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.skew.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      \n",
      " |              .. note::\n",
      " |      \n",
      " |                 This implementation follows the dask.array.stats implementation\n",
      " |                 of skewness and calculates skewness without taking into account\n",
      " |                 a bias term for finite sample size, which corresponds to the\n",
      " |                 default settings of the scipy.stats skewness calculation. However,\n",
      " |                 Pandas corrects for this, so the values differ by a factor of\n",
      " |                 (n * (n - 1)) ** 0.5 / (n - 2), where n is the number of samples.\n",
      " |      \n",
      " |                 Further, this method currently does not support filtering out NaN\n",
      " |                 values, which is again a difference to Pandas.\n",
      " |              \n",
      " |      \n",
      " |      Normalized by N-1.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {index (0), columns (1)}\n",
      " |          Axis for the function to be applied on.\n",
      " |      skipna : bool, default True  (Not supported in Dask)\n",
      " |          Exclude NA/null values when computing the result.\n",
      " |      level : int or level name, default None  (Not supported in Dask)\n",
      " |          If the axis is a MultiIndex (hierarchical), count along a\n",
      " |          particular level, collapsing into a Series.\n",
      " |      numeric_only : bool, default None\n",
      " |          Include only float, int, boolean columns. If None, will attempt to use\n",
      " |          everything, then use only numeric data. Not implemented for Series.\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments to be passed to the function.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame (if level specified)\n",
      " |  \n",
      " |  std(self, axis=None, skipna=True, ddof=1, split_every=False, dtype=None, out=None, numeric_only=None)\n",
      " |      Return sample standard deviation over requested axis.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.std.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Normalized by N-1 by default. This can be changed using the ddof argument.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {index (0), columns (1)}\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values. If an entire row/column is NA, the result\n",
      " |          will be NA.\n",
      " |      level : int or level name, default None  (Not supported in Dask)\n",
      " |          If the axis is a MultiIndex (hierarchical), count along a\n",
      " |          particular level, collapsing into a Series.\n",
      " |      ddof : int, default 1\n",
      " |          Delta Degrees of Freedom. The divisor used in calculations is N - ddof,\n",
      " |          where N represents the number of elements.\n",
      " |      numeric_only : bool, default None\n",
      " |          Include only float, int, boolean columns. If None, will attempt to use\n",
      " |          everything, then use only numeric data. Not implemented for Series.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame (if level specified) \n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      To have the same behaviour as `numpy.std`, use `ddof=0` (instead of the\n",
      " |      default `ddof=1`)\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'person_id': [0, 1, 2, 3],  # doctest: +SKIP\n",
      " |      ...                   'age': [21, 25, 62, 43],\n",
      " |      ...                   'height': [1.61, 1.87, 1.49, 2.01]}\n",
      " |      ...                  ).set_index('person_id')\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |                 age  height\n",
      " |      person_id\n",
      " |      0           21    1.61\n",
      " |      1           25    1.87\n",
      " |      2           62    1.49\n",
      " |      3           43    2.01\n",
      " |      \n",
      " |      The standard deviation of the columns can be found as follows:\n",
      " |      \n",
      " |      >>> df.std()  # doctest: +SKIP\n",
      " |      age       18.786076\n",
      " |      height     0.237417\n",
      " |      \n",
      " |      Alternatively, `ddof=0` can be set to normalize by N instead of N-1:\n",
      " |      \n",
      " |      >>> df.std(ddof=0)  # doctest: +SKIP\n",
      " |      age       16.269219\n",
      " |      height     0.205609\n",
      " |  \n",
      " |  sum(self, axis=None, skipna=True, split_every=False, dtype=None, out=None, min_count=None, numeric_only=None)\n",
      " |      Return the sum of the values over the requested axis.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.sum.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      This is equivalent to the method ``numpy.sum``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {index (0), columns (1)}\n",
      " |          Axis for the function to be applied on.\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values when computing the result.\n",
      " |      level : int or level name, default None  (Not supported in Dask)\n",
      " |          If the axis is a MultiIndex (hierarchical), count along a\n",
      " |          particular level, collapsing into a Series.\n",
      " |      numeric_only : bool, default None\n",
      " |          Include only float, int, boolean columns. If None, will attempt to use\n",
      " |          everything, then use only numeric data. Not implemented for Series.\n",
      " |      min_count : int, default 0\n",
      " |          The required number of valid values to perform the operation. If fewer than\n",
      " |          ``min_count`` non-NA values are present the result will be NA.\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments to be passed to the function.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame (if level specified)\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Series.sum : Return the sum.\n",
      " |      Series.min : Return the minimum.\n",
      " |      Series.max : Return the maximum.\n",
      " |      Series.idxmin : Return the index of the minimum.\n",
      " |      Series.idxmax : Return the index of the maximum.\n",
      " |      DataFrame.sum : Return the sum over the requested axis.\n",
      " |      DataFrame.min : Return the minimum over the requested axis.\n",
      " |      DataFrame.max : Return the maximum over the requested axis.\n",
      " |      DataFrame.idxmin : Return the index of the minimum over the requested axis.\n",
      " |      DataFrame.idxmax : Return the index of the maximum over the requested axis.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> idx = pd.MultiIndex.from_arrays([  # doctest: +SKIP\n",
      " |      ...     ['warm', 'warm', 'cold', 'cold'],\n",
      " |      ...     ['dog', 'falcon', 'fish', 'spider']],\n",
      " |      ...     names=['blooded', 'animal'])\n",
      " |      >>> s = pd.Series([4, 2, 0, 8], name='legs', index=idx)  # doctest: +SKIP\n",
      " |      >>> s  # doctest: +SKIP\n",
      " |      blooded  animal\n",
      " |      warm     dog       4\n",
      " |               falcon    2\n",
      " |      cold     fish      0\n",
      " |               spider    8\n",
      " |      Name: legs, dtype: int64\n",
      " |      \n",
      " |      >>> s.sum()  # doctest: +SKIP\n",
      " |      14\n",
      " |      \n",
      " |      By default, the sum of an empty or all-NA Series is ``0``.\n",
      " |      \n",
      " |      >>> pd.Series([], dtype=\"float64\").sum()  # min_count=0 is the default  # doctest: +SKIP\n",
      " |      0.0\n",
      " |      \n",
      " |      This can be controlled with the ``min_count`` parameter. For example, if\n",
      " |      you'd like the sum of an empty series to be NaN, pass ``min_count=1``.\n",
      " |      \n",
      " |      >>> pd.Series([], dtype=\"float64\").sum(min_count=1)  # doctest: +SKIP\n",
      " |      nan\n",
      " |      \n",
      " |      Thanks to the ``skipna`` parameter, ``min_count`` handles all-NA and\n",
      " |      empty series identically.\n",
      " |      \n",
      " |      >>> pd.Series([np.nan]).sum()  # doctest: +SKIP\n",
      " |      0.0\n",
      " |      \n",
      " |      >>> pd.Series([np.nan]).sum(min_count=1)  # doctest: +SKIP\n",
      " |      nan\n",
      " |  \n",
      " |  tail(self, n=5, compute=True)\n",
      " |      Last n rows of the dataset\n",
      " |      \n",
      " |      Caveat, the only checks the last n rows of the last partition.\n",
      " |  \n",
      " |  to_csv(self, filename, **kwargs)\n",
      " |      Store Dask DataFrame to CSV files\n",
      " |      \n",
      " |      One filename per partition will be created. You can specify the\n",
      " |      filenames in a variety of ways.\n",
      " |      \n",
      " |      Use a globstring::\n",
      " |      \n",
      " |      >>> df.to_csv('/path/to/data/export-*.csv')  # doctest: +SKIP\n",
      " |      \n",
      " |      The * will be replaced by the increasing sequence 0, 1, 2, ...\n",
      " |      \n",
      " |      ::\n",
      " |      \n",
      " |          /path/to/data/export-0.csv\n",
      " |          /path/to/data/export-1.csv\n",
      " |      \n",
      " |      Use a globstring and a ``name_function=`` keyword argument.  The\n",
      " |      name_function function should expect an integer and produce a string.\n",
      " |      Strings produced by name_function must preserve the order of their\n",
      " |      respective partition indices.\n",
      " |      \n",
      " |      >>> from datetime import date, timedelta\n",
      " |      >>> def name(i):\n",
      " |      ...     return str(date(2015, 1, 1) + i * timedelta(days=1))\n",
      " |      \n",
      " |      >>> name(0)\n",
      " |      '2015-01-01'\n",
      " |      >>> name(15)\n",
      " |      '2015-01-16'\n",
      " |      \n",
      " |      >>> df.to_csv('/path/to/data/export-*.csv', name_function=name)  # doctest: +SKIP\n",
      " |      \n",
      " |      ::\n",
      " |      \n",
      " |          /path/to/data/export-2015-01-01.csv\n",
      " |          /path/to/data/export-2015-01-02.csv\n",
      " |          ...\n",
      " |      \n",
      " |      You can also provide an explicit list of paths::\n",
      " |      \n",
      " |      >>> paths = ['/path/to/data/alice.csv', '/path/to/data/bob.csv', ...]  # doctest: +SKIP\n",
      " |      >>> df.to_csv(paths) # doctest: +SKIP\n",
      " |      \n",
      " |      You can also provide a directory name:\n",
      " |      \n",
      " |      >>> df.to_csv('/path/to/data') # doctest: +SKIP\n",
      " |      \n",
      " |      The files will be numbered 0, 1, 2, (and so on) suffixed with '.part':\n",
      " |      \n",
      " |      ::\n",
      " |      \n",
      " |          /path/to/data/0.part\n",
      " |          /path/to/data/1.part\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      df : dask.DataFrame\n",
      " |          Data to save\n",
      " |      filename : string or list\n",
      " |          Absolute or relative filepath(s). Prefix with a protocol like ``s3://``\n",
      " |          to save to remote filesystems.\n",
      " |      single_file : bool, default False\n",
      " |          Whether to save everything into a single CSV file. Under the\n",
      " |          single file mode, each partition is appended at the end of the\n",
      " |          specified CSV file.\n",
      " |      encoding : string, default 'utf-8'\n",
      " |          A string representing the encoding to use in the output file.\n",
      " |      mode : str, default 'w'\n",
      " |          Python file mode. The default is 'w' (or 'wt'), for writing\n",
      " |          a new file or overwriting an existing file in text mode. 'a'\n",
      " |          (or 'at') will append to an existing file in text mode or\n",
      " |          create a new file if it does not already exist. See :py:func:`open`.\n",
      " |      name_function : callable, default None\n",
      " |          Function accepting an integer (partition index) and producing a\n",
      " |          string to replace the asterisk in the given filename globstring.\n",
      " |          Should preserve the lexicographic order of partitions. Not\n",
      " |          supported when ``single_file`` is True.\n",
      " |      compression : string, optional\n",
      " |          A string representing the compression to use in the output file,\n",
      " |          allowed values are 'gzip', 'bz2', 'xz',\n",
      " |          only used when the first argument is a filename.\n",
      " |      compute : bool, default True\n",
      " |          If True, immediately executes. If False, returns a set of delayed\n",
      " |          objects, which can be computed at a later time.\n",
      " |      storage_options : dict\n",
      " |          Parameters passed on to the backend filesystem class.\n",
      " |      header_first_partition_only : bool, default None\n",
      " |          If set to True, only write the header row in the first output\n",
      " |          file. By default, headers are written to all partitions under\n",
      " |          the multiple file mode (``single_file`` is False) and written\n",
      " |          only once under the single file mode (``single_file`` is True).\n",
      " |          It must be True under the single file mode.\n",
      " |      compute_kwargs : dict, optional\n",
      " |          Options to be passed in to the compute method\n",
      " |      kwargs : dict, optional\n",
      " |          Additional parameters to pass to :meth:`pandas.DataFrame.to_csv`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      The names of the file written if they were computed right away.\n",
      " |      If not, the delayed tasks associated with writing the files.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ValueError\n",
      " |          If ``header_first_partition_only`` is set to False or\n",
      " |          ``name_function`` is specified when ``single_file`` is True.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      fsspec.open_files\n",
      " |  \n",
      " |  to_dask_array(self, lengths=None, meta=None)\n",
      " |      Convert a dask DataFrame to a dask array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      lengths : bool or Sequence of ints, optional\n",
      " |          How to determine the chunks sizes for the output array.\n",
      " |          By default, the output array will have unknown chunk lengths\n",
      " |          along the first axis, which can cause some later operations\n",
      " |          to fail.\n",
      " |      \n",
      " |          * True : immediately compute the length of each partition\n",
      " |          * Sequence : a sequence of integers to use for the chunk sizes\n",
      " |            on the first axis. These values are *not* validated for\n",
      " |            correctness, beyond ensuring that the number of items\n",
      " |            matches the number of partitions.\n",
      " |      meta : object, optional\n",
      " |          An optional `meta` parameter can be passed for dask to override the\n",
      " |          default metadata on the underlying dask array.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |  \n",
      " |  to_delayed(self, optimize_graph=True)\n",
      " |      Convert into a list of ``dask.delayed`` objects, one per partition.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      optimize_graph : bool, optional\n",
      " |          If True [default], the graph is optimized before converting into\n",
      " |          ``dask.delayed`` objects.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> partitions = df.to_delayed()  # doctest: +SKIP\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      dask.dataframe.from_delayed\n",
      " |  \n",
      " |  to_hdf(self, path_or_buf, key, mode='a', append=False, **kwargs)\n",
      " |      Store Dask Dataframe to Hierarchical Data Format (HDF) files\n",
      " |      \n",
      " |      This is a parallel version of the Pandas function of the same name.  Please\n",
      " |      see the Pandas docstring for more detailed information about shared keyword\n",
      " |      arguments.\n",
      " |      \n",
      " |      This function differs from the Pandas version by saving the many partitions\n",
      " |      of a Dask DataFrame in parallel, either to many files, or to many datasets\n",
      " |      within the same file.  You may specify this parallelism with an asterix\n",
      " |      ``*`` within the filename or datapath, and an optional ``name_function``.\n",
      " |      The asterix will be replaced with an increasing sequence of integers\n",
      " |      starting from ``0`` or with the result of calling ``name_function`` on each\n",
      " |      of those integers.\n",
      " |      \n",
      " |      This function only supports the Pandas ``'table'`` format, not the more\n",
      " |      specialized ``'fixed'`` format.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : string, pathlib.Path\n",
      " |          Path to a target filename. Supports strings, ``pathlib.Path``, or any\n",
      " |          object implementing the ``__fspath__`` protocol. May contain a ``*`` to\n",
      " |          denote many filenames.\n",
      " |      key : string\n",
      " |          Datapath within the files.  May contain a ``*`` to denote many locations\n",
      " |      name_function : function\n",
      " |          A function to convert the ``*`` in the above options to a string.\n",
      " |          Should take in a number from 0 to the number of partitions and return a\n",
      " |          string. (see examples below)\n",
      " |      compute : bool\n",
      " |          Whether or not to execute immediately.  If False then this returns a\n",
      " |          ``dask.Delayed`` value.\n",
      " |      lock : bool, Lock, optional\n",
      " |          Lock to use to prevent concurrency issues.  By default a\n",
      " |          ``threading.Lock``, ``multiprocessing.Lock`` or ``SerializableLock``\n",
      " |          will be used depending on your scheduler if a lock is required. See\n",
      " |          dask.utils.get_scheduler_lock for more information about lock\n",
      " |          selection.\n",
      " |      scheduler : string\n",
      " |          The scheduler to use, like \"threads\" or \"processes\"\n",
      " |      **other:\n",
      " |          See pandas.to_hdf for more information\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Save Data to a single file\n",
      " |      \n",
      " |      >>> df.to_hdf('output.hdf', '/data')            # doctest: +SKIP\n",
      " |      \n",
      " |      Save data to multiple datapaths within the same file:\n",
      " |      \n",
      " |      >>> df.to_hdf('output.hdf', '/data-*')          # doctest: +SKIP\n",
      " |      \n",
      " |      Save data to multiple files:\n",
      " |      \n",
      " |      >>> df.to_hdf('output-*.hdf', '/data')          # doctest: +SKIP\n",
      " |      \n",
      " |      Save data to multiple files, using the multiprocessing scheduler:\n",
      " |      \n",
      " |      >>> df.to_hdf('output-*.hdf', '/data', scheduler='processes') # doctest: +SKIP\n",
      " |      \n",
      " |      Specify custom naming scheme.  This writes files as\n",
      " |      '2000-01-01.hdf', '2000-01-02.hdf', '2000-01-03.hdf', etc..\n",
      " |      \n",
      " |      >>> from datetime import date, timedelta\n",
      " |      >>> base = date(year=2000, month=1, day=1)\n",
      " |      >>> def name_function(i):\n",
      " |      ...     ''' Convert integer 0 to n to a string '''\n",
      " |      ...     return base + timedelta(days=i)\n",
      " |      \n",
      " |      >>> df.to_hdf('*.hdf', '/data', name_function=name_function) # doctest: +SKIP\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      filenames : list\n",
      " |          Returned if ``compute`` is True. List of file names that each partition\n",
      " |          is saved to.\n",
      " |      delayed : dask.Delayed\n",
      " |          Returned if ``compute`` is False. Delayed object to execute ``to_hdf``\n",
      " |          when computed.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      read_hdf:\n",
      " |      to_parquet:\n",
      " |  \n",
      " |  to_json(self, filename, *args, **kwargs)\n",
      " |      See dd.to_json docstring for more information\n",
      " |  \n",
      " |  to_sql(self, name: 'str', uri: 'str', schema=None, if_exists: 'str' = 'fail', index: 'bool' = True, index_label=None, chunksize=None, dtype=None, method=None, compute=True, parallel=False, engine_kwargs=None)\n",
      " |      See dd.to_sql docstring for more information\n",
      " |  \n",
      " |  var(self, axis=None, skipna=True, ddof=1, split_every=False, dtype=None, out=None, numeric_only=None)\n",
      " |      Return unbiased variance over requested axis.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.var.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Normalized by N-1 by default. This can be changed using the ddof argument.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {index (0), columns (1)}\n",
      " |      skipna : bool, default True\n",
      " |          Exclude NA/null values. If an entire row/column is NA, the result\n",
      " |          will be NA.\n",
      " |      level : int or level name, default None  (Not supported in Dask)\n",
      " |          If the axis is a MultiIndex (hierarchical), count along a\n",
      " |          particular level, collapsing into a Series.\n",
      " |      ddof : int, default 1\n",
      " |          Delta Degrees of Freedom. The divisor used in calculations is N - ddof,\n",
      " |          where N represents the number of elements.\n",
      " |      numeric_only : bool, default None\n",
      " |          Include only float, int, boolean columns. If None, will attempt to use\n",
      " |          everything, then use only numeric data. Not implemented for Series.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Series or DataFrame (if level specified) \n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = pd.DataFrame({'person_id': [0, 1, 2, 3],  # doctest: +SKIP\n",
      " |      ...                   'age': [21, 25, 62, 43],\n",
      " |      ...                   'height': [1.61, 1.87, 1.49, 2.01]}\n",
      " |      ...                  ).set_index('person_id')\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |                 age  height\n",
      " |      person_id\n",
      " |      0           21    1.61\n",
      " |      1           25    1.87\n",
      " |      2           62    1.49\n",
      " |      3           43    2.01\n",
      " |      \n",
      " |      >>> df.var()  # doctest: +SKIP\n",
      " |      age       352.916667\n",
      " |      height      0.056367\n",
      " |      \n",
      " |      Alternatively, ``ddof=0`` can be set to normalize by N instead of N-1:\n",
      " |      \n",
      " |      >>> df.var(ddof=0)  # doctest: +SKIP\n",
      " |      age       264.687500\n",
      " |      height      0.042275\n",
      " |  \n",
      " |  where(self, cond, other=nan)\n",
      " |      Replace values where the condition is False.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.where.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cond : bool Series/DataFrame, array-like, or callable\n",
      " |          Where `cond` is True, keep the original value. Where\n",
      " |          False, replace with corresponding value from `other`.\n",
      " |          If `cond` is callable, it is computed on the Series/DataFrame and\n",
      " |          should return boolean Series/DataFrame or array. The callable must\n",
      " |          not change input Series/DataFrame (though pandas doesn't check it).\n",
      " |      other : scalar, Series/DataFrame, or callable\n",
      " |          Entries where `cond` is False are replaced with\n",
      " |          corresponding value from `other`.\n",
      " |          If other is callable, it is computed on the Series/DataFrame and\n",
      " |          should return scalar or Series/DataFrame. The callable must not\n",
      " |          change input Series/DataFrame (though pandas doesn't check it).\n",
      " |      inplace : bool, default False  (Not supported in Dask)\n",
      " |          Whether to perform the operation in place on the data.\n",
      " |      axis : int, default None  (Not supported in Dask)\n",
      " |          Alignment axis if needed.\n",
      " |      level : int, default None  (Not supported in Dask)\n",
      " |          Alignment level if needed.\n",
      " |      errors : str, {'raise', 'ignore'}, default 'raise'  (Not supported in Dask)\n",
      " |          Note that currently this parameter won't affect\n",
      " |          the results and will always coerce to a suitable dtype.\n",
      " |      \n",
      " |          - 'raise' : allow exceptions to be raised.\n",
      " |          - 'ignore' : suppress exceptions. On error return original object.\n",
      " |      \n",
      " |      try_cast : bool, default None  (Not supported in Dask)\n",
      " |          Try to cast the result back to the input type (if possible).\n",
      " |      \n",
      " |          .. deprecated:: 1.3.0\n",
      " |              Manually cast back if necessary.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Same type as caller or None if ``inplace=True``.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :func:`DataFrame.mask` : Return an object of same shape as\n",
      " |          self.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The where method is an application of the if-then idiom. For each\n",
      " |      element in the calling DataFrame, if ``cond`` is ``True`` the\n",
      " |      element is used; otherwise the corresponding element from the DataFrame\n",
      " |      ``other`` is used.\n",
      " |      \n",
      " |      The signature for :func:`DataFrame.where` differs from\n",
      " |      :func:`numpy.where`. Roughly ``df1.where(m, df2)`` is equivalent to\n",
      " |      ``np.where(m, df1, df2)``.\n",
      " |      \n",
      " |      For further details and examples see the ``where`` documentation in\n",
      " |      :ref:`indexing <indexing.where_mask>`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> s = pd.Series(range(5))  # doctest: +SKIP\n",
      " |      >>> s.where(s > 0)  # doctest: +SKIP\n",
      " |      0    NaN\n",
      " |      1    1.0\n",
      " |      2    2.0\n",
      " |      3    3.0\n",
      " |      4    4.0\n",
      " |      dtype: float64\n",
      " |      >>> s.mask(s > 0)  # doctest: +SKIP\n",
      " |      0    0.0\n",
      " |      1    NaN\n",
      " |      2    NaN\n",
      " |      3    NaN\n",
      " |      4    NaN\n",
      " |      dtype: float64\n",
      " |      \n",
      " |      >>> s.where(s > 1, 10)  # doctest: +SKIP\n",
      " |      0    10\n",
      " |      1    10\n",
      " |      2    2\n",
      " |      3    3\n",
      " |      4    4\n",
      " |      dtype: int64\n",
      " |      >>> s.mask(s > 1, 10)  # doctest: +SKIP\n",
      " |      0     0\n",
      " |      1     1\n",
      " |      2    10\n",
      " |      3    10\n",
      " |      4    10\n",
      " |      dtype: int64\n",
      " |      \n",
      " |      >>> df = pd.DataFrame(np.arange(10).reshape(-1, 2), columns=['A', 'B'])  # doctest: +SKIP\n",
      " |      >>> df  # doctest: +SKIP\n",
      " |         A  B\n",
      " |      0  0  1\n",
      " |      1  2  3\n",
      " |      2  4  5\n",
      " |      3  6  7\n",
      " |      4  8  9\n",
      " |      >>> m = df % 3 == 0  # doctest: +SKIP\n",
      " |      >>> df.where(m, -df)  # doctest: +SKIP\n",
      " |         A  B\n",
      " |      0  0 -1\n",
      " |      1 -2  3\n",
      " |      2 -4 -5\n",
      " |      3  6 -7\n",
      " |      4 -8  9\n",
      " |      >>> df.where(m, -df) == np.where(m, df, -df)  # doctest: +SKIP\n",
      " |            A     B\n",
      " |      0  True  True\n",
      " |      1  True  True\n",
      " |      2  True  True\n",
      " |      3  True  True\n",
      " |      4  True  True\n",
      " |      >>> df.where(m, -df) == df.mask(~m, -df)  # doctest: +SKIP\n",
      " |            A     B\n",
      " |      0  True  True\n",
      " |      1  True  True\n",
      " |      2  True  True\n",
      " |      3  True  True\n",
      " |      4  True  True\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from _Frame:\n",
      " |  \n",
      " |  __dask_scheduler__ = get(dsk: 'Mapping', keys: 'Sequence[Hashable] | Hashable', cache=None, num_workers=None, pool=None, **kwargs)\n",
      " |      Threaded cached implementation of dask.get\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      dsk: dict\n",
      " |          A dask dictionary specifying a workflow\n",
      " |      keys: key or list of keys\n",
      " |          Keys corresponding to desired data\n",
      " |      num_workers: integer of thread count\n",
      " |          The number of threads to use in the ThreadPool that will actually execute tasks\n",
      " |      cache: dict-like (optional)\n",
      " |          Temporary storage of results\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> inc = lambda x: x + 1\n",
      " |      >>> add = lambda x, y: x + y\n",
      " |      >>> dsk = {'x': 1, 'y': 2, 'z': (inc, 'x'), 'w': (add, 'z', 'y')}\n",
      " |      >>> get(dsk, 'w')\n",
      " |      4\n",
      " |      >>> get(dsk, ['w', 'y'])\n",
      " |      (4, 2)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from _Frame:\n",
      " |  \n",
      " |  known_divisions\n",
      " |      Whether divisions are already known\n",
      " |  \n",
      " |  loc\n",
      " |      Purely label-location based indexer for selection by label.\n",
      " |      \n",
      " |      >>> df.loc[\"b\"]  # doctest: +SKIP\n",
      " |      >>> df.loc[\"b\":\"d\"]  # doctest: +SKIP\n",
      " |  \n",
      " |  npartitions\n",
      " |      Return number of partitions\n",
      " |  \n",
      " |  partitions\n",
      " |      Slice dataframe by partitions\n",
      " |      \n",
      " |      This allows partitionwise slicing of a Dask Dataframe.  You can perform normal\n",
      " |      Numpy-style slicing but now rather than slice elements of the array you\n",
      " |      slice along partitions so, for example, ``df.partitions[:5]`` produces a new\n",
      " |      Dask Dataframe of the first five partitions.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.partitions[0]  # doctest: +SKIP\n",
      " |      >>> df.partitions[:3]  # doctest: +SKIP\n",
      " |      >>> df.partitions[::10]  # doctest: +SKIP\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      A Dask DataFrame\n",
      " |  \n",
      " |  size\n",
      " |      Size of the Series or DataFrame as a Delayed object.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> series.size  # doctest: +SKIP\n",
      " |      dd.Scalar<size-ag..., dtype=int64>\n",
      " |  \n",
      " |  values\n",
      " |      Return a dask.array of the values of this dataframe\n",
      " |      \n",
      " |      Warning: This creates a dask.array without precise shape information.\n",
      " |      Operations that depend on shape information, like slicing or reshaping,\n",
      " |      will not work.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from _Frame:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  attrs\n",
      " |      Dictionary of global attributes of this dataset.\n",
      " |      \n",
      " |      This docstring was copied from pandas.core.frame.DataFrame.attrs.\n",
      " |      \n",
      " |      Some inconsistencies with the Dask version may exist.\n",
      " |      \n",
      " |      .. warning::\n",
      " |      \n",
      " |         attrs is experimental and may change without warning.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.flags : Global flags applying to this object.\n",
      " |  \n",
      " |  divisions\n",
      " |      Tuple of ``npartitions + 1`` values, in ascending order, marking the\n",
      " |      lower/upper bounds of each partition's index. Divisions allow Dask\n",
      " |      to know which partition will contain a given value, significantly\n",
      " |      speeding up operations like `loc`, `merge`, and `groupby` by not\n",
      " |      having to search the full dataset.\n",
      " |      \n",
      " |      Example: for ``divisions = (0, 10, 50, 100)``, there are three partitions,\n",
      " |      where the index in each partition contains values [0, 10), [10, 50),\n",
      " |      and [50, 100], respectively. Dask therefore knows ``df.loc[45]``\n",
      " |      will be in the second partition.\n",
      " |      \n",
      " |      When every item in ``divisions`` is ``None``, the divisions are unknown.\n",
      " |      Most operations can still be performed, but some will be much slower,\n",
      " |      and a few may fail.\n",
      " |      \n",
      " |      It is uncommon to set ``divisions`` directly. Instead, use ``set_index``,\n",
      " |      which sorts and splits the data as needed.\n",
      " |      See https://docs.dask.org/en/latest/dataframe-design.html#partitions.\n",
      " |  \n",
      " |  index\n",
      " |      Return dask Index instance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from dask.base.DaskMethodsMixin:\n",
      " |  \n",
      " |  __await__(self)\n",
      " |  \n",
      " |  compute(self, **kwargs)\n",
      " |      Compute this dask collection\n",
      " |      \n",
      " |      This turns a lazy Dask collection into its in-memory equivalent.\n",
      " |      For example a Dask array turns into a NumPy array and a Dask dataframe\n",
      " |      turns into a Pandas dataframe.  The entire dataset must fit into memory\n",
      " |      before calling this operation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      scheduler : string, optional\n",
      " |          Which scheduler to use like \"threads\", \"synchronous\" or \"processes\".\n",
      " |          If not provided, the default is to check the global settings first,\n",
      " |          and then fall back to the collection defaults.\n",
      " |      optimize_graph : bool, optional\n",
      " |          If True [default], the graph is optimized before computation.\n",
      " |          Otherwise the graph is run as is. This can be useful for debugging.\n",
      " |      kwargs\n",
      " |          Extra keywords to forward to the scheduler function.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      dask.base.compute\n",
      " |  \n",
      " |  persist(self, **kwargs)\n",
      " |      Persist this dask collection into memory\n",
      " |      \n",
      " |      This turns a lazy Dask collection into a Dask collection with the same\n",
      " |      metadata, but now with the results fully computed or actively computing\n",
      " |      in the background.\n",
      " |      \n",
      " |      The action of function differs significantly depending on the active\n",
      " |      task scheduler.  If the task scheduler supports asynchronous computing,\n",
      " |      such as is the case of the dask.distributed scheduler, then persist\n",
      " |      will return *immediately* and the return value's task graph will\n",
      " |      contain Dask Future objects.  However if the task scheduler only\n",
      " |      supports blocking computation then the call to persist will *block*\n",
      " |      and the return value's task graph will contain concrete Python results.\n",
      " |      \n",
      " |      This function is particularly useful when using distributed systems,\n",
      " |      because the results will be kept in distributed memory, rather than\n",
      " |      returned to the local process as with compute.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      scheduler : string, optional\n",
      " |          Which scheduler to use like \"threads\", \"synchronous\" or \"processes\".\n",
      " |          If not provided, the default is to check the global settings first,\n",
      " |          and then fall back to the collection defaults.\n",
      " |      optimize_graph : bool, optional\n",
      " |          If True [default], the graph is optimized before computation.\n",
      " |          Otherwise the graph is run as is. This can be useful for debugging.\n",
      " |      **kwargs\n",
      " |          Extra keywords to forward to the scheduler function.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      New dask collections backed by in-memory data\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      dask.base.persist\n",
      " |  \n",
      " |  visualize(self, filename='mydask', format=None, optimize_graph=False, **kwargs)\n",
      " |      Render the computation of this object's task graph using graphviz.\n",
      " |      \n",
      " |      Requires ``graphviz`` to be installed.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      filename : str or None, optional\n",
      " |          The name of the file to write to disk. If the provided `filename`\n",
      " |          doesn't include an extension, '.png' will be used by default.\n",
      " |          If `filename` is None, no file will be written, and we communicate\n",
      " |          with dot using only pipes.\n",
      " |      format : {'png', 'pdf', 'dot', 'svg', 'jpeg', 'jpg'}, optional\n",
      " |          Format in which to write output file.  Default is 'png'.\n",
      " |      optimize_graph : bool, optional\n",
      " |          If True, the graph is optimized before rendering.  Otherwise,\n",
      " |          the graph is displayed as is. Default is False.\n",
      " |      color: {None, 'order'}, optional\n",
      " |          Options to color nodes.  Provide ``cmap=`` keyword for additional\n",
      " |          colormap\n",
      " |      **kwargs\n",
      " |         Additional keyword arguments to forward to ``to_graphviz``.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x.visualize(filename='dask.pdf')  # doctest: +SKIP\n",
      " |      >>> x.visualize(filename='dask.pdf', color='order')  # doctest: +SKIP\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      result : IPython.diplay.Image, IPython.display.SVG, or None\n",
      " |          See dask.dot.dot_graph for more information.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      dask.base.visualize\n",
      " |      dask.dot.dot_graph\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For more information on optimization see here:\n",
      " |      \n",
      " |      https://docs.dask.org/en/latest/optimize.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#help(vehicles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7260e404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "fe116c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATASOURCE</th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>VEHICLEID</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>MINUTES</th>\n",
       "      <th>LASTUPDATE</th>\n",
       "      <th>NOTE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DB</td>\n",
       "      <td>23-NOV-18 00:00:00</td>\n",
       "      <td>3303848</td>\n",
       "      <td>286166</td>\n",
       "      <td>58849</td>\n",
       "      <td>04-DEC-18 08:03:09</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DB</td>\n",
       "      <td>23-NOV-18 00:00:00</td>\n",
       "      <td>3303847</td>\n",
       "      <td>259545</td>\n",
       "      <td>56828</td>\n",
       "      <td>04-DEC-18 08:03:09</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  DATASOURCE        DAYOFSERVICE  VEHICLEID  DISTANCE  MINUTES  \\\n",
       "0         DB  23-NOV-18 00:00:00    3303848    286166    58849   \n",
       "1         DB  23-NOV-18 00:00:00    3303847    259545    56828   \n",
       "\n",
       "           LASTUPDATE  NOTE  \n",
       "0  04-DEC-18 08:03:09  <NA>  \n",
       "1  04-DEC-18 08:03:09  <NA>  "
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vehicles.drop({\"NOTE\", \"DATASOURCE\"}, axis=1).compute()\n",
    "#vehicles.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "90caed7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 7 entries, DATASOURCE to NOTE\n",
      "dtypes: category(1), int32(3), string(3)"
     ]
    }
   ],
   "source": [
    "#vehicles.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "3f370bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vehicles = vehicles.rename(columns={\"LASTUPDATE\": \"LASTUPDATE_V\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "fcd9b923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATASOURCE</th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>VEHICLEID</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>MINUTES</th>\n",
       "      <th>LASTUPDATE_V</th>\n",
       "      <th>NOTE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DB</td>\n",
       "      <td>23-NOV-18 00:00:00</td>\n",
       "      <td>3303848</td>\n",
       "      <td>286166</td>\n",
       "      <td>58849</td>\n",
       "      <td>04-DEC-18 08:03:09</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DB</td>\n",
       "      <td>23-NOV-18 00:00:00</td>\n",
       "      <td>3303847</td>\n",
       "      <td>259545</td>\n",
       "      <td>56828</td>\n",
       "      <td>04-DEC-18 08:03:09</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DB</td>\n",
       "      <td>28-FEB-18 00:00:00</td>\n",
       "      <td>2868329</td>\n",
       "      <td>103096</td>\n",
       "      <td>40967</td>\n",
       "      <td>08-MAR-18 10:35:59</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  DATASOURCE        DAYOFSERVICE  VEHICLEID  DISTANCE  MINUTES  \\\n",
       "0         DB  23-NOV-18 00:00:00    3303848    286166    58849   \n",
       "1         DB  23-NOV-18 00:00:00    3303847    259545    56828   \n",
       "2         DB  28-FEB-18 00:00:00    2868329    103096    40967   \n",
       "\n",
       "         LASTUPDATE_V  NOTE  \n",
       "0  04-DEC-18 08:03:09  <NA>  \n",
       "1  04-DEC-18 08:03:09  <NA>  \n",
       "2  08-MAR-18 10:35:59  <NA>  "
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vehicles.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "f26a1800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# print(type(vehicles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb5274d",
   "metadata": {},
   "source": [
    "##### work backwards bit.\n",
    " - none of the columns really matter. keeping vehicle ID just incase but expect it to be useless. its in leacvetimes anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd34409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecb8effc",
   "metadata": {},
   "source": [
    "## Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "cf7bf617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 7 entries, DATASOURCE to NOTE\n",
      "dtypes: category(1), int32(3), string(3)"
     ]
    }
   ],
   "source": [
    "#vehicles.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dc954da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 7 entries, DAYOFSERVICE to ACTUALTIME_DEP\n",
      "dtypes: category(4), float64(2), int32(1)"
     ]
    }
   ],
   "source": [
    "trips.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c533a316",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>PROGRNUMBER</th>\n",
       "      <th>STOPPOINTID</th>\n",
       "      <th>PLANNEDTIME_ARR</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_ARR</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "      <th>VEHICLEID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>48030.0</td>\n",
       "      <td>48030.0</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>2693211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5966674</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>54001.0</td>\n",
       "      <td>54001.0</td>\n",
       "      <td>54023.0</td>\n",
       "      <td>54023.0</td>\n",
       "      <td>2693267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5959105</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>60001.0</td>\n",
       "      <td>60001.0</td>\n",
       "      <td>59955.0</td>\n",
       "      <td>59955.0</td>\n",
       "      <td>2693263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5966888</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>58801.0</td>\n",
       "      <td>58801.0</td>\n",
       "      <td>58771.0</td>\n",
       "      <td>58771.0</td>\n",
       "      <td>2693284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5965960</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>56401.0</td>\n",
       "      <td>56401.0</td>\n",
       "      <td>56309.0</td>\n",
       "      <td>56323.0</td>\n",
       "      <td>2693209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664446</th>\n",
       "      <td>31-DEC-18 00:00:00</td>\n",
       "      <td>8588153</td>\n",
       "      <td>78</td>\n",
       "      <td>4383</td>\n",
       "      <td>28605.0</td>\n",
       "      <td>28605.0</td>\n",
       "      <td>28998.0</td>\n",
       "      <td>29013.0</td>\n",
       "      <td>3265721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664447</th>\n",
       "      <td>31-DEC-18 00:00:00</td>\n",
       "      <td>8587459</td>\n",
       "      <td>78</td>\n",
       "      <td>4383</td>\n",
       "      <td>22695.0</td>\n",
       "      <td>22695.0</td>\n",
       "      <td>23247.0</td>\n",
       "      <td>23247.0</td>\n",
       "      <td>3265687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664448</th>\n",
       "      <td>31-DEC-18 00:00:00</td>\n",
       "      <td>8586183</td>\n",
       "      <td>78</td>\n",
       "      <td>4383</td>\n",
       "      <td>51481.0</td>\n",
       "      <td>51481.0</td>\n",
       "      <td>52237.0</td>\n",
       "      <td>52283.0</td>\n",
       "      <td>2693229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664449</th>\n",
       "      <td>31-DEC-18 00:00:00</td>\n",
       "      <td>8589374</td>\n",
       "      <td>23</td>\n",
       "      <td>7053</td>\n",
       "      <td>53659.0</td>\n",
       "      <td>53659.0</td>\n",
       "      <td>53525.0</td>\n",
       "      <td>53525.0</td>\n",
       "      <td>3265669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664450</th>\n",
       "      <td>31-DEC-18 00:00:00</td>\n",
       "      <td>8589372</td>\n",
       "      <td>24</td>\n",
       "      <td>2088</td>\n",
       "      <td>46383.0</td>\n",
       "      <td>46383.0</td>\n",
       "      <td>46315.0</td>\n",
       "      <td>46325.0</td>\n",
       "      <td>3265669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116949113 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              DAYOFSERVICE   TRIPID  PROGRNUMBER  STOPPOINTID  \\\n",
       "0       01-JAN-18 00:00:00  5972116           12          119   \n",
       "1       01-JAN-18 00:00:00  5966674           12          119   \n",
       "2       01-JAN-18 00:00:00  5959105           12          119   \n",
       "3       01-JAN-18 00:00:00  5966888           12          119   \n",
       "4       01-JAN-18 00:00:00  5965960           12          119   \n",
       "...                    ...      ...          ...          ...   \n",
       "664446  31-DEC-18 00:00:00  8588153           78         4383   \n",
       "664447  31-DEC-18 00:00:00  8587459           78         4383   \n",
       "664448  31-DEC-18 00:00:00  8586183           78         4383   \n",
       "664449  31-DEC-18 00:00:00  8589374           23         7053   \n",
       "664450  31-DEC-18 00:00:00  8589372           24         2088   \n",
       "\n",
       "        PLANNEDTIME_ARR  PLANNEDTIME_DEP  ACTUALTIME_ARR  ACTUALTIME_DEP  \\\n",
       "0               48030.0          48030.0         48012.0         48012.0   \n",
       "1               54001.0          54001.0         54023.0         54023.0   \n",
       "2               60001.0          60001.0         59955.0         59955.0   \n",
       "3               58801.0          58801.0         58771.0         58771.0   \n",
       "4               56401.0          56401.0         56309.0         56323.0   \n",
       "...                 ...              ...             ...             ...   \n",
       "664446          28605.0          28605.0         28998.0         29013.0   \n",
       "664447          22695.0          22695.0         23247.0         23247.0   \n",
       "664448          51481.0          51481.0         52237.0         52283.0   \n",
       "664449          53659.0          53659.0         53525.0         53525.0   \n",
       "664450          46383.0          46383.0         46315.0         46325.0   \n",
       "\n",
       "        VEHICLEID  \n",
       "0         2693211  \n",
       "1         2693267  \n",
       "2         2693263  \n",
       "3         2693284  \n",
       "4         2693209  \n",
       "...           ...  \n",
       "664446    3265721  \n",
       "664447    3265687  \n",
       "664448    2693229  \n",
       "664449    3265669  \n",
       "664450    3265669  \n",
       "\n",
       "[116949113 rows x 9 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaveTimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e8ecc96d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>PROGRNUMBER</th>\n",
       "      <th>STOPPOINTID</th>\n",
       "      <th>PLANNEDTIME_ARR</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_ARR</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "      <th>VEHICLEID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>48030.0</td>\n",
       "      <td>48030.0</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>2693211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5966674</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>54001.0</td>\n",
       "      <td>54001.0</td>\n",
       "      <td>54023.0</td>\n",
       "      <td>54023.0</td>\n",
       "      <td>2693267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5959105</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>60001.0</td>\n",
       "      <td>60001.0</td>\n",
       "      <td>59955.0</td>\n",
       "      <td>59955.0</td>\n",
       "      <td>2693263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         DAYOFSERVICE   TRIPID  PROGRNUMBER  STOPPOINTID  PLANNEDTIME_ARR  \\\n",
       "0  01-JAN-18 00:00:00  5972116           12          119          48030.0   \n",
       "1  01-JAN-18 00:00:00  5966674           12          119          54001.0   \n",
       "2  01-JAN-18 00:00:00  5959105           12          119          60001.0   \n",
       "\n",
       "   PLANNEDTIME_DEP  ACTUALTIME_ARR  ACTUALTIME_DEP  VEHICLEID  \n",
       "0          48030.0         48012.0         48012.0    2693211  \n",
       "1          54001.0         54023.0         54023.0    2693267  \n",
       "2          60001.0         59955.0         59955.0    2693263  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaveTimes.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a09f653f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116949113, 9)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaveTimes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de500e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips= trips.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86c8bb8",
   "metadata": {},
   "source": [
    "- merge needed to add the route id and line id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2330035c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can only merge Series or DataFrame objects, a <class 'dask.dataframe.core.DataFrame'> was passed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [98]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m merged_dd \u001b[38;5;241m=\u001b[39m \u001b[43mleaveTimes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrips\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTRIPID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDAYOFSERVICE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m merged_dd\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m50\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/comp47360py39/lib/python3.9/site-packages/pandas/core/frame.py:9345\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m   9326\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   9327\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   9328\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9341\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   9342\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   9343\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[0;32m-> 9345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   9346\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9354\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9355\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9359\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/comp47360py39/lib/python3.9/site-packages/pandas/core/reshape/merge.py:107\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mleft : DataFrame or named Series\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m--> 107\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/miniconda3/envs/comp47360py39/lib/python3.9/site-packages/pandas/core/reshape/merge.py:629\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    612\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    613\u001b[0m     left: DataFrame \u001b[38;5;241m|\u001b[39m Series,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    626\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    627\u001b[0m ):\n\u001b[1;32m    628\u001b[0m     _left \u001b[38;5;241m=\u001b[39m _validate_operand(left)\n\u001b[0;32m--> 629\u001b[0m     _right \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_operand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_left \u001b[38;5;241m=\u001b[39m _left\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_right \u001b[38;5;241m=\u001b[39m _right\n",
      "File \u001b[0;32m~/miniconda3/envs/comp47360py39/lib/python3.9/site-packages/pandas/core/reshape/merge.py:2281\u001b[0m, in \u001b[0;36m_validate_operand\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2279\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   2280\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   2282\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only merge Series or DataFrame objects, a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was passed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2283\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: Can only merge Series or DataFrame objects, a <class 'dask.dataframe.core.DataFrame'> was passed"
     ]
    }
   ],
   "source": [
    "merged_dd = leaveTimes.merge(trips, how=\"left\", on=[\"TRIPID\", \"DAYOFSERVICE\"])\n",
    "merged_dd.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "ae962010",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_dd = merged_dd.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ecdbf2",
   "metadata": {},
   "source": [
    "- tried to merge on tripid but there is a data issue because the same id was sometimes reused, resulting in the creation of 600million more rows \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "7decc337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATASOURCE</th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>VEHICLEID</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>MINUTES</th>\n",
       "      <th>LASTUPDATE_V</th>\n",
       "      <th>NOTE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DB</td>\n",
       "      <td>23-NOV-18 00:00:00</td>\n",
       "      <td>3303848</td>\n",
       "      <td>286166</td>\n",
       "      <td>58849</td>\n",
       "      <td>04-DEC-18 08:03:09</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DB</td>\n",
       "      <td>23-NOV-18 00:00:00</td>\n",
       "      <td>3303847</td>\n",
       "      <td>259545</td>\n",
       "      <td>56828</td>\n",
       "      <td>04-DEC-18 08:03:09</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DB</td>\n",
       "      <td>28-FEB-18 00:00:00</td>\n",
       "      <td>2868329</td>\n",
       "      <td>103096</td>\n",
       "      <td>40967</td>\n",
       "      <td>08-MAR-18 10:35:59</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  DATASOURCE        DAYOFSERVICE  VEHICLEID  DISTANCE  MINUTES  \\\n",
       "0         DB  23-NOV-18 00:00:00    3303848    286166    58849   \n",
       "1         DB  23-NOV-18 00:00:00    3303847    259545    56828   \n",
       "2         DB  28-FEB-18 00:00:00    2868329    103096    40967   \n",
       "\n",
       "         LASTUPDATE_V  NOTE  \n",
       "0  04-DEC-18 08:03:09  <NA>  \n",
       "1  04-DEC-18 08:03:09  <NA>  \n",
       "2  08-MAR-18 10:35:59  <NA>  "
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vehicles.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "68848beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(vehicles))\n",
    "# print(len(leaveTimes)) 116949113 rows\n",
    "# print(len(trips))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86759c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "700f24f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 14 entries, DAYOFSERVICE to ACTUALTIME_DEP_T\n",
      "dtypes: category(1), category(1), category(1), object(1), float64(6), int32(4)"
     ]
    }
   ],
   "source": [
    "merged_dd.info()\n",
    "#len(merged_dd) \n",
    "#116949113 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4342871a",
   "metadata": {},
   "source": [
    "- too many rows means many dont match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd556597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3c356b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "2d12c5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vehicles['DAYOFSERVICE'] = vehicles.DAYOFSERVICE.astype('category')\n",
    "# had made a mistake earlier in assigning categories. \n",
    "# migh go back and change later bu t dont want to wait for code to run now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "71efa44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_dd = merged_dd.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "26f76710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>PROGRNUMBER</th>\n",
       "      <th>STOPPOINTID</th>\n",
       "      <th>PLANNEDTIME_ARR</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_ARR</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "      <th>VEHICLEID</th>\n",
       "      <th>LINEID</th>\n",
       "      <th>ROUTEID</th>\n",
       "      <th>DIRECTION</th>\n",
       "      <th>PLANNEDTIME_DEP_T</th>\n",
       "      <th>ACTUALTIME_DEP_T</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=176</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>object</td>\n",
       "      <td>int32</td>\n",
       "      <td>int32</td>\n",
       "      <td>int32</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>int32</td>\n",
       "      <td>category[known]</td>\n",
       "      <td>category[known]</td>\n",
       "      <td>category[known]</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: merge_chunk, 353 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                DAYOFSERVICE TRIPID PROGRNUMBER STOPPOINTID PLANNEDTIME_ARR PLANNEDTIME_DEP ACTUALTIME_ARR ACTUALTIME_DEP VEHICLEID           LINEID          ROUTEID        DIRECTION PLANNEDTIME_DEP_T ACTUALTIME_DEP_T\n",
       "npartitions=176                                                                                                                                                                                                          \n",
       "                      object  int32       int32       int32         float64         float64        float64        float64     int32  category[known]  category[known]  category[known]           float64          float64\n",
       "                         ...    ...         ...         ...             ...             ...            ...            ...       ...              ...              ...              ...               ...              ...\n",
       "...                      ...    ...         ...         ...             ...             ...            ...            ...       ...              ...              ...              ...               ...              ...\n",
       "                         ...    ...         ...         ...             ...             ...            ...            ...       ...              ...              ...              ...               ...              ...\n",
       "                         ...    ...         ...         ...             ...             ...            ...            ...       ...              ...              ...              ...               ...              ...\n",
       "Dask Name: merge_chunk, 353 tasks"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "0db00f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(merged_dd) # 116949113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "d67777e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_dd.memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "65242839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>PROGRNUMBER</th>\n",
       "      <th>STOPPOINTID</th>\n",
       "      <th>PLANNEDTIME_ARR</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_ARR</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "      <th>VEHICLEID</th>\n",
       "      <th>LINEID</th>\n",
       "      <th>ROUTEID</th>\n",
       "      <th>DIRECTION</th>\n",
       "      <th>PLANNEDTIME_DEP_T</th>\n",
       "      <th>ACTUALTIME_DEP_T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>48030.0</td>\n",
       "      <td>48030.0</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5966674</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>54001.0</td>\n",
       "      <td>54001.0</td>\n",
       "      <td>54023.0</td>\n",
       "      <td>54023.0</td>\n",
       "      <td>2693267</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>53400.0</td>\n",
       "      <td>53410.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5959105</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>60001.0</td>\n",
       "      <td>60001.0</td>\n",
       "      <td>59955.0</td>\n",
       "      <td>59955.0</td>\n",
       "      <td>2693263</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>59400.0</td>\n",
       "      <td>59426.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5966888</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>58801.0</td>\n",
       "      <td>58801.0</td>\n",
       "      <td>58771.0</td>\n",
       "      <td>58771.0</td>\n",
       "      <td>2693284</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>58200.0</td>\n",
       "      <td>58220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5965960</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>56401.0</td>\n",
       "      <td>56401.0</td>\n",
       "      <td>56309.0</td>\n",
       "      <td>56323.0</td>\n",
       "      <td>2693209</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>55800.0</td>\n",
       "      <td>55807.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5965964</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>70688.0</td>\n",
       "      <td>70688.0</td>\n",
       "      <td>70663.0</td>\n",
       "      <td>70679.0</td>\n",
       "      <td>2693209</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>70200.0</td>\n",
       "      <td>70236.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5958117</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>72488.0</td>\n",
       "      <td>72488.0</td>\n",
       "      <td>72539.0</td>\n",
       "      <td>72539.0</td>\n",
       "      <td>2172293</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>72023.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5959109</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>74288.0</td>\n",
       "      <td>74288.0</td>\n",
       "      <td>74173.0</td>\n",
       "      <td>74173.0</td>\n",
       "      <td>2693263</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>73800.0</td>\n",
       "      <td>73806.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972114</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>40187.0</td>\n",
       "      <td>40187.0</td>\n",
       "      <td>40096.0</td>\n",
       "      <td>40111.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>39600.0</td>\n",
       "      <td>39681.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5959099</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>38387.0</td>\n",
       "      <td>38387.0</td>\n",
       "      <td>38271.0</td>\n",
       "      <td>38271.0</td>\n",
       "      <td>2693263</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>37800.0</td>\n",
       "      <td>37810.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5965956</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>41987.0</td>\n",
       "      <td>41987.0</td>\n",
       "      <td>42017.0</td>\n",
       "      <td>42035.0</td>\n",
       "      <td>2693209</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>41400.0</td>\n",
       "      <td>41482.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5956265</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>36587.0</td>\n",
       "      <td>36587.0</td>\n",
       "      <td>36521.0</td>\n",
       "      <td>36521.0</td>\n",
       "      <td>2693264</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>36000.0</td>\n",
       "      <td>36048.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5966676</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>61178.0</td>\n",
       "      <td>61178.0</td>\n",
       "      <td>61280.0</td>\n",
       "      <td>61280.0</td>\n",
       "      <td>2693267</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>60600.0</td>\n",
       "      <td>60697.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5965962</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>63578.0</td>\n",
       "      <td>63578.0</td>\n",
       "      <td>63390.0</td>\n",
       "      <td>63390.0</td>\n",
       "      <td>2693209</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>63000.0</td>\n",
       "      <td>63022.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972120</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>62378.0</td>\n",
       "      <td>62378.0</td>\n",
       "      <td>62215.0</td>\n",
       "      <td>62238.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>61800.0</td>\n",
       "      <td>61767.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5966890</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>65936.0</td>\n",
       "      <td>65936.0</td>\n",
       "      <td>65944.0</td>\n",
       "      <td>65944.0</td>\n",
       "      <td>2693228</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>65400.0</td>\n",
       "      <td>65427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5959107</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>67136.0</td>\n",
       "      <td>67136.0</td>\n",
       "      <td>67120.0</td>\n",
       "      <td>67120.0</td>\n",
       "      <td>2693263</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>66600.0</td>\n",
       "      <td>66613.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5966680</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>76062.0</td>\n",
       "      <td>76062.0</td>\n",
       "      <td>75913.0</td>\n",
       "      <td>75925.0</td>\n",
       "      <td>2693267</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>75600.0</td>\n",
       "      <td>75529.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5959111</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>81462.0</td>\n",
       "      <td>81462.0</td>\n",
       "      <td>81447.0</td>\n",
       "      <td>81447.0</td>\n",
       "      <td>2693263</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>81000.0</td>\n",
       "      <td>81010.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5958119</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>79662.0</td>\n",
       "      <td>79662.0</td>\n",
       "      <td>79676.0</td>\n",
       "      <td>79676.0</td>\n",
       "      <td>2172293</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>79200.0</td>\n",
       "      <td>79215.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5965966</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>77862.0</td>\n",
       "      <td>77862.0</td>\n",
       "      <td>77801.0</td>\n",
       "      <td>77801.0</td>\n",
       "      <td>2693209</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>77400.0</td>\n",
       "      <td>77434.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5966682</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>83262.0</td>\n",
       "      <td>83262.0</td>\n",
       "      <td>83279.0</td>\n",
       "      <td>83279.0</td>\n",
       "      <td>2693267</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>82800.0</td>\n",
       "      <td>82826.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5959101</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>45679.0</td>\n",
       "      <td>45679.0</td>\n",
       "      <td>45696.0</td>\n",
       "      <td>45714.0</td>\n",
       "      <td>2693263</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>45000.0</td>\n",
       "      <td>45026.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5959103</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>52879.0</td>\n",
       "      <td>52879.0</td>\n",
       "      <td>52815.0</td>\n",
       "      <td>52834.0</td>\n",
       "      <td>2693263</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>52200.0</td>\n",
       "      <td>52217.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5966886</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>51679.0</td>\n",
       "      <td>51679.0</td>\n",
       "      <td>51669.0</td>\n",
       "      <td>51669.0</td>\n",
       "      <td>2693284</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>51000.0</td>\n",
       "      <td>51043.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5965958</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>49279.0</td>\n",
       "      <td>49279.0</td>\n",
       "      <td>49187.0</td>\n",
       "      <td>49187.0</td>\n",
       "      <td>2693209</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>48600.0</td>\n",
       "      <td>48666.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5966672</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>46879.0</td>\n",
       "      <td>46879.0</td>\n",
       "      <td>46824.0</td>\n",
       "      <td>46835.0</td>\n",
       "      <td>2693267</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>46200.0</td>\n",
       "      <td>46208.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5956267</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>43279.0</td>\n",
       "      <td>43279.0</td>\n",
       "      <td>43237.0</td>\n",
       "      <td>43237.0</td>\n",
       "      <td>2693264</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>42600.0</td>\n",
       "      <td>42633.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5966884</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>44479.0</td>\n",
       "      <td>44479.0</td>\n",
       "      <td>44401.0</td>\n",
       "      <td>44401.0</td>\n",
       "      <td>2693284</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>43800.0</td>\n",
       "      <td>43822.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>48079.0</td>\n",
       "      <td>48079.0</td>\n",
       "      <td>48058.0</td>\n",
       "      <td>48058.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972118</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>55246.0</td>\n",
       "      <td>55246.0</td>\n",
       "      <td>55025.0</td>\n",
       "      <td>55025.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>54600.0</td>\n",
       "      <td>54271.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5959105</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>60046.0</td>\n",
       "      <td>60046.0</td>\n",
       "      <td>59983.0</td>\n",
       "      <td>59983.0</td>\n",
       "      <td>2693263</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>59400.0</td>\n",
       "      <td>59426.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5966888</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>58846.0</td>\n",
       "      <td>58846.0</td>\n",
       "      <td>58796.0</td>\n",
       "      <td>58803.0</td>\n",
       "      <td>2693284</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>58200.0</td>\n",
       "      <td>58220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5965960</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>56446.0</td>\n",
       "      <td>56446.0</td>\n",
       "      <td>56346.0</td>\n",
       "      <td>56398.0</td>\n",
       "      <td>2693209</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>55800.0</td>\n",
       "      <td>55807.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972122</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>69528.0</td>\n",
       "      <td>69528.0</td>\n",
       "      <td>69594.0</td>\n",
       "      <td>69594.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>69000.0</td>\n",
       "      <td>69008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5965964</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>70728.0</td>\n",
       "      <td>70728.0</td>\n",
       "      <td>70721.0</td>\n",
       "      <td>70721.0</td>\n",
       "      <td>2693209</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>70200.0</td>\n",
       "      <td>70236.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5966678</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>68328.0</td>\n",
       "      <td>68328.0</td>\n",
       "      <td>68201.0</td>\n",
       "      <td>68201.0</td>\n",
       "      <td>2693267</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>67800.0</td>\n",
       "      <td>67784.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5959109</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>74328.0</td>\n",
       "      <td>74328.0</td>\n",
       "      <td>74190.0</td>\n",
       "      <td>74190.0</td>\n",
       "      <td>2693263</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>73800.0</td>\n",
       "      <td>73806.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5959099</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>38432.0</td>\n",
       "      <td>38432.0</td>\n",
       "      <td>38297.0</td>\n",
       "      <td>38297.0</td>\n",
       "      <td>2693263</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>37800.0</td>\n",
       "      <td>37810.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5965956</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>42032.0</td>\n",
       "      <td>42032.0</td>\n",
       "      <td>42055.0</td>\n",
       "      <td>42055.0</td>\n",
       "      <td>2693209</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>41400.0</td>\n",
       "      <td>41482.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5956265</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>36632.0</td>\n",
       "      <td>36632.0</td>\n",
       "      <td>36572.0</td>\n",
       "      <td>36584.0</td>\n",
       "      <td>2693264</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>36000.0</td>\n",
       "      <td>36048.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5966676</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>61219.0</td>\n",
       "      <td>61219.0</td>\n",
       "      <td>61302.0</td>\n",
       "      <td>61302.0</td>\n",
       "      <td>2693267</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>60600.0</td>\n",
       "      <td>60697.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5965962</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>63619.0</td>\n",
       "      <td>63619.0</td>\n",
       "      <td>63415.0</td>\n",
       "      <td>63415.0</td>\n",
       "      <td>2693209</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>63000.0</td>\n",
       "      <td>63022.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972120</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>62419.0</td>\n",
       "      <td>62419.0</td>\n",
       "      <td>62265.0</td>\n",
       "      <td>62271.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>61800.0</td>\n",
       "      <td>61767.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5958115</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>64775.0</td>\n",
       "      <td>64775.0</td>\n",
       "      <td>64727.0</td>\n",
       "      <td>64727.0</td>\n",
       "      <td>2172293</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>64200.0</td>\n",
       "      <td>64247.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5966890</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>65975.0</td>\n",
       "      <td>65975.0</td>\n",
       "      <td>65971.0</td>\n",
       "      <td>65971.0</td>\n",
       "      <td>2693228</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>65400.0</td>\n",
       "      <td>65427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5959107</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>67175.0</td>\n",
       "      <td>67175.0</td>\n",
       "      <td>67142.0</td>\n",
       "      <td>67142.0</td>\n",
       "      <td>2693263</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>66600.0</td>\n",
       "      <td>66613.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5959111</td>\n",
       "      <td>14</td>\n",
       "      <td>7603</td>\n",
       "      <td>81482.0</td>\n",
       "      <td>81482.0</td>\n",
       "      <td>81458.0</td>\n",
       "      <td>81458.0</td>\n",
       "      <td>2693263</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>81000.0</td>\n",
       "      <td>81010.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5958119</td>\n",
       "      <td>14</td>\n",
       "      <td>7603</td>\n",
       "      <td>79682.0</td>\n",
       "      <td>79682.0</td>\n",
       "      <td>79693.0</td>\n",
       "      <td>79693.0</td>\n",
       "      <td>2172293</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>79200.0</td>\n",
       "      <td>79215.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5965966</td>\n",
       "      <td>14</td>\n",
       "      <td>7603</td>\n",
       "      <td>77882.0</td>\n",
       "      <td>77882.0</td>\n",
       "      <td>77820.0</td>\n",
       "      <td>77837.0</td>\n",
       "      <td>2693209</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>77400.0</td>\n",
       "      <td>77434.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          DAYOFSERVICE   TRIPID  PROGRNUMBER  STOPPOINTID  PLANNEDTIME_ARR  \\\n",
       "0   01-JAN-18 00:00:00  5972116           12          119          48030.0   \n",
       "1   01-JAN-18 00:00:00  5966674           12          119          54001.0   \n",
       "2   01-JAN-18 00:00:00  5959105           12          119          60001.0   \n",
       "3   01-JAN-18 00:00:00  5966888           12          119          58801.0   \n",
       "4   01-JAN-18 00:00:00  5965960           12          119          56401.0   \n",
       "5   01-JAN-18 00:00:00  5965964           12          119          70688.0   \n",
       "6   01-JAN-18 00:00:00  5958117           12          119          72488.0   \n",
       "7   01-JAN-18 00:00:00  5959109           12          119          74288.0   \n",
       "8   01-JAN-18 00:00:00  5972114           12          119          40187.0   \n",
       "9   01-JAN-18 00:00:00  5959099           12          119          38387.0   \n",
       "10  01-JAN-18 00:00:00  5965956           12          119          41987.0   \n",
       "11  01-JAN-18 00:00:00  5956265           12          119          36587.0   \n",
       "12  01-JAN-18 00:00:00  5966676           12          119          61178.0   \n",
       "13  01-JAN-18 00:00:00  5965962           12          119          63578.0   \n",
       "14  01-JAN-18 00:00:00  5972120           12          119          62378.0   \n",
       "15  01-JAN-18 00:00:00  5966890           12          119          65936.0   \n",
       "16  01-JAN-18 00:00:00  5959107           12          119          67136.0   \n",
       "17  01-JAN-18 00:00:00  5966680           13           44          76062.0   \n",
       "18  01-JAN-18 00:00:00  5959111           13           44          81462.0   \n",
       "19  01-JAN-18 00:00:00  5958119           13           44          79662.0   \n",
       "20  01-JAN-18 00:00:00  5965966           13           44          77862.0   \n",
       "21  01-JAN-18 00:00:00  5966682           13           44          83262.0   \n",
       "22  01-JAN-18 00:00:00  5959101           13           44          45679.0   \n",
       "23  01-JAN-18 00:00:00  5959103           13           44          52879.0   \n",
       "24  01-JAN-18 00:00:00  5966886           13           44          51679.0   \n",
       "25  01-JAN-18 00:00:00  5965958           13           44          49279.0   \n",
       "26  01-JAN-18 00:00:00  5966672           13           44          46879.0   \n",
       "27  01-JAN-18 00:00:00  5956267           13           44          43279.0   \n",
       "28  01-JAN-18 00:00:00  5966884           13           44          44479.0   \n",
       "29  01-JAN-18 00:00:00  5972116           13           44          48079.0   \n",
       "30  01-JAN-18 00:00:00  5972118           13           44          55246.0   \n",
       "31  01-JAN-18 00:00:00  5959105           13           44          60046.0   \n",
       "32  01-JAN-18 00:00:00  5966888           13           44          58846.0   \n",
       "33  01-JAN-18 00:00:00  5965960           13           44          56446.0   \n",
       "34  01-JAN-18 00:00:00  5972122           13           44          69528.0   \n",
       "35  01-JAN-18 00:00:00  5965964           13           44          70728.0   \n",
       "36  01-JAN-18 00:00:00  5966678           13           44          68328.0   \n",
       "37  01-JAN-18 00:00:00  5959109           13           44          74328.0   \n",
       "38  01-JAN-18 00:00:00  5959099           13           44          38432.0   \n",
       "39  01-JAN-18 00:00:00  5965956           13           44          42032.0   \n",
       "40  01-JAN-18 00:00:00  5956265           13           44          36632.0   \n",
       "41  01-JAN-18 00:00:00  5966676           13           44          61219.0   \n",
       "42  01-JAN-18 00:00:00  5965962           13           44          63619.0   \n",
       "43  01-JAN-18 00:00:00  5972120           13           44          62419.0   \n",
       "44  01-JAN-18 00:00:00  5958115           13           44          64775.0   \n",
       "45  01-JAN-18 00:00:00  5966890           13           44          65975.0   \n",
       "46  01-JAN-18 00:00:00  5959107           13           44          67175.0   \n",
       "47  01-JAN-18 00:00:00  5959111           14         7603          81482.0   \n",
       "48  01-JAN-18 00:00:00  5958119           14         7603          79682.0   \n",
       "49  01-JAN-18 00:00:00  5965966           14         7603          77882.0   \n",
       "\n",
       "    PLANNEDTIME_DEP  ACTUALTIME_ARR  ACTUALTIME_DEP  VEHICLEID LINEID ROUTEID  \\\n",
       "0           48030.0         48012.0         48012.0    2693211      1    1_37   \n",
       "1           54001.0         54023.0         54023.0    2693267      1    1_37   \n",
       "2           60001.0         59955.0         59955.0    2693263      1    1_37   \n",
       "3           58801.0         58771.0         58771.0    2693284      1    1_37   \n",
       "4           56401.0         56309.0         56323.0    2693209      1    1_37   \n",
       "5           70688.0         70663.0         70679.0    2693209      1    1_37   \n",
       "6           72488.0         72539.0         72539.0    2172293      1    1_37   \n",
       "7           74288.0         74173.0         74173.0    2693263      1    1_37   \n",
       "8           40187.0         40096.0         40111.0    2693211      1    1_37   \n",
       "9           38387.0         38271.0         38271.0    2693263      1    1_37   \n",
       "10          41987.0         42017.0         42035.0    2693209      1    1_37   \n",
       "11          36587.0         36521.0         36521.0    2693264      1    1_37   \n",
       "12          61178.0         61280.0         61280.0    2693267      1    1_37   \n",
       "13          63578.0         63390.0         63390.0    2693209      1    1_37   \n",
       "14          62378.0         62215.0         62238.0    2693211      1    1_37   \n",
       "15          65936.0         65944.0         65944.0    2693228      1    1_37   \n",
       "16          67136.0         67120.0         67120.0    2693263      1    1_37   \n",
       "17          76062.0         75913.0         75925.0    2693267      1    1_37   \n",
       "18          81462.0         81447.0         81447.0    2693263      1    1_37   \n",
       "19          79662.0         79676.0         79676.0    2172293      1    1_37   \n",
       "20          77862.0         77801.0         77801.0    2693209      1    1_37   \n",
       "21          83262.0         83279.0         83279.0    2693267      1    1_37   \n",
       "22          45679.0         45696.0         45714.0    2693263      1    1_37   \n",
       "23          52879.0         52815.0         52834.0    2693263      1    1_37   \n",
       "24          51679.0         51669.0         51669.0    2693284      1    1_37   \n",
       "25          49279.0         49187.0         49187.0    2693209      1    1_37   \n",
       "26          46879.0         46824.0         46835.0    2693267      1    1_37   \n",
       "27          43279.0         43237.0         43237.0    2693264      1    1_37   \n",
       "28          44479.0         44401.0         44401.0    2693284      1    1_37   \n",
       "29          48079.0         48058.0         48058.0    2693211      1    1_37   \n",
       "30          55246.0         55025.0         55025.0    2693211      1    1_37   \n",
       "31          60046.0         59983.0         59983.0    2693263      1    1_37   \n",
       "32          58846.0         58796.0         58803.0    2693284      1    1_37   \n",
       "33          56446.0         56346.0         56398.0    2693209      1    1_37   \n",
       "34          69528.0         69594.0         69594.0    2693211      1    1_37   \n",
       "35          70728.0         70721.0         70721.0    2693209      1    1_37   \n",
       "36          68328.0         68201.0         68201.0    2693267      1    1_37   \n",
       "37          74328.0         74190.0         74190.0    2693263      1    1_37   \n",
       "38          38432.0         38297.0         38297.0    2693263      1    1_37   \n",
       "39          42032.0         42055.0         42055.0    2693209      1    1_37   \n",
       "40          36632.0         36572.0         36584.0    2693264      1    1_37   \n",
       "41          61219.0         61302.0         61302.0    2693267      1    1_37   \n",
       "42          63619.0         63415.0         63415.0    2693209      1    1_37   \n",
       "43          62419.0         62265.0         62271.0    2693211      1    1_37   \n",
       "44          64775.0         64727.0         64727.0    2172293      1    1_37   \n",
       "45          65975.0         65971.0         65971.0    2693228      1    1_37   \n",
       "46          67175.0         67142.0         67142.0    2693263      1    1_37   \n",
       "47          81482.0         81458.0         81458.0    2693263      1    1_37   \n",
       "48          79682.0         79693.0         79693.0    2172293      1    1_37   \n",
       "49          77882.0         77820.0         77837.0    2693209      1    1_37   \n",
       "\n",
       "   DIRECTION  PLANNEDTIME_DEP_T  ACTUALTIME_DEP_T  \n",
       "0         1             47400.0           47427.0  \n",
       "1         1             53400.0           53410.0  \n",
       "2         1             59400.0           59426.0  \n",
       "3         1             58200.0           58220.0  \n",
       "4         1             55800.0           55807.0  \n",
       "5         1             70200.0           70236.0  \n",
       "6         1             72000.0           72023.0  \n",
       "7         1             73800.0           73806.0  \n",
       "8         1             39600.0           39681.0  \n",
       "9         1             37800.0           37810.0  \n",
       "10        1             41400.0           41482.0  \n",
       "11        1             36000.0           36048.0  \n",
       "12        1             60600.0           60697.0  \n",
       "13        1             63000.0           63022.0  \n",
       "14        1             61800.0           61767.0  \n",
       "15        1             65400.0           65427.0  \n",
       "16        1             66600.0           66613.0  \n",
       "17        1             75600.0           75529.0  \n",
       "18        1             81000.0           81010.0  \n",
       "19        1             79200.0           79215.0  \n",
       "20        1             77400.0           77434.0  \n",
       "21        1             82800.0           82826.0  \n",
       "22        1             45000.0           45026.0  \n",
       "23        1             52200.0           52217.0  \n",
       "24        1             51000.0           51043.0  \n",
       "25        1             48600.0           48666.0  \n",
       "26        1             46200.0           46208.0  \n",
       "27        1             42600.0           42633.0  \n",
       "28        1             43800.0           43822.0  \n",
       "29        1             47400.0           47427.0  \n",
       "30        1             54600.0           54271.0  \n",
       "31        1             59400.0           59426.0  \n",
       "32        1             58200.0           58220.0  \n",
       "33        1             55800.0           55807.0  \n",
       "34        1             69000.0           69008.0  \n",
       "35        1             70200.0           70236.0  \n",
       "36        1             67800.0           67784.0  \n",
       "37        1             73800.0           73806.0  \n",
       "38        1             37800.0           37810.0  \n",
       "39        1             41400.0           41482.0  \n",
       "40        1             36000.0           36048.0  \n",
       "41        1             60600.0           60697.0  \n",
       "42        1             63000.0           63022.0  \n",
       "43        1             61800.0           61767.0  \n",
       "44        1             64200.0           64247.0  \n",
       "45        1             65400.0           65427.0  \n",
       "46        1             66600.0           66613.0  \n",
       "47        1             81000.0           81010.0  \n",
       "48        1             79200.0           79215.0  \n",
       "49        1             77400.0           77434.0  "
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dd.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "6a9a5315",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tripid = merged_dd.loc[merged_dd[\"TRIPID\"] == 5972116]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "c29f6a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/team9/miniconda3/envs/comp47360py39/lib/python3.9/site-packages/dask/dataframe/core.py:7506: UserWarning: Insufficient elements for `head`. 80 elements requested, only 42 elements available. Try passing larger `npartitions` to `head`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>PROGRNUMBER</th>\n",
       "      <th>STOPPOINTID</th>\n",
       "      <th>PLANNEDTIME_ARR</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_ARR</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "      <th>VEHICLEID</th>\n",
       "      <th>LINEID</th>\n",
       "      <th>ROUTEID</th>\n",
       "      <th>DIRECTION</th>\n",
       "      <th>PLANNEDTIME_DEP_T</th>\n",
       "      <th>ACTUALTIME_DEP_T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>48030.0</td>\n",
       "      <td>48030.0</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>48079.0</td>\n",
       "      <td>48079.0</td>\n",
       "      <td>48058.0</td>\n",
       "      <td>48058.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>14</td>\n",
       "      <td>7603</td>\n",
       "      <td>48114.0</td>\n",
       "      <td>48114.0</td>\n",
       "      <td>48071.0</td>\n",
       "      <td>48071.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>15</td>\n",
       "      <td>45</td>\n",
       "      <td>48155.0</td>\n",
       "      <td>48155.0</td>\n",
       "      <td>48089.0</td>\n",
       "      <td>48100.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>16</td>\n",
       "      <td>46</td>\n",
       "      <td>48221.0</td>\n",
       "      <td>48221.0</td>\n",
       "      <td>48137.0</td>\n",
       "      <td>48137.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>17</td>\n",
       "      <td>47</td>\n",
       "      <td>48300.0</td>\n",
       "      <td>48300.0</td>\n",
       "      <td>48171.0</td>\n",
       "      <td>48171.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>18</td>\n",
       "      <td>48</td>\n",
       "      <td>48366.0</td>\n",
       "      <td>48366.0</td>\n",
       "      <td>48189.0</td>\n",
       "      <td>48189.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>19</td>\n",
       "      <td>49</td>\n",
       "      <td>48460.0</td>\n",
       "      <td>48460.0</td>\n",
       "      <td>48276.0</td>\n",
       "      <td>48276.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>21</td>\n",
       "      <td>52</td>\n",
       "      <td>48610.0</td>\n",
       "      <td>48610.0</td>\n",
       "      <td>48358.0</td>\n",
       "      <td>48378.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>22</td>\n",
       "      <td>265</td>\n",
       "      <td>48744.0</td>\n",
       "      <td>48744.0</td>\n",
       "      <td>48463.0</td>\n",
       "      <td>48463.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>23</td>\n",
       "      <td>271</td>\n",
       "      <td>48924.0</td>\n",
       "      <td>48924.0</td>\n",
       "      <td>48553.0</td>\n",
       "      <td>48584.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>24</td>\n",
       "      <td>340</td>\n",
       "      <td>49058.0</td>\n",
       "      <td>49058.0</td>\n",
       "      <td>48724.0</td>\n",
       "      <td>48745.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>25</td>\n",
       "      <td>350</td>\n",
       "      <td>49196.0</td>\n",
       "      <td>49196.0</td>\n",
       "      <td>48794.0</td>\n",
       "      <td>48794.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>27</td>\n",
       "      <td>352</td>\n",
       "      <td>49363.0</td>\n",
       "      <td>49363.0</td>\n",
       "      <td>48943.0</td>\n",
       "      <td>48943.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>28</td>\n",
       "      <td>353</td>\n",
       "      <td>49414.0</td>\n",
       "      <td>49414.0</td>\n",
       "      <td>48987.0</td>\n",
       "      <td>49002.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>29</td>\n",
       "      <td>354</td>\n",
       "      <td>49467.0</td>\n",
       "      <td>49467.0</td>\n",
       "      <td>49033.0</td>\n",
       "      <td>49033.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>30</td>\n",
       "      <td>355</td>\n",
       "      <td>49524.0</td>\n",
       "      <td>49524.0</td>\n",
       "      <td>49058.0</td>\n",
       "      <td>49150.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>31</td>\n",
       "      <td>356</td>\n",
       "      <td>49628.0</td>\n",
       "      <td>49628.0</td>\n",
       "      <td>49210.0</td>\n",
       "      <td>49217.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>32</td>\n",
       "      <td>357</td>\n",
       "      <td>49700.0</td>\n",
       "      <td>49700.0</td>\n",
       "      <td>49274.0</td>\n",
       "      <td>49274.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1656</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>33</td>\n",
       "      <td>390</td>\n",
       "      <td>49755.0</td>\n",
       "      <td>49755.0</td>\n",
       "      <td>49310.0</td>\n",
       "      <td>49310.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1685</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>34</td>\n",
       "      <td>372</td>\n",
       "      <td>49806.0</td>\n",
       "      <td>49806.0</td>\n",
       "      <td>49347.0</td>\n",
       "      <td>49347.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1870</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>35</td>\n",
       "      <td>373</td>\n",
       "      <td>49847.0</td>\n",
       "      <td>49847.0</td>\n",
       "      <td>49364.0</td>\n",
       "      <td>49364.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1924</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>37</td>\n",
       "      <td>375</td>\n",
       "      <td>49948.0</td>\n",
       "      <td>49948.0</td>\n",
       "      <td>49466.0</td>\n",
       "      <td>49466.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2110</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>38</td>\n",
       "      <td>2804</td>\n",
       "      <td>50047.0</td>\n",
       "      <td>50047.0</td>\n",
       "      <td>49548.0</td>\n",
       "      <td>49548.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2133</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>39</td>\n",
       "      <td>376</td>\n",
       "      <td>50088.0</td>\n",
       "      <td>50088.0</td>\n",
       "      <td>49559.0</td>\n",
       "      <td>49559.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2725</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>1</td>\n",
       "      <td>226</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "      <td>47427.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2754</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>2</td>\n",
       "      <td>228</td>\n",
       "      <td>47451.0</td>\n",
       "      <td>47451.0</td>\n",
       "      <td>47461.0</td>\n",
       "      <td>47461.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2963</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>4</td>\n",
       "      <td>227</td>\n",
       "      <td>47562.0</td>\n",
       "      <td>47562.0</td>\n",
       "      <td>47596.0</td>\n",
       "      <td>47596.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2993</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>5</td>\n",
       "      <td>230</td>\n",
       "      <td>47604.0</td>\n",
       "      <td>47604.0</td>\n",
       "      <td>47620.0</td>\n",
       "      <td>47620.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>7</td>\n",
       "      <td>1641</td>\n",
       "      <td>47755.0</td>\n",
       "      <td>47755.0</td>\n",
       "      <td>47800.0</td>\n",
       "      <td>47800.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3224</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>8</td>\n",
       "      <td>1642</td>\n",
       "      <td>47814.0</td>\n",
       "      <td>47814.0</td>\n",
       "      <td>47833.0</td>\n",
       "      <td>47833.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3430</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>10</td>\n",
       "      <td>214</td>\n",
       "      <td>47924.0</td>\n",
       "      <td>47924.0</td>\n",
       "      <td>47915.0</td>\n",
       "      <td>47915.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3459</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>11</td>\n",
       "      <td>4432</td>\n",
       "      <td>47975.0</td>\n",
       "      <td>47975.0</td>\n",
       "      <td>47966.0</td>\n",
       "      <td>47966.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3661</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>41</td>\n",
       "      <td>378</td>\n",
       "      <td>50170.0</td>\n",
       "      <td>50170.0</td>\n",
       "      <td>49623.0</td>\n",
       "      <td>49623.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3689</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>42</td>\n",
       "      <td>380</td>\n",
       "      <td>50199.0</td>\n",
       "      <td>50199.0</td>\n",
       "      <td>49653.0</td>\n",
       "      <td>49653.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168280</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>3</td>\n",
       "      <td>229</td>\n",
       "      <td>47493.0</td>\n",
       "      <td>47493.0</td>\n",
       "      <td>47483.0</td>\n",
       "      <td>47554.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168281</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>6</td>\n",
       "      <td>231</td>\n",
       "      <td>47661.0</td>\n",
       "      <td>47661.0</td>\n",
       "      <td>47726.0</td>\n",
       "      <td>47740.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168282</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>9</td>\n",
       "      <td>213</td>\n",
       "      <td>47874.0</td>\n",
       "      <td>47874.0</td>\n",
       "      <td>47870.0</td>\n",
       "      <td>47892.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168283</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>20</td>\n",
       "      <td>51</td>\n",
       "      <td>48557.0</td>\n",
       "      <td>48557.0</td>\n",
       "      <td>48307.0</td>\n",
       "      <td>48319.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168284</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>26</td>\n",
       "      <td>351</td>\n",
       "      <td>49310.0</td>\n",
       "      <td>49310.0</td>\n",
       "      <td>48906.0</td>\n",
       "      <td>48921.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168285</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>36</td>\n",
       "      <td>374</td>\n",
       "      <td>49882.0</td>\n",
       "      <td>49882.0</td>\n",
       "      <td>49382.0</td>\n",
       "      <td>49416.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168286</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>40</td>\n",
       "      <td>377</td>\n",
       "      <td>50119.0</td>\n",
       "      <td>50119.0</td>\n",
       "      <td>49580.0</td>\n",
       "      <td>49580.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              DAYOFSERVICE   TRIPID  PROGRNUMBER  STOPPOINTID  \\\n",
       "0       01-JAN-18 00:00:00  5972116           12          119   \n",
       "29      01-JAN-18 00:00:00  5972116           13           44   \n",
       "57      01-JAN-18 00:00:00  5972116           14         7603   \n",
       "232     01-JAN-18 00:00:00  5972116           15           45   \n",
       "261     01-JAN-18 00:00:00  5972116           16           46   \n",
       "289     01-JAN-18 00:00:00  5972116           17           47   \n",
       "471     01-JAN-18 00:00:00  5972116           18           48   \n",
       "499     01-JAN-18 00:00:00  5972116           19           49   \n",
       "709     01-JAN-18 00:00:00  5972116           21           52   \n",
       "737     01-JAN-18 00:00:00  5972116           22          265   \n",
       "765     01-JAN-18 00:00:00  5972116           23          271   \n",
       "939     01-JAN-18 00:00:00  5972116           24          340   \n",
       "969     01-JAN-18 00:00:00  5972116           25          350   \n",
       "1176    01-JAN-18 00:00:00  5972116           27          352   \n",
       "1204    01-JAN-18 00:00:00  5972116           28          353   \n",
       "1389    01-JAN-18 00:00:00  5972116           29          354   \n",
       "1418    01-JAN-18 00:00:00  5972116           30          355   \n",
       "1444    01-JAN-18 00:00:00  5972116           31          356   \n",
       "1630    01-JAN-18 00:00:00  5972116           32          357   \n",
       "1656    01-JAN-18 00:00:00  5972116           33          390   \n",
       "1685    01-JAN-18 00:00:00  5972116           34          372   \n",
       "1870    01-JAN-18 00:00:00  5972116           35          373   \n",
       "1924    01-JAN-18 00:00:00  5972116           37          375   \n",
       "2110    01-JAN-18 00:00:00  5972116           38         2804   \n",
       "2133    01-JAN-18 00:00:00  5972116           39          376   \n",
       "2725    01-JAN-18 00:00:00  5972116            1          226   \n",
       "2754    01-JAN-18 00:00:00  5972116            2          228   \n",
       "2963    01-JAN-18 00:00:00  5972116            4          227   \n",
       "2993    01-JAN-18 00:00:00  5972116            5          230   \n",
       "3197    01-JAN-18 00:00:00  5972116            7         1641   \n",
       "3224    01-JAN-18 00:00:00  5972116            8         1642   \n",
       "3430    01-JAN-18 00:00:00  5972116           10          214   \n",
       "3459    01-JAN-18 00:00:00  5972116           11         4432   \n",
       "3661    01-JAN-18 00:00:00  5972116           41          378   \n",
       "3689    01-JAN-18 00:00:00  5972116           42          380   \n",
       "168280  01-JAN-18 00:00:00  5972116            3          229   \n",
       "168281  01-JAN-18 00:00:00  5972116            6          231   \n",
       "168282  01-JAN-18 00:00:00  5972116            9          213   \n",
       "168283  01-JAN-18 00:00:00  5972116           20           51   \n",
       "168284  01-JAN-18 00:00:00  5972116           26          351   \n",
       "168285  01-JAN-18 00:00:00  5972116           36          374   \n",
       "168286  01-JAN-18 00:00:00  5972116           40          377   \n",
       "\n",
       "        PLANNEDTIME_ARR  PLANNEDTIME_DEP  ACTUALTIME_ARR  ACTUALTIME_DEP  \\\n",
       "0               48030.0          48030.0         48012.0         48012.0   \n",
       "29              48079.0          48079.0         48058.0         48058.0   \n",
       "57              48114.0          48114.0         48071.0         48071.0   \n",
       "232             48155.0          48155.0         48089.0         48100.0   \n",
       "261             48221.0          48221.0         48137.0         48137.0   \n",
       "289             48300.0          48300.0         48171.0         48171.0   \n",
       "471             48366.0          48366.0         48189.0         48189.0   \n",
       "499             48460.0          48460.0         48276.0         48276.0   \n",
       "709             48610.0          48610.0         48358.0         48378.0   \n",
       "737             48744.0          48744.0         48463.0         48463.0   \n",
       "765             48924.0          48924.0         48553.0         48584.0   \n",
       "939             49058.0          49058.0         48724.0         48745.0   \n",
       "969             49196.0          49196.0         48794.0         48794.0   \n",
       "1176            49363.0          49363.0         48943.0         48943.0   \n",
       "1204            49414.0          49414.0         48987.0         49002.0   \n",
       "1389            49467.0          49467.0         49033.0         49033.0   \n",
       "1418            49524.0          49524.0         49058.0         49150.0   \n",
       "1444            49628.0          49628.0         49210.0         49217.0   \n",
       "1630            49700.0          49700.0         49274.0         49274.0   \n",
       "1656            49755.0          49755.0         49310.0         49310.0   \n",
       "1685            49806.0          49806.0         49347.0         49347.0   \n",
       "1870            49847.0          49847.0         49364.0         49364.0   \n",
       "1924            49948.0          49948.0         49466.0         49466.0   \n",
       "2110            50047.0          50047.0         49548.0         49548.0   \n",
       "2133            50088.0          50088.0         49559.0         49559.0   \n",
       "2725            47400.0          47400.0         47427.0         47427.0   \n",
       "2754            47451.0          47451.0         47461.0         47461.0   \n",
       "2963            47562.0          47562.0         47596.0         47596.0   \n",
       "2993            47604.0          47604.0         47620.0         47620.0   \n",
       "3197            47755.0          47755.0         47800.0         47800.0   \n",
       "3224            47814.0          47814.0         47833.0         47833.0   \n",
       "3430            47924.0          47924.0         47915.0         47915.0   \n",
       "3459            47975.0          47975.0         47966.0         47966.0   \n",
       "3661            50170.0          50170.0         49623.0         49623.0   \n",
       "3689            50199.0          50199.0         49653.0         49653.0   \n",
       "168280          47493.0          47493.0         47483.0         47554.0   \n",
       "168281          47661.0          47661.0         47726.0         47740.0   \n",
       "168282          47874.0          47874.0         47870.0         47892.0   \n",
       "168283          48557.0          48557.0         48307.0         48319.0   \n",
       "168284          49310.0          49310.0         48906.0         48921.0   \n",
       "168285          49882.0          49882.0         49382.0         49416.0   \n",
       "168286          50119.0          50119.0         49580.0         49580.0   \n",
       "\n",
       "        VEHICLEID LINEID ROUTEID DIRECTION  PLANNEDTIME_DEP_T  \\\n",
       "0         2693211      1    1_37        1             47400.0   \n",
       "29        2693211      1    1_37        1             47400.0   \n",
       "57        2693211      1    1_37        1             47400.0   \n",
       "232       2693211      1    1_37        1             47400.0   \n",
       "261       2693211      1    1_37        1             47400.0   \n",
       "289       2693211      1    1_37        1             47400.0   \n",
       "471       2693211      1    1_37        1             47400.0   \n",
       "499       2693211      1    1_37        1             47400.0   \n",
       "709       2693211      1    1_37        1             47400.0   \n",
       "737       2693211      1    1_37        1             47400.0   \n",
       "765       2693211      1    1_37        1             47400.0   \n",
       "939       2693211      1    1_37        1             47400.0   \n",
       "969       2693211      1    1_37        1             47400.0   \n",
       "1176      2693211      1    1_37        1             47400.0   \n",
       "1204      2693211      1    1_37        1             47400.0   \n",
       "1389      2693211      1    1_37        1             47400.0   \n",
       "1418      2693211      1    1_37        1             47400.0   \n",
       "1444      2693211      1    1_37        1             47400.0   \n",
       "1630      2693211      1    1_37        1             47400.0   \n",
       "1656      2693211      1    1_37        1             47400.0   \n",
       "1685      2693211      1    1_37        1             47400.0   \n",
       "1870      2693211      1    1_37        1             47400.0   \n",
       "1924      2693211      1    1_37        1             47400.0   \n",
       "2110      2693211      1    1_37        1             47400.0   \n",
       "2133      2693211      1    1_37        1             47400.0   \n",
       "2725      2693211      1    1_37        1             47400.0   \n",
       "2754      2693211      1    1_37        1             47400.0   \n",
       "2963      2693211      1    1_37        1             47400.0   \n",
       "2993      2693211      1    1_37        1             47400.0   \n",
       "3197      2693211      1    1_37        1             47400.0   \n",
       "3224      2693211      1    1_37        1             47400.0   \n",
       "3430      2693211      1    1_37        1             47400.0   \n",
       "3459      2693211      1    1_37        1             47400.0   \n",
       "3661      2693211      1    1_37        1             47400.0   \n",
       "3689      2693211      1    1_37        1             47400.0   \n",
       "168280    2693211      1    1_37        1             47400.0   \n",
       "168281    2693211      1    1_37        1             47400.0   \n",
       "168282    2693211      1    1_37        1             47400.0   \n",
       "168283    2693211      1    1_37        1             47400.0   \n",
       "168284    2693211      1    1_37        1             47400.0   \n",
       "168285    2693211      1    1_37        1             47400.0   \n",
       "168286    2693211      1    1_37        1             47400.0   \n",
       "\n",
       "        ACTUALTIME_DEP_T  \n",
       "0                47427.0  \n",
       "29               47427.0  \n",
       "57               47427.0  \n",
       "232              47427.0  \n",
       "261              47427.0  \n",
       "289              47427.0  \n",
       "471              47427.0  \n",
       "499              47427.0  \n",
       "709              47427.0  \n",
       "737              47427.0  \n",
       "765              47427.0  \n",
       "939              47427.0  \n",
       "969              47427.0  \n",
       "1176             47427.0  \n",
       "1204             47427.0  \n",
       "1389             47427.0  \n",
       "1418             47427.0  \n",
       "1444             47427.0  \n",
       "1630             47427.0  \n",
       "1656             47427.0  \n",
       "1685             47427.0  \n",
       "1870             47427.0  \n",
       "1924             47427.0  \n",
       "2110             47427.0  \n",
       "2133             47427.0  \n",
       "2725             47427.0  \n",
       "2754             47427.0  \n",
       "2963             47427.0  \n",
       "2993             47427.0  \n",
       "3197             47427.0  \n",
       "3224             47427.0  \n",
       "3430             47427.0  \n",
       "3459             47427.0  \n",
       "3661             47427.0  \n",
       "3689             47427.0  \n",
       "168280           47427.0  \n",
       "168281           47427.0  \n",
       "168282           47427.0  \n",
       "168283           47427.0  \n",
       "168284           47427.0  \n",
       "168285           47427.0  \n",
       "168286           47427.0  "
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tripid.head(80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314f109b",
   "metadata": {},
   "source": [
    "- latest update column all at wrong day so completely useless\n",
    "- prognumber is each stop it was just at, beginning at starting point of route\n",
    "    - no real point of keeping end point of route\n",
    "- some questions of reliability of the data because I found instances when the reported arrival time in the trips data was earlier than some of the stops arrival times\n",
    "- date seems insignificant so will be converting it to days of the week to give bigger volume of data for models\n",
    "- vehicle stats seem insignificantas they only refer to the stats of the vehicle on the day and dont give context to how many times the drivers ahve changed\n",
    "\n",
    "\n",
    "##### kernel could not compute so i had to go backwards and remove them before entering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "244775e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_dd = merged_dd.drop({\"LASTUPDATE_V\", \"MINUTES\", \"DISTANCE\", \"ACTUALTIME_ARR_T\", \"PLANNEDTIME_ARR_T\", \"LASTUPDATE\"}, axis=1).compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "e4883b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8759 entries, 0 to 8758\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   dt            8759 non-null   int64         \n",
      " 1   temp          8759 non-null   int64         \n",
      " 2   visibility    8759 non-null   int64         \n",
      " 3   wind_speed    8759 non-null   int64         \n",
      " 4   weather_main  8759 non-null   int64         \n",
      " 5   date          8759 non-null   datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(5)\n",
      "memory usage: 410.7 KB\n"
     ]
    }
   ],
   "source": [
    "weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "7a90495e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>temp</th>\n",
       "      <th>visibility</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>weather_main</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1514768400</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-01 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1514772000</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1514775600</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1514779200</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1514782800</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01 05:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  temp  visibility  wind_speed  weather_main                date\n",
       "0  1514768400     2          10           6             1 2018-01-01 01:00:00\n",
       "1  1514772000     2          10           6             0 2018-01-01 02:00:00\n",
       "2  1514775600     2          10           6             0 2018-01-01 03:00:00\n",
       "3  1514779200     2          10           6             0 2018-01-01 04:00:00\n",
       "4  1514782800     2          10           5             0 2018-01-01 05:00:00"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "2699b98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['temp'] = weather['temp'].astype('int8')\n",
    "weather['visibility'] = weather['visibility'].astype('int8')\n",
    "weather['wind_speed'] = weather['wind_speed'].astype('int8')\n",
    "#weather['weather_description'] = weather['weather_description'].astype('int8')\n",
    "weather['weather_main'] = weather['weather_main'].astype('int8')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "4d864b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8759 entries, 0 to 8758\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   dt            8759 non-null   int64         \n",
      " 1   temp          8759 non-null   int8          \n",
      " 2   visibility    8759 non-null   int8          \n",
      " 3   wind_speed    8759 non-null   int8          \n",
      " 4   weather_main  8759 non-null   int8          \n",
      " 5   date          8759 non-null   datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(1), int8(4)\n",
      "memory usage: 171.2 KB\n"
     ]
    }
   ],
   "source": [
    "weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd56f016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "953eef03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>PROGRNUMBER</th>\n",
       "      <th>STOPPOINTID</th>\n",
       "      <th>PLANNEDTIME_ARR</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_ARR</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "      <th>VEHICLEID</th>\n",
       "      <th>LINEID</th>\n",
       "      <th>ROUTEID</th>\n",
       "      <th>DIRECTION</th>\n",
       "      <th>PLANNEDTIME_DEP_T</th>\n",
       "      <th>ACTUALTIME_DEP_T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>48030.0</td>\n",
       "      <td>48030.0</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5966674</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>54001.0</td>\n",
       "      <td>54001.0</td>\n",
       "      <td>54023.0</td>\n",
       "      <td>54023.0</td>\n",
       "      <td>2693267</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>53400.0</td>\n",
       "      <td>53410.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5959105</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>60001.0</td>\n",
       "      <td>60001.0</td>\n",
       "      <td>59955.0</td>\n",
       "      <td>59955.0</td>\n",
       "      <td>2693263</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>59400.0</td>\n",
       "      <td>59426.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         DAYOFSERVICE   TRIPID  PROGRNUMBER  STOPPOINTID  PLANNEDTIME_ARR  \\\n",
       "0  01-JAN-18 00:00:00  5972116           12          119          48030.0   \n",
       "1  01-JAN-18 00:00:00  5966674           12          119          54001.0   \n",
       "2  01-JAN-18 00:00:00  5959105           12          119          60001.0   \n",
       "\n",
       "   PLANNEDTIME_DEP  ACTUALTIME_ARR  ACTUALTIME_DEP  VEHICLEID LINEID ROUTEID  \\\n",
       "0          48030.0         48012.0         48012.0    2693211      1    1_37   \n",
       "1          54001.0         54023.0         54023.0    2693267      1    1_37   \n",
       "2          60001.0         59955.0         59955.0    2693263      1    1_37   \n",
       "\n",
       "  DIRECTION  PLANNEDTIME_DEP_T  ACTUALTIME_DEP_T  \n",
       "0        1             47400.0           47427.0  \n",
       "1        1             53400.0           53410.0  \n",
       "2        1             59400.0           59426.0  "
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dd.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "6953b619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DAYOFSERVICE           object\n",
       "TRIPID                  int32\n",
       "PROGRNUMBER             int32\n",
       "STOPPOINTID             int32\n",
       "PLANNEDTIME_ARR       float64\n",
       "PLANNEDTIME_DEP       float64\n",
       "ACTUALTIME_ARR        float64\n",
       "ACTUALTIME_DEP        float64\n",
       "VEHICLEID               int32\n",
       "LINEID               category\n",
       "ROUTEID              category\n",
       "DIRECTION            category\n",
       "PLANNEDTIME_DEP_T     float64\n",
       "ACTUALTIME_DEP_T      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dd.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "e9788071",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dd['PROGRNUMBER'] = merged_dd['PROGRNUMBER'].astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "cbb1970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dd['DIRECTION'] = merged_dd['DIRECTION'].astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "8e07c5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_dd = merged_dd.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "acda79d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DAYOFSERVICE           object\n",
       "TRIPID                  int32\n",
       "PROGRNUMBER              int8\n",
       "STOPPOINTID             int32\n",
       "PLANNEDTIME_ARR       float64\n",
       "PLANNEDTIME_DEP       float64\n",
       "ACTUALTIME_ARR        float64\n",
       "ACTUALTIME_DEP        float64\n",
       "VEHICLEID               int32\n",
       "LINEID               category\n",
       "ROUTEID              category\n",
       "DIRECTION                int8\n",
       "PLANNEDTIME_DEP_T     float64\n",
       "ACTUALTIME_DEP_T      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dd.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "72f2ea85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>PROGRNUMBER</th>\n",
       "      <th>STOPPOINTID</th>\n",
       "      <th>PLANNEDTIME_ARR</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_ARR</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "      <th>VEHICLEID</th>\n",
       "      <th>LINEID</th>\n",
       "      <th>ROUTEID</th>\n",
       "      <th>DIRECTION</th>\n",
       "      <th>PLANNEDTIME_DEP_T</th>\n",
       "      <th>ACTUALTIME_DEP_T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>48030.0</td>\n",
       "      <td>48030.0</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5966674</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>54001.0</td>\n",
       "      <td>54001.0</td>\n",
       "      <td>54023.0</td>\n",
       "      <td>54023.0</td>\n",
       "      <td>2693267</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>53400.0</td>\n",
       "      <td>53410.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5959105</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>60001.0</td>\n",
       "      <td>60001.0</td>\n",
       "      <td>59955.0</td>\n",
       "      <td>59955.0</td>\n",
       "      <td>2693263</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>59400.0</td>\n",
       "      <td>59426.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         DAYOFSERVICE   TRIPID  PROGRNUMBER  STOPPOINTID  PLANNEDTIME_ARR  \\\n",
       "0  01-JAN-18 00:00:00  5972116           12          119          48030.0   \n",
       "1  01-JAN-18 00:00:00  5966674           12          119          54001.0   \n",
       "2  01-JAN-18 00:00:00  5959105           12          119          60001.0   \n",
       "\n",
       "   PLANNEDTIME_DEP  ACTUALTIME_ARR  ACTUALTIME_DEP  VEHICLEID LINEID ROUTEID  \\\n",
       "0          48030.0         48012.0         48012.0    2693211      1    1_37   \n",
       "1          54001.0         54023.0         54023.0    2693267      1    1_37   \n",
       "2          60001.0         59955.0         59955.0    2693263      1    1_37   \n",
       "\n",
       "   DIRECTION  PLANNEDTIME_DEP_T  ACTUALTIME_DEP_T  \n",
       "0          1            47400.0           47427.0  \n",
       "1          1            53400.0           53410.0  \n",
       "2          1            59400.0           59426.0  "
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dd.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566c816c",
   "metadata": {},
   "source": [
    "###### remove nan values from buses that never left\n",
    "- cant change the NaN values in dask so must do it in pandas when in individual files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "25f1a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_dd[\"PLANNEDTIME_DEP_T\"] = merged_dd[\"PLANNEDTIME_DEP_T\"].replace(regex={\"NaN\": 0})\n",
    "merged_dd[\"PLANNEDTIME_DEP_T\"] = merged_dd[\"PLANNEDTIME_DEP_T\"].astype('int32').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "8d89e1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dd[\"PLANNEDTIME_ARR\"] = merged_dd[\"PLANNEDTIME_ARR\"].astype('int32').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "0530cc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dd[\"PLANNEDTIME_DEP\"] = merged_dd[\"PLANNEDTIME_DEP\"].astype('int32').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "a42146b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DAYOFSERVICE           object\n",
       "TRIPID                  int32\n",
       "PROGRNUMBER              int8\n",
       "STOPPOINTID             int32\n",
       "PLANNEDTIME_ARR         int32\n",
       "PLANNEDTIME_DEP         int32\n",
       "ACTUALTIME_ARR        float64\n",
       "ACTUALTIME_DEP        float64\n",
       "VEHICLEID               int32\n",
       "LINEID               category\n",
       "ROUTEID              category\n",
       "DIRECTION                int8\n",
       "PLANNEDTIME_DEP_T       int32\n",
       "ACTUALTIME_DEP_T      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dd.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "07d287c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>PROGRNUMBER</th>\n",
       "      <th>STOPPOINTID</th>\n",
       "      <th>PLANNEDTIME_ARR</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_ARR</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "      <th>VEHICLEID</th>\n",
       "      <th>LINEID</th>\n",
       "      <th>ROUTEID</th>\n",
       "      <th>DIRECTION</th>\n",
       "      <th>PLANNEDTIME_DEP_T</th>\n",
       "      <th>ACTUALTIME_DEP_T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>48030</td>\n",
       "      <td>48030</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400</td>\n",
       "      <td>47427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5966674</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>54001</td>\n",
       "      <td>54001</td>\n",
       "      <td>54023.0</td>\n",
       "      <td>54023.0</td>\n",
       "      <td>2693267</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>53400</td>\n",
       "      <td>53410.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5959105</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>60001</td>\n",
       "      <td>60001</td>\n",
       "      <td>59955.0</td>\n",
       "      <td>59955.0</td>\n",
       "      <td>2693263</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>59400</td>\n",
       "      <td>59426.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         DAYOFSERVICE   TRIPID  PROGRNUMBER  STOPPOINTID  PLANNEDTIME_ARR  \\\n",
       "0  01-JAN-18 00:00:00  5972116           12          119            48030   \n",
       "1  01-JAN-18 00:00:00  5966674           12          119            54001   \n",
       "2  01-JAN-18 00:00:00  5959105           12          119            60001   \n",
       "\n",
       "   PLANNEDTIME_DEP  ACTUALTIME_ARR  ACTUALTIME_DEP  VEHICLEID LINEID ROUTEID  \\\n",
       "0            48030         48012.0         48012.0    2693211      1    1_37   \n",
       "1            54001         54023.0         54023.0    2693267      1    1_37   \n",
       "2            60001         59955.0         59955.0    2693263      1    1_37   \n",
       "\n",
       "   DIRECTION  PLANNEDTIME_DEP_T  ACTUALTIME_DEP_T  \n",
       "0          1              47400           47427.0  \n",
       "1          1              53400           53410.0  \n",
       "2          1              59400           59426.0  "
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dd.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "3ec2a636",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_dd = merged_dd.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "cd67bf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_from_epoch(x):\n",
    "    y = dt.datetime.fromtimestamp (x).strftime('%Y-%m-%d')\n",
    "    return y   \n",
    "\n",
    "weather['DAYOFSERVICE'] = weather['dt'].apply(lambda x: date_from_epoch(x))\n",
    "\n",
    "def round_epoch_hour(x):\n",
    "    day = x // 86400\n",
    "    day = day * 86400\n",
    "    time = x - day\n",
    "    round_time = time // 3600\n",
    "    return round_time\n",
    "\n",
    "weather['HOUR_DEPARTURE'] = weather['dt'].apply(lambda x: round_epoch_hour(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "50e074c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>temp</th>\n",
       "      <th>visibility</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>weather_main</th>\n",
       "      <th>date</th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>HOUR_DEPARTURE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1514768400</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-01 01:00:00</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1514772000</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01 02:00:00</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1514775600</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01 03:00:00</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1514779200</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01 04:00:00</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1514782800</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01 05:00:00</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  temp  visibility  wind_speed  weather_main                date  \\\n",
       "0  1514768400     2          10           6             1 2018-01-01 01:00:00   \n",
       "1  1514772000     2          10           6             0 2018-01-01 02:00:00   \n",
       "2  1514775600     2          10           6             0 2018-01-01 03:00:00   \n",
       "3  1514779200     2          10           6             0 2018-01-01 04:00:00   \n",
       "4  1514782800     2          10           5             0 2018-01-01 05:00:00   \n",
       "\n",
       "  DAYOFSERVICE  HOUR_DEPARTURE  \n",
       "0   2018-01-01               1  \n",
       "1   2018-01-01               2  \n",
       "2   2018-01-01               3  \n",
       "3   2018-01-01               4  \n",
       "4   2018-01-01               5  "
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "50c8c791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/team9/miniconda3/envs/comp47360py39/lib/python3.9/site-packages/dask/dataframe/core.py:3974: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('ACTUALTIME_DEP', 'float64'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    }
   ],
   "source": [
    "merged_dd['HOUR_DEPARTURE'] = merged_dd['ACTUALTIME_DEP'].apply(lambda row: row//3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "b215ab69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>PROGRNUMBER</th>\n",
       "      <th>STOPPOINTID</th>\n",
       "      <th>PLANNEDTIME_ARR</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_ARR</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "      <th>VEHICLEID</th>\n",
       "      <th>LINEID</th>\n",
       "      <th>ROUTEID</th>\n",
       "      <th>DIRECTION</th>\n",
       "      <th>PLANNEDTIME_DEP_T</th>\n",
       "      <th>ACTUALTIME_DEP_T</th>\n",
       "      <th>HOUR_DEPARTURE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5972116</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>48030</td>\n",
       "      <td>48030</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400</td>\n",
       "      <td>47427.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5966674</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>54001</td>\n",
       "      <td>54001</td>\n",
       "      <td>54023.0</td>\n",
       "      <td>54023.0</td>\n",
       "      <td>2693267</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>53400</td>\n",
       "      <td>53410.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5959105</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>60001</td>\n",
       "      <td>60001</td>\n",
       "      <td>59955.0</td>\n",
       "      <td>59955.0</td>\n",
       "      <td>2693263</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>59400</td>\n",
       "      <td>59426.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5966888</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>58801</td>\n",
       "      <td>58801</td>\n",
       "      <td>58771.0</td>\n",
       "      <td>58771.0</td>\n",
       "      <td>2693284</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>58200</td>\n",
       "      <td>58220.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01-JAN-18 00:00:00</td>\n",
       "      <td>5965960</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>56401</td>\n",
       "      <td>56401</td>\n",
       "      <td>56309.0</td>\n",
       "      <td>56323.0</td>\n",
       "      <td>2693209</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>55800</td>\n",
       "      <td>55807.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         DAYOFSERVICE   TRIPID  PROGRNUMBER  STOPPOINTID  PLANNEDTIME_ARR  \\\n",
       "0  01-JAN-18 00:00:00  5972116           12          119            48030   \n",
       "1  01-JAN-18 00:00:00  5966674           12          119            54001   \n",
       "2  01-JAN-18 00:00:00  5959105           12          119            60001   \n",
       "3  01-JAN-18 00:00:00  5966888           12          119            58801   \n",
       "4  01-JAN-18 00:00:00  5965960           12          119            56401   \n",
       "\n",
       "   PLANNEDTIME_DEP  ACTUALTIME_ARR  ACTUALTIME_DEP  VEHICLEID LINEID ROUTEID  \\\n",
       "0            48030         48012.0         48012.0    2693211      1    1_37   \n",
       "1            54001         54023.0         54023.0    2693267      1    1_37   \n",
       "2            60001         59955.0         59955.0    2693263      1    1_37   \n",
       "3            58801         58771.0         58771.0    2693284      1    1_37   \n",
       "4            56401         56309.0         56323.0    2693209      1    1_37   \n",
       "\n",
       "   DIRECTION  PLANNEDTIME_DEP_T  ACTUALTIME_DEP_T  HOUR_DEPARTURE  \n",
       "0          1              47400           47427.0            13.0  \n",
       "1          1              53400           53410.0            15.0  \n",
       "2          1              59400           59426.0            16.0  \n",
       "3          1              58200           58220.0            16.0  \n",
       "4          1              55800           55807.0            15.0  "
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "181d2cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DAYOFSERVICE           object\n",
       "TRIPID                  int32\n",
       "PROGRNUMBER              int8\n",
       "STOPPOINTID             int32\n",
       "PLANNEDTIME_ARR         int32\n",
       "PLANNEDTIME_DEP         int32\n",
       "ACTUALTIME_ARR        float64\n",
       "ACTUALTIME_DEP        float64\n",
       "VEHICLEID               int32\n",
       "LINEID               category\n",
       "ROUTEID              category\n",
       "DIRECTION                int8\n",
       "PLANNEDTIME_DEP_T       int32\n",
       "ACTUALTIME_DEP_T      float64\n",
       "HOUR_DEPARTURE        float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dd.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "edcdd038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt                         int64\n",
       "temp                        int8\n",
       "visibility                  int8\n",
       "wind_speed                  int8\n",
       "weather_main                int8\n",
       "date              datetime64[ns]\n",
       "DAYOFSERVICE              object\n",
       "HOUR_DEPARTURE             int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "35bb0fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dd[\"DAYOFSERVICE\"] = merged_dd[\"DAYOFSERVICE\"].astype(\"datetime64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "6b5535d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change day of service to match \n",
    "merged_dd[\"DAYOFSERVICE\"] = merged_dd[\"DAYOFSERVICE\"].dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "0ae4ba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dd[\"DAYOFSERVICE\"] = merged_dd[\"DAYOFSERVICE\"].astype(\"datetime64\")\n",
    "merged_dd['DAYOFWEEK'] = merged_dd['DAYOFSERVICE'].dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "1e539fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>PROGRNUMBER</th>\n",
       "      <th>STOPPOINTID</th>\n",
       "      <th>PLANNEDTIME_ARR</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_ARR</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "      <th>VEHICLEID</th>\n",
       "      <th>LINEID</th>\n",
       "      <th>ROUTEID</th>\n",
       "      <th>DIRECTION</th>\n",
       "      <th>PLANNEDTIME_DEP_T</th>\n",
       "      <th>ACTUALTIME_DEP_T</th>\n",
       "      <th>HOUR_DEPARTURE</th>\n",
       "      <th>DAYOFWEEK</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=176</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>int32</td>\n",
       "      <td>int8</td>\n",
       "      <td>int32</td>\n",
       "      <td>int32</td>\n",
       "      <td>int32</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>int32</td>\n",
       "      <td>category[known]</td>\n",
       "      <td>category[known]</td>\n",
       "      <td>int8</td>\n",
       "      <td>int32</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: assign, 6161 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                   DAYOFSERVICE TRIPID PROGRNUMBER STOPPOINTID PLANNEDTIME_ARR PLANNEDTIME_DEP ACTUALTIME_ARR ACTUALTIME_DEP VEHICLEID           LINEID          ROUTEID DIRECTION PLANNEDTIME_DEP_T ACTUALTIME_DEP_T HOUR_DEPARTURE DAYOFWEEK\n",
       "npartitions=176                                                                                                                                                                                                                               \n",
       "                 datetime64[ns]  int32        int8       int32           int32           int32        float64        float64     int32  category[known]  category[known]      int8             int32          float64        float64     int64\n",
       "                            ...    ...         ...         ...             ...             ...            ...            ...       ...              ...              ...       ...               ...              ...            ...       ...\n",
       "...                         ...    ...         ...         ...             ...             ...            ...            ...       ...              ...              ...       ...               ...              ...            ...       ...\n",
       "                            ...    ...         ...         ...             ...             ...            ...            ...       ...              ...              ...       ...               ...              ...            ...       ...\n",
       "                            ...    ...         ...         ...             ...             ...            ...            ...       ...              ...              ...       ...               ...              ...            ...       ...\n",
       "Dask Name: assign, 6161 tasks"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "b43e5915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>PROGRNUMBER</th>\n",
       "      <th>STOPPOINTID</th>\n",
       "      <th>PLANNEDTIME_ARR</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_ARR</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "      <th>VEHICLEID</th>\n",
       "      <th>LINEID</th>\n",
       "      <th>ROUTEID</th>\n",
       "      <th>DIRECTION</th>\n",
       "      <th>PLANNEDTIME_DEP_T</th>\n",
       "      <th>ACTUALTIME_DEP_T</th>\n",
       "      <th>HOUR_DEPARTURE</th>\n",
       "      <th>DAYOFWEEK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>5972116</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>48030</td>\n",
       "      <td>48030</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400</td>\n",
       "      <td>47427.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>5966674</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>54001</td>\n",
       "      <td>54001</td>\n",
       "      <td>54023.0</td>\n",
       "      <td>54023.0</td>\n",
       "      <td>2693267</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>53400</td>\n",
       "      <td>53410.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>5959105</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>60001</td>\n",
       "      <td>60001</td>\n",
       "      <td>59955.0</td>\n",
       "      <td>59955.0</td>\n",
       "      <td>2693263</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>59400</td>\n",
       "      <td>59426.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>5966888</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>58801</td>\n",
       "      <td>58801</td>\n",
       "      <td>58771.0</td>\n",
       "      <td>58771.0</td>\n",
       "      <td>2693284</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>58200</td>\n",
       "      <td>58220.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>5965960</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>56401</td>\n",
       "      <td>56401</td>\n",
       "      <td>56309.0</td>\n",
       "      <td>56323.0</td>\n",
       "      <td>2693209</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>55800</td>\n",
       "      <td>55807.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  DAYOFSERVICE   TRIPID  PROGRNUMBER  STOPPOINTID  PLANNEDTIME_ARR  \\\n",
       "0   2018-01-01  5972116           12          119            48030   \n",
       "1   2018-01-01  5966674           12          119            54001   \n",
       "2   2018-01-01  5959105           12          119            60001   \n",
       "3   2018-01-01  5966888           12          119            58801   \n",
       "4   2018-01-01  5965960           12          119            56401   \n",
       "\n",
       "   PLANNEDTIME_DEP  ACTUALTIME_ARR  ACTUALTIME_DEP  VEHICLEID LINEID ROUTEID  \\\n",
       "0            48030         48012.0         48012.0    2693211      1    1_37   \n",
       "1            54001         54023.0         54023.0    2693267      1    1_37   \n",
       "2            60001         59955.0         59955.0    2693263      1    1_37   \n",
       "3            58801         58771.0         58771.0    2693284      1    1_37   \n",
       "4            56401         56309.0         56323.0    2693209      1    1_37   \n",
       "\n",
       "   DIRECTION  PLANNEDTIME_DEP_T  ACTUALTIME_DEP_T  HOUR_DEPARTURE  DAYOFWEEK  \n",
       "0          1              47400           47427.0            13.0          0  \n",
       "1          1              53400           53410.0            15.0          0  \n",
       "2          1              59400           59426.0            16.0          0  \n",
       "3          1              58200           58220.0            16.0          0  \n",
       "4          1              55800           55807.0            15.0          0  "
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "bf9a2114",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = merged_dd.loc[merged_dd['DAYOFWEEK'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "55acea34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>PROGRNUMBER</th>\n",
       "      <th>STOPPOINTID</th>\n",
       "      <th>PLANNEDTIME_ARR</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_ARR</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "      <th>VEHICLEID</th>\n",
       "      <th>LINEID</th>\n",
       "      <th>ROUTEID</th>\n",
       "      <th>DIRECTION</th>\n",
       "      <th>PLANNEDTIME_DEP_T</th>\n",
       "      <th>ACTUALTIME_DEP_T</th>\n",
       "      <th>HOUR_DEPARTURE</th>\n",
       "      <th>DAYOFWEEK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>157744</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>5958030</td>\n",
       "      <td>59</td>\n",
       "      <td>4385</td>\n",
       "      <td>51599</td>\n",
       "      <td>51599</td>\n",
       "      <td>51331.0</td>\n",
       "      <td>51349.0</td>\n",
       "      <td>2172293</td>\n",
       "      <td>15</td>\n",
       "      <td>15_17</td>\n",
       "      <td>2</td>\n",
       "      <td>47400</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157745</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>5967345</td>\n",
       "      <td>59</td>\n",
       "      <td>4385</td>\n",
       "      <td>55199</td>\n",
       "      <td>55199</td>\n",
       "      <td>54902.0</td>\n",
       "      <td>54920.0</td>\n",
       "      <td>2868404</td>\n",
       "      <td>15</td>\n",
       "      <td>15_17</td>\n",
       "      <td>2</td>\n",
       "      <td>51000</td>\n",
       "      <td>51000.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157746</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>5965972</td>\n",
       "      <td>59</td>\n",
       "      <td>4385</td>\n",
       "      <td>47999</td>\n",
       "      <td>47999</td>\n",
       "      <td>47612.0</td>\n",
       "      <td>47612.0</td>\n",
       "      <td>2693250</td>\n",
       "      <td>15</td>\n",
       "      <td>15_17</td>\n",
       "      <td>2</td>\n",
       "      <td>43800</td>\n",
       "      <td>43777.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157747</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>5957141</td>\n",
       "      <td>59</td>\n",
       "      <td>4385</td>\n",
       "      <td>50999</td>\n",
       "      <td>50999</td>\n",
       "      <td>50140.0</td>\n",
       "      <td>50152.0</td>\n",
       "      <td>2693284</td>\n",
       "      <td>15</td>\n",
       "      <td>15_17</td>\n",
       "      <td>2</td>\n",
       "      <td>46800</td>\n",
       "      <td>46887.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157748</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>5967112</td>\n",
       "      <td>59</td>\n",
       "      <td>4385</td>\n",
       "      <td>44399</td>\n",
       "      <td>44399</td>\n",
       "      <td>44292.0</td>\n",
       "      <td>44304.0</td>\n",
       "      <td>2693282</td>\n",
       "      <td>15</td>\n",
       "      <td>15_17</td>\n",
       "      <td>2</td>\n",
       "      <td>40200</td>\n",
       "      <td>40203.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       DAYOFSERVICE   TRIPID  PROGRNUMBER  STOPPOINTID  PLANNEDTIME_ARR  \\\n",
       "157744   2018-01-02  5958030           59         4385            51599   \n",
       "157745   2018-01-02  5967345           59         4385            55199   \n",
       "157746   2018-01-02  5965972           59         4385            47999   \n",
       "157747   2018-01-02  5957141           59         4385            50999   \n",
       "157748   2018-01-02  5967112           59         4385            44399   \n",
       "\n",
       "        PLANNEDTIME_DEP  ACTUALTIME_ARR  ACTUALTIME_DEP  VEHICLEID LINEID  \\\n",
       "157744            51599         51331.0         51349.0    2172293     15   \n",
       "157745            55199         54902.0         54920.0    2868404     15   \n",
       "157746            47999         47612.0         47612.0    2693250     15   \n",
       "157747            50999         50140.0         50152.0    2693284     15   \n",
       "157748            44399         44292.0         44304.0    2693282     15   \n",
       "\n",
       "       ROUTEID  DIRECTION  PLANNEDTIME_DEP_T  ACTUALTIME_DEP_T  \\\n",
       "157744   15_17          2              47400           47400.0   \n",
       "157745   15_17          2              51000           51000.0   \n",
       "157746   15_17          2              43800           43777.0   \n",
       "157747   15_17          2              46800           46887.0   \n",
       "157748   15_17          2              40200           40203.0   \n",
       "\n",
       "        HOUR_DEPARTURE  DAYOFWEEK  \n",
       "157744            14.0          1  \n",
       "157745            15.0          1  \n",
       "157746            13.0          1  \n",
       "157747            13.0          1  \n",
       "157748            12.0          1  "
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "4afecc80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DAYOFSERVICE         datetime64[ns]\n",
       "TRIPID                        int32\n",
       "PROGRNUMBER                    int8\n",
       "STOPPOINTID                   int32\n",
       "PLANNEDTIME_ARR               int32\n",
       "PLANNEDTIME_DEP               int32\n",
       "ACTUALTIME_ARR              float64\n",
       "ACTUALTIME_DEP              float64\n",
       "VEHICLEID                     int32\n",
       "LINEID                     category\n",
       "ROUTEID                    category\n",
       "DIRECTION                      int8\n",
       "PLANNEDTIME_DEP_T             int32\n",
       "ACTUALTIME_DEP_T            float64\n",
       "HOUR_DEPARTURE              float64\n",
       "DAYOFWEEK                     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dd.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "4de34b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt                         int64\n",
       "temp                        int8\n",
       "visibility                  int8\n",
       "wind_speed                  int8\n",
       "weather_main                int8\n",
       "date              datetime64[ns]\n",
       "DAYOFSERVICE              object\n",
       "HOUR_DEPARTURE             int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "e561575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather[\"DAYOFSERVICE\"] = weather[\"DAYOFSERVICE\"].astype(\"datetime64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "b8a2088f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>PROGRNUMBER</th>\n",
       "      <th>STOPPOINTID</th>\n",
       "      <th>PLANNEDTIME_ARR</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_ARR</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "      <th>VEHICLEID</th>\n",
       "      <th>LINEID</th>\n",
       "      <th>ROUTEID</th>\n",
       "      <th>DIRECTION</th>\n",
       "      <th>PLANNEDTIME_DEP_T</th>\n",
       "      <th>ACTUALTIME_DEP_T</th>\n",
       "      <th>HOUR_DEPARTURE</th>\n",
       "      <th>DAYOFWEEK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>5972116</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>48030</td>\n",
       "      <td>48030</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>47400</td>\n",
       "      <td>47427.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>5966674</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>54001</td>\n",
       "      <td>54001</td>\n",
       "      <td>54023.0</td>\n",
       "      <td>54023.0</td>\n",
       "      <td>2693267</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>53400</td>\n",
       "      <td>53410.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>5959105</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>60001</td>\n",
       "      <td>60001</td>\n",
       "      <td>59955.0</td>\n",
       "      <td>59955.0</td>\n",
       "      <td>2693263</td>\n",
       "      <td>1</td>\n",
       "      <td>1_37</td>\n",
       "      <td>1</td>\n",
       "      <td>59400</td>\n",
       "      <td>59426.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  DAYOFSERVICE   TRIPID  PROGRNUMBER  STOPPOINTID  PLANNEDTIME_ARR  \\\n",
       "0   2018-01-01  5972116           12          119            48030   \n",
       "1   2018-01-01  5966674           12          119            54001   \n",
       "2   2018-01-01  5959105           12          119            60001   \n",
       "\n",
       "   PLANNEDTIME_DEP  ACTUALTIME_ARR  ACTUALTIME_DEP  VEHICLEID LINEID ROUTEID  \\\n",
       "0            48030         48012.0         48012.0    2693211      1    1_37   \n",
       "1            54001         54023.0         54023.0    2693267      1    1_37   \n",
       "2            60001         59955.0         59955.0    2693263      1    1_37   \n",
       "\n",
       "   DIRECTION  PLANNEDTIME_DEP_T  ACTUALTIME_DEP_T  HOUR_DEPARTURE  DAYOFWEEK  \n",
       "0          1              47400           47427.0            13.0          0  \n",
       "1          1              53400           53410.0            15.0          0  \n",
       "2          1              59400           59426.0            16.0          0  "
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dd.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "1727e016",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dd = dd.merge(merged_dd, weather, on=['DAYOFSERVICE', 'HOUR_DEPARTURE'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "d4243052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>PROGRNUMBER</th>\n",
       "      <th>STOPPOINTID</th>\n",
       "      <th>PLANNEDTIME_ARR</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_ARR</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "      <th>VEHICLEID</th>\n",
       "      <th>LINEID</th>\n",
       "      <th>...</th>\n",
       "      <th>PLANNEDTIME_DEP_T</th>\n",
       "      <th>ACTUALTIME_DEP_T</th>\n",
       "      <th>HOUR_DEPARTURE</th>\n",
       "      <th>DAYOFWEEK</th>\n",
       "      <th>dt</th>\n",
       "      <th>temp</th>\n",
       "      <th>visibility</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>weather_main</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>5972116</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>48030</td>\n",
       "      <td>48030</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>48012.0</td>\n",
       "      <td>2693211</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>47400</td>\n",
       "      <td>47427.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.514812e+09</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018-01-01 13:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>5966674</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>54001</td>\n",
       "      <td>54001</td>\n",
       "      <td>54023.0</td>\n",
       "      <td>54023.0</td>\n",
       "      <td>2693267</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>53400</td>\n",
       "      <td>53410.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.514819e+09</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018-01-01 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>5959105</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>60001</td>\n",
       "      <td>60001</td>\n",
       "      <td>59955.0</td>\n",
       "      <td>59955.0</td>\n",
       "      <td>2693263</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>59400</td>\n",
       "      <td>59426.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.514822e+09</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-01-01 16:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>5966888</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>58801</td>\n",
       "      <td>58801</td>\n",
       "      <td>58771.0</td>\n",
       "      <td>58771.0</td>\n",
       "      <td>2693284</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>58200</td>\n",
       "      <td>58220.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.514822e+09</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-01-01 16:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>5965960</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>56401</td>\n",
       "      <td>56401</td>\n",
       "      <td>56309.0</td>\n",
       "      <td>56323.0</td>\n",
       "      <td>2693209</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>55800</td>\n",
       "      <td>55807.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.514819e+09</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018-01-01 15:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  DAYOFSERVICE   TRIPID  PROGRNUMBER  STOPPOINTID  PLANNEDTIME_ARR  \\\n",
       "0   2018-01-01  5972116           12          119            48030   \n",
       "1   2018-01-01  5966674           12          119            54001   \n",
       "2   2018-01-01  5959105           12          119            60001   \n",
       "3   2018-01-01  5966888           12          119            58801   \n",
       "4   2018-01-01  5965960           12          119            56401   \n",
       "\n",
       "   PLANNEDTIME_DEP  ACTUALTIME_ARR  ACTUALTIME_DEP  VEHICLEID LINEID  ...  \\\n",
       "0            48030         48012.0         48012.0    2693211      1  ...   \n",
       "1            54001         54023.0         54023.0    2693267      1  ...   \n",
       "2            60001         59955.0         59955.0    2693263      1  ...   \n",
       "3            58801         58771.0         58771.0    2693284      1  ...   \n",
       "4            56401         56309.0         56323.0    2693209      1  ...   \n",
       "\n",
       "  PLANNEDTIME_DEP_T  ACTUALTIME_DEP_T  HOUR_DEPARTURE  DAYOFWEEK  \\\n",
       "0             47400           47427.0            13.0          0   \n",
       "1             53400           53410.0            15.0          0   \n",
       "2             59400           59426.0            16.0          0   \n",
       "3             58200           58220.0            16.0          0   \n",
       "4             55800           55807.0            15.0          0   \n",
       "\n",
       "             dt  temp  visibility  wind_speed  weather_main  \\\n",
       "0  1.514812e+09   2.0         7.0         6.0           1.0   \n",
       "1  1.514819e+09   2.0        10.0         6.0           1.0   \n",
       "2  1.514822e+09   2.0        10.0         5.0           0.0   \n",
       "3  1.514822e+09   2.0        10.0         5.0           0.0   \n",
       "4  1.514819e+09   2.0        10.0         6.0           1.0   \n",
       "\n",
       "                 date  \n",
       "0 2018-01-01 13:00:00  \n",
       "1 2018-01-01 15:00:00  \n",
       "2 2018-01-01 16:00:00  \n",
       "3 2018-01-01 16:00:00  \n",
       "4 2018-01-01 15:00:00  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "bf86bfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_dd = merged_dd.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "7664fb30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DAYOFSERVICE         datetime64[ns]\n",
       "TRIPID                        int32\n",
       "PROGRNUMBER                    int8\n",
       "STOPPOINTID                   int32\n",
       "PLANNEDTIME_ARR               int32\n",
       "PLANNEDTIME_DEP               int32\n",
       "ACTUALTIME_ARR              float64\n",
       "ACTUALTIME_DEP              float64\n",
       "VEHICLEID                     int32\n",
       "LINEID                     category\n",
       "ROUTEID                    category\n",
       "DIRECTION                      int8\n",
       "PLANNEDTIME_DEP_T             int32\n",
       "ACTUALTIME_DEP_T            float64\n",
       "HOUR_DEPARTURE              float64\n",
       "DAYOFWEEK                     int64\n",
       "dt                            int64\n",
       "temp                           int8\n",
       "visibility                     int8\n",
       "wind_speed                     int8\n",
       "weather_main                   int8\n",
       "date                 datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dd.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "63f48813",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dd['ROUTEID'] = merged_dd['ROUTEID'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "4784a602",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dd['ROUTE_DIRECTION'] = merged_dd['ROUTEID'] + '_' + merged_dd['DIRECTION'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "0e6da165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DAYOFSERVICE         datetime64[ns]\n",
       "TRIPID                        int32\n",
       "PROGRNUMBER                    int8\n",
       "STOPPOINTID                   int32\n",
       "PLANNEDTIME_ARR               int32\n",
       "PLANNEDTIME_DEP               int32\n",
       "ACTUALTIME_ARR              float64\n",
       "ACTUALTIME_DEP              float64\n",
       "VEHICLEID                     int32\n",
       "LINEID                     category\n",
       "ROUTEID                      object\n",
       "DIRECTION                      int8\n",
       "PLANNEDTIME_DEP_T             int32\n",
       "ACTUALTIME_DEP_T            float64\n",
       "HOUR_DEPARTURE              float64\n",
       "DAYOFWEEK                     int64\n",
       "dt                            int64\n",
       "temp                           int8\n",
       "visibility                     int8\n",
       "wind_speed                     int8\n",
       "weather_main                   int8\n",
       "date                 datetime64[ns]\n",
       "ROUTE_DIRECTION              object\n",
       "dtype: object"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dd.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "50f67cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dd['ROUTEID'] = merged_dd['ROUTEID'].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "2e952927",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dd['ROUTE_DIRECTION'] = merged_dd['ROUTE_DIRECTION'].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "50704dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_dd = merged_dd.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "aadbef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fortySixA1 = merged_dd.loc[merged_dd['ROUTE_DIRECTION']==\"46A_68_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "2a34ea4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>PROGRNUMBER</th>\n",
       "      <th>STOPPOINTID</th>\n",
       "      <th>PLANNEDTIME_ARR</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_ARR</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "      <th>VEHICLEID</th>\n",
       "      <th>LINEID</th>\n",
       "      <th>ROUTEID</th>\n",
       "      <th>DIRECTION</th>\n",
       "      <th>PLANNEDTIME_DEP_T</th>\n",
       "      <th>ACTUALTIME_DEP_T</th>\n",
       "      <th>HOUR_DEPARTURE</th>\n",
       "      <th>DAYOFWEEK</th>\n",
       "      <th>dt</th>\n",
       "      <th>temp</th>\n",
       "      <th>visibility</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>weather_main</th>\n",
       "      <th>date</th>\n",
       "      <th>ROUTE_DIRECTION</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=176</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>int32</td>\n",
       "      <td>int8</td>\n",
       "      <td>int32</td>\n",
       "      <td>int32</td>\n",
       "      <td>int32</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>int32</td>\n",
       "      <td>category[known]</td>\n",
       "      <td>category[unknown]</td>\n",
       "      <td>int8</td>\n",
       "      <td>int32</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int8</td>\n",
       "      <td>int8</td>\n",
       "      <td>int8</td>\n",
       "      <td>int8</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>category[unknown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: loc-series, 9506 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                   DAYOFSERVICE TRIPID PROGRNUMBER STOPPOINTID PLANNEDTIME_ARR PLANNEDTIME_DEP ACTUALTIME_ARR ACTUALTIME_DEP VEHICLEID           LINEID            ROUTEID DIRECTION PLANNEDTIME_DEP_T ACTUALTIME_DEP_T HOUR_DEPARTURE DAYOFWEEK     dt  temp visibility wind_speed weather_main            date    ROUTE_DIRECTION\n",
       "npartitions=176                                                                                                                                                                                                                                                                                                                    \n",
       "                 datetime64[ns]  int32        int8       int32           int32           int32        float64        float64     int32  category[known]  category[unknown]      int8             int32          float64        float64     int64  int64  int8       int8       int8         int8  datetime64[ns]  category[unknown]\n",
       "                            ...    ...         ...         ...             ...             ...            ...            ...       ...              ...                ...       ...               ...              ...            ...       ...    ...   ...        ...        ...          ...             ...                ...\n",
       "...                         ...    ...         ...         ...             ...             ...            ...            ...       ...              ...                ...       ...               ...              ...            ...       ...    ...   ...        ...        ...          ...             ...                ...\n",
       "                            ...    ...         ...         ...             ...             ...            ...            ...       ...              ...                ...       ...               ...              ...            ...       ...    ...   ...        ...        ...          ...             ...                ...\n",
       "                            ...    ...         ...         ...             ...             ...            ...            ...       ...              ...                ...       ...               ...              ...            ...       ...    ...   ...        ...        ...          ...             ...                ...\n",
       "Dask Name: loc-series, 9506 tasks"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fortySixA1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "0553cbce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYOFSERVICE</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>PROGRNUMBER</th>\n",
       "      <th>STOPPOINTID</th>\n",
       "      <th>PLANNEDTIME_ARR</th>\n",
       "      <th>PLANNEDTIME_DEP</th>\n",
       "      <th>ACTUALTIME_ARR</th>\n",
       "      <th>ACTUALTIME_DEP</th>\n",
       "      <th>VEHICLEID</th>\n",
       "      <th>LINEID</th>\n",
       "      <th>ROUTEID</th>\n",
       "      <th>DIRECTION</th>\n",
       "      <th>PLANNEDTIME_DEP_T</th>\n",
       "      <th>ACTUALTIME_DEP_T</th>\n",
       "      <th>HOUR_DEPARTURE</th>\n",
       "      <th>DAYOFWEEK</th>\n",
       "      <th>dt</th>\n",
       "      <th>temp</th>\n",
       "      <th>visibility</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>weather_main</th>\n",
       "      <th>date</th>\n",
       "      <th>ROUTE_DIRECTION</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=176</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>int32</td>\n",
       "      <td>int8</td>\n",
       "      <td>int32</td>\n",
       "      <td>int32</td>\n",
       "      <td>int32</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>int32</td>\n",
       "      <td>category[known]</td>\n",
       "      <td>category[unknown]</td>\n",
       "      <td>int8</td>\n",
       "      <td>int32</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int8</td>\n",
       "      <td>int8</td>\n",
       "      <td>int8</td>\n",
       "      <td>int8</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>category[unknown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: assign, 8978 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                   DAYOFSERVICE TRIPID PROGRNUMBER STOPPOINTID PLANNEDTIME_ARR PLANNEDTIME_DEP ACTUALTIME_ARR ACTUALTIME_DEP VEHICLEID           LINEID            ROUTEID DIRECTION PLANNEDTIME_DEP_T ACTUALTIME_DEP_T HOUR_DEPARTURE DAYOFWEEK     dt  temp visibility wind_speed weather_main            date    ROUTE_DIRECTION\n",
       "npartitions=176                                                                                                                                                                                                                                                                                                                    \n",
       "                 datetime64[ns]  int32        int8       int32           int32           int32        float64        float64     int32  category[known]  category[unknown]      int8             int32          float64        float64     int64  int64  int8       int8       int8         int8  datetime64[ns]  category[unknown]\n",
       "                            ...    ...         ...         ...             ...             ...            ...            ...       ...              ...                ...       ...               ...              ...            ...       ...    ...   ...        ...        ...          ...             ...                ...\n",
       "...                         ...    ...         ...         ...             ...             ...            ...            ...       ...              ...                ...       ...               ...              ...            ...       ...    ...   ...        ...        ...          ...             ...                ...\n",
       "                            ...    ...         ...         ...             ...             ...            ...            ...       ...              ...                ...       ...               ...              ...            ...       ...    ...   ...        ...        ...          ...             ...                ...\n",
       "                            ...    ...         ...         ...             ...             ...            ...            ...       ...              ...                ...       ...               ...              ...            ...       ...    ...   ...        ...        ...          ...             ...                ...\n",
       "Dask Name: assign, 8978 tasks"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50de473",
   "metadata": {},
   "source": [
    "# make the task number smaller\n",
    "- starting on 9506 for fortysixa1 and 8978 for merged_dd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "248ba625",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fortySixA1 = fortySixA1.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "fa5e9247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(fortySixA1)) 14474"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "92701c80",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fortySixA1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [314]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(\u001b[43mfortySixA1\u001b[49m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fortySixA1' is not defined"
     ]
    }
   ],
   "source": [
    "print(type(fortySixA1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f94578",
   "metadata": {},
   "source": [
    "- converts to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7425a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#fortySixA1_pd = fortySixA1.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea9ae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(fortySixA1_pd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb40edbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fortySixA1_pd.to_csv('./test_46a_dask.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c4764d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88d0a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57a7bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_dd = pd.merge_asof(merged_dd, weather, on=\"col_name\", direction='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d684d0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_dd['epoch'] = merged_dd.DAYOFSERVICE.values.astype(np.int64).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39de97e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_dd['DAYOFSERVICE'] = merged_dd.DAYOFSERVICE.dt.day_name()\n",
    "#merged_dd['DAYOFSERVICE'] = pd.factorize(merged_dd['DAYOFSERVICE'], sort=True)[0] # wasn't right\n",
    "\n",
    "\n",
    "# idea is to create an extra column that rounds to the most recent hour in epoch form and join weather on that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcb57bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dd.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a957786",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2f1431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# safety cell to make sure code below doens't run\n",
    "printsdfgdsdg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83155684",
   "metadata": {},
   "source": [
    "### code for when ready to export file\n",
    "- must sort the weather join first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264ec293",
   "metadata": {},
   "outputs": [],
   "source": [
    "routes = merged_dd.ROUTEID.unique()\n",
    "routes = sorted(routes)\n",
    "print(len(routes))\n",
    "print(routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85cabed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8722612",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = merged_dd.LINEID.unique()\n",
    "lines = sorted(lines)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242fa1fa",
   "metadata": {},
   "source": [
    "- pretty sure lines is the correct one to use but could look into it further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5050f68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = [1, 2]\n",
    "#for d in directions:\n",
    "    for i in routes:\n",
    "        temp_rows = merged_dd.loc[(merged_dd['LINEID'] == i) & (merged_dd['DIRECTION'] == d)]\n",
    "        name = f\"line_{i}_direction{d}\"\n",
    "        \n",
    "        \n",
    "        temp_rows.to_csv(f'.{}/{name}.csv')\n",
    "        \n",
    "        \n",
    "#for d in directions:\n",
    "    for r in routes:\n",
    "        temp_rows = merged_dd.loc[(merged_dd['ROUTEID'] == r) & (merged_dd['DIRECTION'] == d)]\n",
    "        \n",
    "        line_folder = temp_rows[\"LINEID\"]\n",
    "        \n",
    "        \n",
    "        name = f\"route_{r}_direction{d}\"\n",
    "        \n",
    "        \n",
    "        temp_rows.to_csv(f'.{line_folder}/{name}.csv')\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
